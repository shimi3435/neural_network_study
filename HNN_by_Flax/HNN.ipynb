{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import optax                           # Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(32)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "\n",
    "MY_BATCH_SIZE = 100\n",
    "\n",
    "dftarget = pd.read_csv(\"target.csv\", header=None, dtype=jnp.float32)\n",
    "dfinput = pd.read_csv(\"input.csv\", header=None, dtype=jnp.float32)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dfinput.values, dftarget.values, test_size=0.2)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensors((X_train, Y_train))\n",
    "train_ds = train_ds.shuffle(len(train_ds), seed=0, reshuffle_each_iteration=True).batch(MY_BATCH_SIZE).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensors((X_test, Y_test))\n",
    "test_ds = test_ds.batch(MY_BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "O = jnp.zeros((N,N))\n",
    "Id = jnp.eye(N)\n",
    "S = jnp.vstack([jnp.hstack([O, Id]), jnp.hstack([-Id, O])])\n",
    "St = S.T\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "params = model.init(rng, jnp.ones(2))['params'] # initialize parameters by passing a template image\n",
    "tx = optax.adam(learning_rate)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(preds, targets):\n",
    "    return jnp.square(preds - targets).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def step(inputs, targets, state, is_train=True):\n",
    "    def sum_hamiltonian(params, x):\n",
    "        hamiltonian = state.apply_fn({'params': params}, x)\n",
    "        return jnp.sum(hamiltonian)\n",
    "    dhdu_dhdv = jax.jit(jax.grad(sum_hamiltonian, argnums=1))\n",
    "    def grad(params, x):\n",
    "        gradient = dhdu_dhdv(params, x)\n",
    "        return jnp.matmul(gradient, St)\n",
    "    def loss_fn(params):\n",
    "        preds = grad(params, inputs)\n",
    "        loss = MSE_loss(preds, targets)\n",
    "        return loss, preds\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n",
    "\n",
    "    if is_train:\n",
    "        (loss, preds), grads = grad_fn(state.params)\n",
    "        state = state.apply_gradients(grads = grads)\n",
    "    else:\n",
    "        loss, preds = loss_fn(state.params)\n",
    "\n",
    "    return loss, preds, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3000 training loss: 2.8011622428894043, evaluation loss: 2.803647041320801\n",
      "2/3000 training loss: 2.771186590194702, evaluation loss: 2.774057626724243\n",
      "3/3000 training loss: 2.741614580154419, evaluation loss: 2.744877338409424\n",
      "4/3000 training loss: 2.7124507427215576, evaluation loss: 2.7161102294921875\n",
      "5/3000 training loss: 2.683699131011963, evaluation loss: 2.6877593994140625\n",
      "6/3000 training loss: 2.655362844467163, evaluation loss: 2.6598293781280518\n",
      "7/3000 training loss: 2.627446174621582, evaluation loss: 2.632322072982788\n",
      "8/3000 training loss: 2.5999534130096436, evaluation loss: 2.6052417755126953\n",
      "9/3000 training loss: 2.5728871822357178, evaluation loss: 2.578590154647827\n",
      "10/3000 training loss: 2.546250104904175, evaluation loss: 2.5523695945739746\n",
      "11/3000 training loss: 2.5200445652008057, evaluation loss: 2.5265817642211914\n",
      "12/3000 training loss: 2.4942715167999268, evaluation loss: 2.501227617263794\n",
      "13/3000 training loss: 2.4689323902130127, evaluation loss: 2.4763078689575195\n",
      "14/3000 training loss: 2.444027900695801, evaluation loss: 2.451822519302368\n",
      "15/3000 training loss: 2.4195570945739746, evaluation loss: 2.427771806716919\n",
      "16/3000 training loss: 2.3955202102661133, evaluation loss: 2.4041545391082764\n",
      "17/3000 training loss: 2.3719167709350586, evaluation loss: 2.380969524383545\n",
      "18/3000 training loss: 2.3487446308135986, evaluation loss: 2.3582160472869873\n",
      "19/3000 training loss: 2.326003313064575, evaluation loss: 2.3358914852142334\n",
      "20/3000 training loss: 2.3036909103393555, evaluation loss: 2.313995122909546\n",
      "21/3000 training loss: 2.2818055152893066, evaluation loss: 2.292524576187134\n",
      "22/3000 training loss: 2.260345220565796, evaluation loss: 2.271477222442627\n",
      "23/3000 training loss: 2.2393081188201904, evaluation loss: 2.2508506774902344\n",
      "24/3000 training loss: 2.2186906337738037, evaluation loss: 2.230642080307007\n",
      "25/3000 training loss: 2.198490619659424, evaluation loss: 2.210848808288574\n",
      "26/3000 training loss: 2.1787056922912598, evaluation loss: 2.19146728515625\n",
      "27/3000 training loss: 2.1593315601348877, evaluation loss: 2.1724939346313477\n",
      "28/3000 training loss: 2.1403653621673584, evaluation loss: 2.1539251804351807\n",
      "29/3000 training loss: 2.121803045272827, evaluation loss: 2.1357569694519043\n",
      "30/3000 training loss: 2.1036410331726074, evaluation loss: 2.117985248565674\n",
      "31/3000 training loss: 2.0858747959136963, evaluation loss: 2.1006057262420654\n",
      "32/3000 training loss: 2.068500280380249, evaluation loss: 2.083613872528076\n",
      "33/3000 training loss: 2.0515127182006836, evaluation loss: 2.067004919052124\n",
      "34/3000 training loss: 2.034907579421997, evaluation loss: 2.050773859024048\n",
      "35/3000 training loss: 2.0186800956726074, evaluation loss: 2.0349156856536865\n",
      "36/3000 training loss: 2.0028250217437744, evaluation loss: 2.019425630569458\n",
      "37/3000 training loss: 1.9873372316360474, evaluation loss: 2.0042977333068848\n",
      "38/3000 training loss: 1.9722115993499756, evaluation loss: 1.9895267486572266\n",
      "39/3000 training loss: 1.9574427604675293, evaluation loss: 1.9751076698303223\n",
      "40/3000 training loss: 1.94302499294281, evaluation loss: 1.9610342979431152\n",
      "41/3000 training loss: 1.928952693939209, evaluation loss: 1.9473011493682861\n",
      "42/3000 training loss: 1.9152203798294067, evaluation loss: 1.933902382850647\n",
      "43/3000 training loss: 1.9018222093582153, evaluation loss: 1.9208320379257202\n",
      "44/3000 training loss: 1.8887522220611572, evaluation loss: 1.908084511756897\n",
      "45/3000 training loss: 1.8760043382644653, evaluation loss: 1.8956533670425415\n",
      "46/3000 training loss: 1.8635731935501099, evaluation loss: 1.8835326433181763\n",
      "47/3000 training loss: 1.851452112197876, evaluation loss: 1.8717167377471924\n",
      "48/3000 training loss: 1.8396353721618652, evaluation loss: 1.8601990938186646\n",
      "49/3000 training loss: 1.8281168937683105, evaluation loss: 1.8489739894866943\n",
      "50/3000 training loss: 1.816890835762024, evaluation loss: 1.838034987449646\n",
      "51/3000 training loss: 1.8059507608413696, evaluation loss: 1.8273762464523315\n",
      "52/3000 training loss: 1.7952907085418701, evaluation loss: 1.8169915676116943\n",
      "53/3000 training loss: 1.7849045991897583, evaluation loss: 1.8068749904632568\n",
      "54/3000 training loss: 1.7747864723205566, evaluation loss: 1.797020435333252\n",
      "55/3000 training loss: 1.7649301290512085, evaluation loss: 1.787421703338623\n",
      "56/3000 training loss: 1.7553298473358154, evaluation loss: 1.7780730724334717\n",
      "57/3000 training loss: 1.7459791898727417, evaluation loss: 1.7689684629440308\n",
      "58/3000 training loss: 1.7368727922439575, evaluation loss: 1.7601019144058228\n",
      "59/3000 training loss: 1.7280042171478271, evaluation loss: 1.7514677047729492\n",
      "60/3000 training loss: 1.7193679809570312, evaluation loss: 1.7430603504180908\n",
      "61/3000 training loss: 1.7109583616256714, evaluation loss: 1.7348734140396118\n",
      "62/3000 training loss: 1.702769160270691, evaluation loss: 1.7269017696380615\n",
      "63/3000 training loss: 1.6947952508926392, evaluation loss: 1.719139575958252\n",
      "64/3000 training loss: 1.687030553817749, evaluation loss: 1.7115811109542847\n",
      "65/3000 training loss: 1.6794698238372803, evaluation loss: 1.7042216062545776\n",
      "66/3000 training loss: 1.672107458114624, evaluation loss: 1.6970546245574951\n",
      "67/3000 training loss: 1.6649380922317505, evaluation loss: 1.6900756359100342\n",
      "68/3000 training loss: 1.6579562425613403, evaluation loss: 1.683279037475586\n",
      "69/3000 training loss: 1.6511569023132324, evaluation loss: 1.6766595840454102\n",
      "70/3000 training loss: 1.6445344686508179, evaluation loss: 1.6702122688293457\n",
      "71/3000 training loss: 1.638083815574646, evaluation loss: 1.6639318466186523\n",
      "72/3000 training loss: 1.6318004131317139, evaluation loss: 1.6578136682510376\n",
      "73/3000 training loss: 1.6256787776947021, evaluation loss: 1.6518524885177612\n",
      "74/3000 training loss: 1.6197141408920288, evaluation loss: 1.6460437774658203\n",
      "75/3000 training loss: 1.6139017343521118, evaluation loss: 1.6403824090957642\n",
      "76/3000 training loss: 1.6082366704940796, evaluation loss: 1.6348639726638794\n",
      "77/3000 training loss: 1.6027146577835083, evaluation loss: 1.629483938217163\n",
      "78/3000 training loss: 1.5973305702209473, evaluation loss: 1.6242377758026123\n",
      "79/3000 training loss: 1.5920802354812622, evaluation loss: 1.6191208362579346\n",
      "80/3000 training loss: 1.5869591236114502, evaluation loss: 1.6141289472579956\n",
      "81/3000 training loss: 1.5819629430770874, evaluation loss: 1.6092578172683716\n",
      "82/3000 training loss: 1.5770872831344604, evaluation loss: 1.6045032739639282\n",
      "83/3000 training loss: 1.5723283290863037, evaluation loss: 1.5998613834381104\n",
      "84/3000 training loss: 1.5676815509796143, evaluation loss: 1.5953278541564941\n",
      "85/3000 training loss: 1.563143253326416, evaluation loss: 1.5908989906311035\n",
      "86/3000 training loss: 1.5587095022201538, evaluation loss: 1.5865707397460938\n",
      "87/3000 training loss: 1.5543761253356934, evaluation loss: 1.5823394060134888\n",
      "88/3000 training loss: 1.5501396656036377, evaluation loss: 1.578201413154602\n",
      "89/3000 training loss: 1.5459965467453003, evaluation loss: 1.574152946472168\n",
      "90/3000 training loss: 1.541943073272705, evaluation loss: 1.570190668106079\n",
      "91/3000 training loss: 1.5379754304885864, evaluation loss: 1.566311240196228\n",
      "92/3000 training loss: 1.5340903997421265, evaluation loss: 1.5625110864639282\n",
      "93/3000 training loss: 1.530285120010376, evaluation loss: 1.5587871074676514\n",
      "94/3000 training loss: 1.5265554189682007, evaluation loss: 1.55513596534729\n",
      "95/3000 training loss: 1.5228989124298096, evaluation loss: 1.5515546798706055\n",
      "96/3000 training loss: 1.5193122625350952, evaluation loss: 1.5480403900146484\n",
      "97/3000 training loss: 1.5157922506332397, evaluation loss: 1.5445897579193115\n",
      "98/3000 training loss: 1.512336015701294, evaluation loss: 1.5412001609802246\n",
      "99/3000 training loss: 1.5089408159255981, evaluation loss: 1.5378687381744385\n",
      "100/3000 training loss: 1.5056039094924927, evaluation loss: 1.5345927476882935\n",
      "101/3000 training loss: 1.5023226737976074, evaluation loss: 1.5313698053359985\n",
      "102/3000 training loss: 1.4990941286087036, evaluation loss: 1.528196930885315\n",
      "103/3000 training loss: 1.4959158897399902, evaluation loss: 1.5250720977783203\n",
      "104/3000 training loss: 1.4927856922149658, evaluation loss: 1.521992564201355\n",
      "105/3000 training loss: 1.4897009134292603, evaluation loss: 1.5189560651779175\n",
      "106/3000 training loss: 1.4866594076156616, evaluation loss: 1.515960454940796\n",
      "107/3000 training loss: 1.483658790588379, evaluation loss: 1.5130035877227783\n",
      "108/3000 training loss: 1.4806967973709106, evaluation loss: 1.5100831985473633\n",
      "109/3000 training loss: 1.477771520614624, evaluation loss: 1.5071971416473389\n",
      "110/3000 training loss: 1.4748808145523071, evaluation loss: 1.5043436288833618\n",
      "111/3000 training loss: 1.4720227718353271, evaluation loss: 1.5015208721160889\n",
      "112/3000 training loss: 1.4691953659057617, evaluation loss: 1.498726725578308\n",
      "113/3000 training loss: 1.4663969278335571, evaluation loss: 1.4959594011306763\n",
      "114/3000 training loss: 1.4636256694793701, evaluation loss: 1.4932174682617188\n",
      "115/3000 training loss: 1.4608796834945679, evaluation loss: 1.4904989004135132\n",
      "116/3000 training loss: 1.4581574201583862, evaluation loss: 1.4878023862838745\n",
      "117/3000 training loss: 1.4554574489593506, evaluation loss: 1.4851263761520386\n",
      "118/3000 training loss: 1.4527779817581177, evaluation loss: 1.4824693202972412\n",
      "119/3000 training loss: 1.450117826461792, evaluation loss: 1.4798296689987183\n",
      "120/3000 training loss: 1.4474753141403198, evaluation loss: 1.4772064685821533\n",
      "121/3000 training loss: 1.4448491334915161, evaluation loss: 1.4745975732803345\n",
      "122/3000 training loss: 1.4422380924224854, evaluation loss: 1.4720027446746826\n",
      "123/3000 training loss: 1.439640998840332, evaluation loss: 1.4694199562072754\n",
      "124/3000 training loss: 1.4370564222335815, evaluation loss: 1.4668484926223755\n",
      "125/3000 training loss: 1.434483289718628, evaluation loss: 1.4642871618270874\n",
      "126/3000 training loss: 1.4319206476211548, evaluation loss: 1.4617347717285156\n",
      "127/3000 training loss: 1.4293673038482666, evaluation loss: 1.4591904878616333\n",
      "128/3000 training loss: 1.426822304725647, evaluation loss: 1.4566532373428345\n",
      "129/3000 training loss: 1.4242846965789795, evaluation loss: 1.4541219472885132\n",
      "130/3000 training loss: 1.4217534065246582, evaluation loss: 1.4515960216522217\n",
      "131/3000 training loss: 1.4192278385162354, evaluation loss: 1.449074387550354\n",
      "132/3000 training loss: 1.416706919670105, evaluation loss: 1.4465564489364624\n",
      "133/3000 training loss: 1.4141899347305298, evaluation loss: 1.444041132926941\n",
      "134/3000 training loss: 1.411676049232483, evaluation loss: 1.4415279626846313\n",
      "135/3000 training loss: 1.4091647863388062, evaluation loss: 1.439016342163086\n",
      "136/3000 training loss: 1.4066553115844727, evaluation loss: 1.4365054368972778\n",
      "137/3000 training loss: 1.4041467905044556, evaluation loss: 1.4339945316314697\n",
      "138/3000 training loss: 1.4016392230987549, evaluation loss: 1.4314831495285034\n",
      "139/3000 training loss: 1.3991312980651855, evaluation loss: 1.4289706945419312\n",
      "140/3000 training loss: 1.396622657775879, evaluation loss: 1.4264568090438843\n",
      "141/3000 training loss: 1.3941130638122559, evaluation loss: 1.423940658569336\n",
      "142/3000 training loss: 1.391601800918579, evaluation loss: 1.4214221239089966\n",
      "143/3000 training loss: 1.3890883922576904, evaluation loss: 1.418900489807129\n",
      "144/3000 training loss: 1.3865724802017212, evaluation loss: 1.4163756370544434\n",
      "145/3000 training loss: 1.3840538263320923, evaluation loss: 1.4138470888137817\n",
      "146/3000 training loss: 1.3815317153930664, evaluation loss: 1.4113142490386963\n",
      "147/3000 training loss: 1.3790061473846436, evaluation loss: 1.408776879310608\n",
      "148/3000 training loss: 1.3764764070510864, evaluation loss: 1.4062347412109375\n",
      "149/3000 training loss: 1.3739423751831055, evaluation loss: 1.4036877155303955\n",
      "150/3000 training loss: 1.3714038133621216, evaluation loss: 1.4011350870132446\n",
      "151/3000 training loss: 1.3688602447509766, evaluation loss: 1.3985769748687744\n",
      "152/3000 training loss: 1.36631178855896, evaluation loss: 1.3960130214691162\n",
      "153/3000 training loss: 1.3637579679489136, evaluation loss: 1.3934428691864014\n",
      "154/3000 training loss: 1.3611985445022583, evaluation loss: 1.3908666372299194\n",
      "155/3000 training loss: 1.358633279800415, evaluation loss: 1.3882839679718018\n",
      "156/3000 training loss: 1.3560621738433838, evaluation loss: 1.3856945037841797\n",
      "157/3000 training loss: 1.353485107421875, evaluation loss: 1.3830984830856323\n",
      "158/3000 training loss: 1.350901484489441, evaluation loss: 1.3804954290390015\n",
      "159/3000 training loss: 1.3483119010925293, evaluation loss: 1.3778855800628662\n",
      "160/3000 training loss: 1.3457157611846924, evaluation loss: 1.3752684593200684\n",
      "161/3000 training loss: 1.343112826347351, evaluation loss: 1.372644066810608\n",
      "162/3000 training loss: 1.3405033349990845, evaluation loss: 1.3700127601623535\n",
      "163/3000 training loss: 1.3378872871398926, evaluation loss: 1.367374062538147\n",
      "164/3000 training loss: 1.3352642059326172, evaluation loss: 1.3647276163101196\n",
      "165/3000 training loss: 1.3326343297958374, evaluation loss: 1.3620741367340088\n",
      "166/3000 training loss: 1.3299974203109741, evaluation loss: 1.3594130277633667\n",
      "167/3000 training loss: 1.3273539543151855, evaluation loss: 1.356744647026062\n",
      "168/3000 training loss: 1.3247032165527344, evaluation loss: 1.354068636894226\n",
      "169/3000 training loss: 1.3220455646514893, evaluation loss: 1.351385235786438\n",
      "170/3000 training loss: 1.3193811178207397, evaluation loss: 1.3486943244934082\n",
      "171/3000 training loss: 1.3167095184326172, evaluation loss: 1.3459960222244263\n",
      "172/3000 training loss: 1.3140310049057007, evaluation loss: 1.3432902097702026\n",
      "173/3000 training loss: 1.3113454580307007, evaluation loss: 1.3405771255493164\n",
      "174/3000 training loss: 1.3086532354354858, evaluation loss: 1.3378565311431885\n",
      "175/3000 training loss: 1.305954098701477, evaluation loss: 1.3351287841796875\n",
      "176/3000 training loss: 1.3032481670379639, evaluation loss: 1.3323938846588135\n",
      "177/3000 training loss: 1.3005353212356567, evaluation loss: 1.3296515941619873\n",
      "178/3000 training loss: 1.2978159189224243, evaluation loss: 1.3269025087356567\n",
      "179/3000 training loss: 1.2950897216796875, evaluation loss: 1.324146032333374\n",
      "180/3000 training loss: 1.292357087135315, evaluation loss: 1.3213828802108765\n",
      "181/3000 training loss: 1.2896180152893066, evaluation loss: 1.3186126947402954\n",
      "182/3000 training loss: 1.286872148513794, evaluation loss: 1.3158358335494995\n",
      "183/3000 training loss: 1.2841202020645142, evaluation loss: 1.3130522966384888\n",
      "184/3000 training loss: 1.2813620567321777, evaluation loss: 1.3102623224258423\n",
      "185/3000 training loss: 1.2785978317260742, evaluation loss: 1.307465672492981\n",
      "186/3000 training loss: 1.275827407836914, evaluation loss: 1.304662823677063\n",
      "187/3000 training loss: 1.2730510234832764, evaluation loss: 1.3018537759780884\n",
      "188/3000 training loss: 1.2702689170837402, evaluation loss: 1.2990384101867676\n",
      "189/3000 training loss: 1.2674809694290161, evaluation loss: 1.2962172031402588\n",
      "190/3000 training loss: 1.2646875381469727, evaluation loss: 1.293390154838562\n",
      "191/3000 training loss: 1.2618886232376099, evaluation loss: 1.2905573844909668\n",
      "192/3000 training loss: 1.2590842247009277, evaluation loss: 1.2877190113067627\n",
      "193/3000 training loss: 1.2562745809555054, evaluation loss: 1.2848752737045288\n",
      "194/3000 training loss: 1.2534600496292114, evaluation loss: 1.2820261716842651\n",
      "195/3000 training loss: 1.2506401538848877, evaluation loss: 1.2791720628738403\n",
      "196/3000 training loss: 1.2478158473968506, evaluation loss: 1.2763125896453857\n",
      "197/3000 training loss: 1.2449865341186523, evaluation loss: 1.2734485864639282\n",
      "198/3000 training loss: 1.2421528100967407, evaluation loss: 1.2705795764923096\n",
      "199/3000 training loss: 1.2393147945404053, evaluation loss: 1.267706274986267\n",
      "200/3000 training loss: 1.2364723682403564, evaluation loss: 1.2648284435272217\n",
      "201/3000 training loss: 1.233625888824463, evaluation loss: 1.261946439743042\n",
      "202/3000 training loss: 1.2307753562927246, evaluation loss: 1.2590603828430176\n",
      "203/3000 training loss: 1.2279211282730103, evaluation loss: 1.2561705112457275\n",
      "204/3000 training loss: 1.2250632047653198, evaluation loss: 1.2532767057418823\n",
      "205/3000 training loss: 1.2222017049789429, evaluation loss: 1.2503794431686401\n",
      "206/3000 training loss: 1.219336986541748, evaluation loss: 1.2474788427352905\n",
      "207/3000 training loss: 1.2164689302444458, evaluation loss: 1.244575023651123\n",
      "208/3000 training loss: 1.2135982513427734, evaluation loss: 1.2416679859161377\n",
      "209/3000 training loss: 1.2107244729995728, evaluation loss: 1.2387583255767822\n",
      "210/3000 training loss: 1.207848072052002, evaluation loss: 1.2358460426330566\n",
      "211/3000 training loss: 1.2049691677093506, evaluation loss: 1.2329310178756714\n",
      "212/3000 training loss: 1.2020879983901978, evaluation loss: 1.2300138473510742\n",
      "213/3000 training loss: 1.199204444885254, evaluation loss: 1.2270944118499756\n",
      "214/3000 training loss: 1.1963192224502563, evaluation loss: 1.2241730690002441\n",
      "215/3000 training loss: 1.193432092666626, evaluation loss: 1.221250057220459\n",
      "216/3000 training loss: 1.1905434131622314, evaluation loss: 1.2183254957199097\n",
      "217/3000 training loss: 1.1876531839370728, evaluation loss: 1.2153995037078857\n",
      "218/3000 training loss: 1.1847617626190186, evaluation loss: 1.2124723196029663\n",
      "219/3000 training loss: 1.1818690299987793, evaluation loss: 1.209544062614441\n",
      "220/3000 training loss: 1.1789757013320923, evaluation loss: 1.2066152095794678\n",
      "221/3000 training loss: 1.1760814189910889, evaluation loss: 1.2036855220794678\n",
      "222/3000 training loss: 1.1731867790222168, evaluation loss: 1.2007555961608887\n",
      "223/3000 training loss: 1.170291781425476, evaluation loss: 1.197825312614441\n",
      "224/3000 training loss: 1.1673964262008667, evaluation loss: 1.1948949098587036\n",
      "225/3000 training loss: 1.1645013093948364, evaluation loss: 1.191964864730835\n",
      "226/3000 training loss: 1.1616061925888062, evaluation loss: 1.1890350580215454\n",
      "227/3000 training loss: 1.158711552619934, evaluation loss: 1.1861059665679932\n",
      "228/3000 training loss: 1.1558176279067993, evaluation loss: 1.1831774711608887\n",
      "229/3000 training loss: 1.1529241800308228, evaluation loss: 1.1802499294281006\n",
      "230/3000 training loss: 1.1500318050384521, evaluation loss: 1.177323818206787\n",
      "231/3000 training loss: 1.147140622138977, evaluation loss: 1.17439866065979\n",
      "232/3000 training loss: 1.1442508697509766, evaluation loss: 1.1714754104614258\n",
      "233/3000 training loss: 1.1413623094558716, evaluation loss: 1.1685535907745361\n",
      "234/3000 training loss: 1.1384756565093994, evaluation loss: 1.1656337976455688\n",
      "235/3000 training loss: 1.13559091091156, evaluation loss: 1.1627161502838135\n",
      "236/3000 training loss: 1.1327080726623535, evaluation loss: 1.1598010063171387\n",
      "237/3000 training loss: 1.1298277378082275, evaluation loss: 1.1568883657455444\n",
      "238/3000 training loss: 1.126949667930603, evaluation loss: 1.1539782285690308\n",
      "239/3000 training loss: 1.1240744590759277, evaluation loss: 1.1510710716247559\n",
      "240/3000 training loss: 1.1212018728256226, evaluation loss: 1.1481671333312988\n",
      "241/3000 training loss: 1.1183323860168457, evaluation loss: 1.1452666521072388\n",
      "242/3000 training loss: 1.1154659986495972, evaluation loss: 1.142369270324707\n",
      "243/3000 training loss: 1.1126030683517456, evaluation loss: 1.1394758224487305\n",
      "244/3000 training loss: 1.109743595123291, evaluation loss: 1.1365864276885986\n",
      "245/3000 training loss: 1.1068881750106812, evaluation loss: 1.133700966835022\n",
      "246/3000 training loss: 1.1040364503860474, evaluation loss: 1.1308197975158691\n",
      "247/3000 training loss: 1.1011888980865479, evaluation loss: 1.1279430389404297\n",
      "248/3000 training loss: 1.0983456373214722, evaluation loss: 1.1250710487365723\n",
      "249/3000 training loss: 1.095507025718689, evaluation loss: 1.122204065322876\n",
      "250/3000 training loss: 1.0926728248596191, evaluation loss: 1.1193418502807617\n",
      "251/3000 training loss: 1.0898436307907104, evaluation loss: 1.1164848804473877\n",
      "252/3000 training loss: 1.087019443511963, evaluation loss: 1.1136335134506226\n",
      "253/3000 training loss: 1.0842005014419556, evaluation loss: 1.1107875108718872\n",
      "254/3000 training loss: 1.081386685371399, evaluation loss: 1.1079474687576294\n",
      "255/3000 training loss: 1.0785785913467407, evaluation loss: 1.1051133871078491\n",
      "256/3000 training loss: 1.075776219367981, evaluation loss: 1.1022851467132568\n",
      "257/3000 training loss: 1.0729798078536987, evaluation loss: 1.0994635820388794\n",
      "258/3000 training loss: 1.070189356803894, evaluation loss: 1.0966482162475586\n",
      "259/3000 training loss: 1.0674052238464355, evaluation loss: 1.0938396453857422\n",
      "260/3000 training loss: 1.0646272897720337, evaluation loss: 1.0910379886627197\n",
      "261/3000 training loss: 1.0618561506271362, evaluation loss: 1.0882433652877808\n",
      "262/3000 training loss: 1.0590914487838745, evaluation loss: 1.0854556560516357\n",
      "263/3000 training loss: 1.0563338994979858, evaluation loss: 1.0826754570007324\n",
      "264/3000 training loss: 1.0535833835601807, evaluation loss: 1.0799026489257812\n",
      "265/3000 training loss: 1.050839900970459, evaluation loss: 1.0771375894546509\n",
      "266/3000 training loss: 1.0481038093566895, evaluation loss: 1.0743803977966309\n",
      "267/3000 training loss: 1.0453752279281616, evaluation loss: 1.0716310739517212\n",
      "268/3000 training loss: 1.042654275894165, evaluation loss: 1.068889856338501\n",
      "269/3000 training loss: 1.0399411916732788, evaluation loss: 1.0661571025848389\n",
      "270/3000 training loss: 1.0372360944747925, evaluation loss: 1.0634326934814453\n",
      "271/3000 training loss: 1.034538984298706, evaluation loss: 1.0607166290283203\n",
      "272/3000 training loss: 1.0318500995635986, evaluation loss: 1.0580095052719116\n",
      "273/3000 training loss: 1.0291696786880493, evaluation loss: 1.0553112030029297\n",
      "274/3000 training loss: 1.026497721672058, evaluation loss: 1.0526219606399536\n",
      "275/3000 training loss: 1.023834228515625, evaluation loss: 1.0499417781829834\n",
      "276/3000 training loss: 1.0211796760559082, evaluation loss: 1.0472708940505981\n",
      "277/3000 training loss: 1.0185339450836182, evaluation loss: 1.0446094274520874\n",
      "278/3000 training loss: 1.015897274017334, evaluation loss: 1.0419574975967407\n",
      "279/3000 training loss: 1.0132697820663452, evaluation loss: 1.0393152236938477\n",
      "280/3000 training loss: 1.0106514692306519, evaluation loss: 1.0366826057434082\n",
      "281/3000 training loss: 1.008042573928833, evaluation loss: 1.034060001373291\n",
      "282/3000 training loss: 1.0054430961608887, evaluation loss: 1.0314472913742065\n",
      "283/3000 training loss: 1.002853274345398, evaluation loss: 1.0288447141647339\n",
      "284/3000 training loss: 1.0002731084823608, evaluation loss: 1.0262525081634521\n",
      "285/3000 training loss: 0.9977027177810669, evaluation loss: 1.0236704349517822\n",
      "286/3000 training loss: 0.9951422810554504, evaluation loss: 1.0210989713668823\n",
      "287/3000 training loss: 0.9925918579101562, evaluation loss: 1.0185378789901733\n",
      "288/3000 training loss: 0.9900515675544739, evaluation loss: 1.015987515449524\n",
      "289/3000 training loss: 0.9875214099884033, evaluation loss: 1.0134477615356445\n",
      "290/3000 training loss: 0.9850015044212341, evaluation loss: 1.0109188556671143\n",
      "291/3000 training loss: 0.9824919700622559, evaluation loss: 1.0084009170532227\n",
      "292/3000 training loss: 0.9799928665161133, evaluation loss: 1.0058939456939697\n",
      "293/3000 training loss: 0.977504312992096, evaluation loss: 1.0033979415893555\n",
      "294/3000 training loss: 0.9750263690948486, evaluation loss: 1.0009130239486694\n",
      "295/3000 training loss: 0.9725589156150818, evaluation loss: 0.9984392523765564\n",
      "296/3000 training loss: 0.9701023101806641, evaluation loss: 0.9959768056869507\n",
      "297/3000 training loss: 0.9676565527915955, evaluation loss: 0.9935257434844971\n",
      "298/3000 training loss: 0.9652215838432312, evaluation loss: 0.991085946559906\n",
      "299/3000 training loss: 0.9627975225448608, evaluation loss: 0.9886576533317566\n",
      "300/3000 training loss: 0.9603844285011292, evaluation loss: 0.986240804195404\n",
      "301/3000 training loss: 0.9579823017120361, evaluation loss: 0.9838355183601379\n",
      "302/3000 training loss: 0.9555912613868713, evaluation loss: 0.9814417958259583\n",
      "303/3000 training loss: 0.9532113671302795, evaluation loss: 0.9790596961975098\n",
      "304/3000 training loss: 0.950842559337616, evaluation loss: 0.9766892790794373\n",
      "305/3000 training loss: 0.9484848976135254, evaluation loss: 0.974330484867096\n",
      "306/3000 training loss: 0.9461384415626526, evaluation loss: 0.9719834923744202\n",
      "307/3000 training loss: 0.9438032507896423, evaluation loss: 0.9696481823921204\n",
      "308/3000 training loss: 0.9414793252944946, evaluation loss: 0.9673247337341309\n",
      "309/3000 training loss: 0.9391666650772095, evaluation loss: 0.9650129079818726\n",
      "310/3000 training loss: 0.9368653297424316, evaluation loss: 0.9627130031585693\n",
      "311/3000 training loss: 0.9345752596855164, evaluation loss: 0.9604249000549316\n",
      "312/3000 training loss: 0.932296633720398, evaluation loss: 0.9581486582756042\n",
      "313/3000 training loss: 0.9300292730331421, evaluation loss: 0.9558840990066528\n",
      "314/3000 training loss: 0.9277730584144592, evaluation loss: 0.953631579875946\n",
      "315/3000 training loss: 0.925528347492218, evaluation loss: 0.9513907432556152\n",
      "316/3000 training loss: 0.9232949614524841, evaluation loss: 0.9491618275642395\n",
      "317/3000 training loss: 0.9210729002952576, evaluation loss: 0.9469446539878845\n",
      "318/3000 training loss: 0.9188621640205383, evaluation loss: 0.9447392821311951\n",
      "319/3000 training loss: 0.9166627526283264, evaluation loss: 0.9425457715988159\n",
      "320/3000 training loss: 0.914474606513977, evaluation loss: 0.9403640031814575\n",
      "321/3000 training loss: 0.9122976660728455, evaluation loss: 0.9381939172744751\n",
      "322/3000 training loss: 0.9101320505142212, evaluation loss: 0.9360356330871582\n",
      "323/3000 training loss: 0.9079776406288147, evaluation loss: 0.9338890314102173\n",
      "324/3000 training loss: 0.9058345556259155, evaluation loss: 0.9317540526390076\n",
      "325/3000 training loss: 0.903702437877655, evaluation loss: 0.929630696773529\n",
      "326/3000 training loss: 0.9015815854072571, evaluation loss: 0.9275189638137817\n",
      "327/3000 training loss: 0.8994718194007874, evaluation loss: 0.9254187345504761\n",
      "328/3000 training loss: 0.8973730802536011, evaluation loss: 0.9233301281929016\n",
      "329/3000 training loss: 0.8952853679656982, evaluation loss: 0.9212529063224792\n",
      "330/3000 training loss: 0.8932086229324341, evaluation loss: 0.9191871881484985\n",
      "331/3000 training loss: 0.8911428451538086, evaluation loss: 0.9171327948570251\n",
      "332/3000 training loss: 0.8890878558158875, evaluation loss: 0.9150896668434143\n",
      "333/3000 training loss: 0.887043833732605, evaluation loss: 0.913057804107666\n",
      "334/3000 training loss: 0.8850104808807373, evaluation loss: 0.9110370874404907\n",
      "335/3000 training loss: 0.8829878568649292, evaluation loss: 0.909027636051178\n",
      "336/3000 training loss: 0.8809758424758911, evaluation loss: 0.9070292115211487\n",
      "337/3000 training loss: 0.8789745569229126, evaluation loss: 0.9050418138504028\n",
      "338/3000 training loss: 0.8769837021827698, evaluation loss: 0.9030653238296509\n",
      "339/3000 training loss: 0.8750033974647522, evaluation loss: 0.901099681854248\n",
      "340/3000 training loss: 0.8730334043502808, evaluation loss: 0.8991448283195496\n",
      "341/3000 training loss: 0.871073842048645, evaluation loss: 0.8972007632255554\n",
      "342/3000 training loss: 0.8691244125366211, evaluation loss: 0.8952673673629761\n",
      "343/3000 training loss: 0.8671852350234985, evaluation loss: 0.893344521522522\n",
      "344/3000 training loss: 0.8652561902999878, evaluation loss: 0.8914321064949036\n",
      "345/3000 training loss: 0.8633371591567993, evaluation loss: 0.8895301818847656\n",
      "346/3000 training loss: 0.8614280819892883, evaluation loss: 0.8876385688781738\n",
      "347/3000 training loss: 0.8595290184020996, evaluation loss: 0.8857572078704834\n",
      "348/3000 training loss: 0.8576396107673645, evaluation loss: 0.8838860392570496\n",
      "349/3000 training loss: 0.8557599782943726, evaluation loss: 0.8820250034332275\n",
      "350/3000 training loss: 0.8538900017738342, evaluation loss: 0.8801738619804382\n",
      "351/3000 training loss: 0.85202956199646, evaluation loss: 0.8783326148986816\n",
      "352/3000 training loss: 0.8501786589622498, evaluation loss: 0.8765013217926025\n",
      "353/3000 training loss: 0.8483370542526245, evaluation loss: 0.8746795654296875\n",
      "354/3000 training loss: 0.8465048670768738, evaluation loss: 0.8728676438331604\n",
      "355/3000 training loss: 0.8446817994117737, evaluation loss: 0.8710651993751526\n",
      "356/3000 training loss: 0.8428679704666138, evaluation loss: 0.8692722320556641\n",
      "357/3000 training loss: 0.8410630226135254, evaluation loss: 0.8674885034561157\n",
      "358/3000 training loss: 0.8392671346664429, evaluation loss: 0.8657141327857971\n",
      "359/3000 training loss: 0.8374800086021423, evaluation loss: 0.8639490008354187\n",
      "360/3000 training loss: 0.8357017636299133, evaluation loss: 0.8621929287910461\n",
      "361/3000 training loss: 0.8339322209358215, evaluation loss: 0.8604457378387451\n",
      "362/3000 training loss: 0.8321712017059326, evaluation loss: 0.8587075471878052\n",
      "363/3000 training loss: 0.8304187655448914, evaluation loss: 0.856978178024292\n",
      "364/3000 training loss: 0.8286746740341187, evaluation loss: 0.8552574515342712\n",
      "365/3000 training loss: 0.8269388675689697, evaluation loss: 0.8535453677177429\n",
      "366/3000 training loss: 0.8252114057540894, evaluation loss: 0.8518417477607727\n",
      "367/3000 training loss: 0.8234920501708984, evaluation loss: 0.8501466512680054\n",
      "368/3000 training loss: 0.8217807412147522, evaluation loss: 0.848459780216217\n",
      "369/3000 training loss: 0.8200774192810059, evaluation loss: 0.8467811942100525\n",
      "370/3000 training loss: 0.8183819651603699, evaluation loss: 0.8451106548309326\n",
      "371/3000 training loss: 0.8166942596435547, evaluation loss: 0.843448281288147\n",
      "372/3000 training loss: 0.8150142431259155, evaluation loss: 0.8417938351631165\n",
      "373/3000 training loss: 0.8133419156074524, evaluation loss: 0.8401472568511963\n",
      "374/3000 training loss: 0.8116770386695862, evaluation loss: 0.8385084271430969\n",
      "375/3000 training loss: 0.8100196123123169, evaluation loss: 0.8368772864341736\n",
      "376/3000 training loss: 0.8083695769309998, evaluation loss: 0.8352537751197815\n",
      "377/3000 training loss: 0.8067267537117004, evaluation loss: 0.8336376547813416\n",
      "378/3000 training loss: 0.8050910830497742, evaluation loss: 0.8320290446281433\n",
      "379/3000 training loss: 0.8034625053405762, evaluation loss: 0.8304277062416077\n",
      "380/3000 training loss: 0.8018409609794617, evaluation loss: 0.8288335800170898\n",
      "381/3000 training loss: 0.8002263307571411, evaluation loss: 0.8272465467453003\n",
      "382/3000 training loss: 0.7986185550689697, evaluation loss: 0.825666606426239\n",
      "383/3000 training loss: 0.797017514705658, evaluation loss: 0.8240936398506165\n",
      "384/3000 training loss: 0.7954232096672058, evaluation loss: 0.8225275874137878\n",
      "385/3000 training loss: 0.7938354015350342, evaluation loss: 0.8209683299064636\n",
      "386/3000 training loss: 0.7922540903091431, evaluation loss: 0.819415807723999\n",
      "387/3000 training loss: 0.790679395198822, evaluation loss: 0.8178699016571045\n",
      "388/3000 training loss: 0.7891108393669128, evaluation loss: 0.8163304924964905\n",
      "389/3000 training loss: 0.7875486612319946, evaluation loss: 0.8147976398468018\n",
      "390/3000 training loss: 0.7859926223754883, evaluation loss: 0.8132710456848145\n",
      "391/3000 training loss: 0.7844427824020386, evaluation loss: 0.8117508292198181\n",
      "392/3000 training loss: 0.782899022102356, evaluation loss: 0.810236930847168\n",
      "393/3000 training loss: 0.7813612818717957, evaluation loss: 0.8087291121482849\n",
      "394/3000 training loss: 0.7798293232917786, evaluation loss: 0.807227373123169\n",
      "395/3000 training loss: 0.7783032059669495, evaluation loss: 0.8057316541671753\n",
      "396/3000 training loss: 0.7767829298973083, evaluation loss: 0.8042418956756592\n",
      "397/3000 training loss: 0.7752683758735657, evaluation loss: 0.802757978439331\n",
      "398/3000 training loss: 0.7737594842910767, evaluation loss: 0.8012798428535461\n",
      "399/3000 training loss: 0.7722560167312622, evaluation loss: 0.7998074293136597\n",
      "400/3000 training loss: 0.7707580924034119, evaluation loss: 0.7983406782150269\n",
      "401/3000 training loss: 0.7692656517028809, evaluation loss: 0.7968795895576477\n",
      "402/3000 training loss: 0.7677785754203796, evaluation loss: 0.7954239249229431\n",
      "403/3000 training loss: 0.7662967443466187, evaluation loss: 0.7939736843109131\n",
      "404/3000 training loss: 0.7648201584815979, evaluation loss: 0.7925288677215576\n",
      "405/3000 training loss: 0.7633488774299622, evaluation loss: 0.7910894751548767\n",
      "406/3000 training loss: 0.7618826627731323, evaluation loss: 0.7896552681922913\n",
      "407/3000 training loss: 0.7604213953018188, evaluation loss: 0.788226306438446\n",
      "408/3000 training loss: 0.7589653134346008, evaluation loss: 0.7868024110794067\n",
      "409/3000 training loss: 0.7575141191482544, evaluation loss: 0.7853835225105286\n",
      "410/3000 training loss: 0.7560678124427795, evaluation loss: 0.7839698195457458\n",
      "411/3000 training loss: 0.7546263337135315, evaluation loss: 0.7825610637664795\n",
      "412/3000 training loss: 0.7531896829605103, evaluation loss: 0.7811571955680847\n",
      "413/3000 training loss: 0.7517577409744263, evaluation loss: 0.779758095741272\n",
      "414/3000 training loss: 0.7503305077552795, evaluation loss: 0.7783638834953308\n",
      "415/3000 training loss: 0.7489078044891357, evaluation loss: 0.7769743800163269\n",
      "416/3000 training loss: 0.7474898099899292, evaluation loss: 0.775589644908905\n",
      "417/3000 training loss: 0.7460762858390808, evaluation loss: 0.7742094993591309\n",
      "418/3000 training loss: 0.7446672320365906, evaluation loss: 0.7728339433670044\n",
      "419/3000 training loss: 0.7432627081871033, evaluation loss: 0.7714629173278809\n",
      "420/3000 training loss: 0.7418624758720398, evaluation loss: 0.7700964212417603\n",
      "421/3000 training loss: 0.7404666543006897, evaluation loss: 0.768734335899353\n",
      "422/3000 training loss: 0.7390751838684082, evaluation loss: 0.7673766613006592\n",
      "423/3000 training loss: 0.7376878261566162, evaluation loss: 0.7660233378410339\n",
      "424/3000 training loss: 0.7363048195838928, evaluation loss: 0.7646743655204773\n",
      "425/3000 training loss: 0.7349259257316589, evaluation loss: 0.7633296847343445\n",
      "426/3000 training loss: 0.7335511445999146, evaluation loss: 0.7619891166687012\n",
      "427/3000 training loss: 0.7321804165840149, evaluation loss: 0.7606528401374817\n",
      "428/3000 training loss: 0.73081374168396, evaluation loss: 0.7593206167221069\n",
      "429/3000 training loss: 0.7294511795043945, evaluation loss: 0.7579925060272217\n",
      "430/3000 training loss: 0.728092610836029, evaluation loss: 0.7566685080528259\n",
      "431/3000 training loss: 0.7267378568649292, evaluation loss: 0.7553484439849854\n",
      "432/3000 training loss: 0.7253870368003845, evaluation loss: 0.7540323734283447\n",
      "433/3000 training loss: 0.7240400910377502, evaluation loss: 0.752720296382904\n",
      "434/3000 training loss: 0.7226970195770264, evaluation loss: 0.751412034034729\n",
      "435/3000 training loss: 0.7213576436042786, evaluation loss: 0.7501077055931091\n",
      "436/3000 training loss: 0.7200221419334412, evaluation loss: 0.7488073110580444\n",
      "437/3000 training loss: 0.7186902165412903, evaluation loss: 0.7475104928016663\n",
      "438/3000 training loss: 0.717362105846405, evaluation loss: 0.7462176084518433\n",
      "439/3000 training loss: 0.7160376310348511, evaluation loss: 0.7449283003807068\n",
      "440/3000 training loss: 0.7147167921066284, evaluation loss: 0.7436427474021912\n",
      "441/3000 training loss: 0.7133994698524475, evaluation loss: 0.7423608303070068\n",
      "442/3000 training loss: 0.7120857238769531, evaluation loss: 0.7410826086997986\n",
      "443/3000 training loss: 0.7107756733894348, evaluation loss: 0.7398079633712769\n",
      "444/3000 training loss: 0.7094689011573792, evaluation loss: 0.7385368943214417\n",
      "445/3000 training loss: 0.70816570520401, evaluation loss: 0.7372692823410034\n",
      "446/3000 training loss: 0.7068659663200378, evaluation loss: 0.7360053658485413\n",
      "447/3000 training loss: 0.7055696845054626, evaluation loss: 0.734744668006897\n",
      "448/3000 training loss: 0.7042768001556396, evaluation loss: 0.7334874868392944\n",
      "449/3000 training loss: 0.7029871940612793, evaluation loss: 0.7322338223457336\n",
      "450/3000 training loss: 0.7017009258270264, evaluation loss: 0.7309834957122803\n",
      "451/3000 training loss: 0.7004179954528809, evaluation loss: 0.7297365069389343\n",
      "452/3000 training loss: 0.6991384029388428, evaluation loss: 0.728492796421051\n",
      "453/3000 training loss: 0.6978619694709778, evaluation loss: 0.7272524237632751\n",
      "454/3000 training loss: 0.6965888142585754, evaluation loss: 0.7260153889656067\n",
      "455/3000 training loss: 0.6953189373016357, evaluation loss: 0.7247815728187561\n",
      "456/3000 training loss: 0.6940521597862244, evaluation loss: 0.7235510349273682\n",
      "457/3000 training loss: 0.6927886009216309, evaluation loss: 0.7223235964775085\n",
      "458/3000 training loss: 0.6915281414985657, evaluation loss: 0.721099317073822\n",
      "459/3000 training loss: 0.6902707815170288, evaluation loss: 0.7198782563209534\n",
      "460/3000 training loss: 0.6890165209770203, evaluation loss: 0.7186603546142578\n",
      "461/3000 training loss: 0.6877654194831848, evaluation loss: 0.7174454927444458\n",
      "462/3000 training loss: 0.6865172386169434, evaluation loss: 0.7162337899208069\n",
      "463/3000 training loss: 0.685272216796875, evaluation loss: 0.7150250673294067\n",
      "464/3000 training loss: 0.6840300559997559, evaluation loss: 0.7138193845748901\n",
      "465/3000 training loss: 0.6827910542488098, evaluation loss: 0.7126166820526123\n",
      "466/3000 training loss: 0.6815548539161682, evaluation loss: 0.711417019367218\n",
      "467/3000 training loss: 0.6803216934204102, evaluation loss: 0.7102203965187073\n",
      "468/3000 training loss: 0.6790913939476013, evaluation loss: 0.7090266346931458\n",
      "469/3000 training loss: 0.6778640151023865, evaluation loss: 0.7078357338905334\n",
      "470/3000 training loss: 0.6766395568847656, evaluation loss: 0.7066478133201599\n",
      "471/3000 training loss: 0.675417959690094, evaluation loss: 0.7054628133773804\n",
      "472/3000 training loss: 0.6741991639137268, evaluation loss: 0.70428067445755\n",
      "473/3000 training loss: 0.6729831695556641, evaluation loss: 0.7031012773513794\n",
      "474/3000 training loss: 0.6717700958251953, evaluation loss: 0.7019248008728027\n",
      "475/3000 training loss: 0.6705597043037415, evaluation loss: 0.700751006603241\n",
      "476/3000 training loss: 0.669352114200592, evaluation loss: 0.6995800733566284\n",
      "477/3000 training loss: 0.6681473255157471, evaluation loss: 0.6984119415283203\n",
      "478/3000 training loss: 0.6669453382492065, evaluation loss: 0.6972465515136719\n",
      "479/3000 training loss: 0.6657459139823914, evaluation loss: 0.6960838437080383\n",
      "480/3000 training loss: 0.6645491719245911, evaluation loss: 0.6949238181114197\n",
      "481/3000 training loss: 0.66335529088974, evaluation loss: 0.6937665343284607\n",
      "482/3000 training loss: 0.662164032459259, evaluation loss: 0.6926119327545166\n",
      "483/3000 training loss: 0.6609753370285034, evaluation loss: 0.6914598941802979\n",
      "484/3000 training loss: 0.6597893834114075, evaluation loss: 0.6903105974197388\n",
      "485/3000 training loss: 0.6586059331893921, evaluation loss: 0.689163863658905\n",
      "486/3000 training loss: 0.6574252247810364, evaluation loss: 0.6880197525024414\n",
      "487/3000 training loss: 0.6562469601631165, evaluation loss: 0.6868781447410583\n",
      "488/3000 training loss: 0.6550713777542114, evaluation loss: 0.6857391595840454\n",
      "489/3000 training loss: 0.6538983583450317, evaluation loss: 0.6846027374267578\n",
      "490/3000 training loss: 0.6527278423309326, evaluation loss: 0.6834688186645508\n",
      "491/3000 training loss: 0.6515598297119141, evaluation loss: 0.6823375225067139\n",
      "492/3000 training loss: 0.6503944396972656, evaluation loss: 0.6812085509300232\n",
      "493/3000 training loss: 0.6492314338684082, evaluation loss: 0.6800822019577026\n",
      "494/3000 training loss: 0.6480709910392761, evaluation loss: 0.6789582371711731\n",
      "495/3000 training loss: 0.6469130516052246, evaluation loss: 0.6778367161750793\n",
      "496/3000 training loss: 0.6457574963569641, evaluation loss: 0.6767176985740662\n",
      "497/3000 training loss: 0.6446043848991394, evaluation loss: 0.6756011247634888\n",
      "498/3000 training loss: 0.6434537768363953, evaluation loss: 0.6744868755340576\n",
      "499/3000 training loss: 0.6423054933547974, evaluation loss: 0.6733750700950623\n",
      "500/3000 training loss: 0.64115971326828, evaluation loss: 0.6722655892372131\n",
      "501/3000 training loss: 0.6400162577629089, evaluation loss: 0.671158492565155\n",
      "502/3000 training loss: 0.6388751864433289, evaluation loss: 0.6700537800788879\n",
      "503/3000 training loss: 0.6377364993095398, evaluation loss: 0.6689513921737671\n",
      "504/3000 training loss: 0.6366001963615417, evaluation loss: 0.6678512692451477\n",
      "505/3000 training loss: 0.6354662775993347, evaluation loss: 0.6667534112930298\n",
      "506/3000 training loss: 0.6343345642089844, evaluation loss: 0.6656579971313477\n",
      "507/3000 training loss: 0.633205235004425, evaluation loss: 0.6645647883415222\n",
      "508/3000 training loss: 0.6320781707763672, evaluation loss: 0.6634737849235535\n",
      "509/3000 training loss: 0.6309534907341003, evaluation loss: 0.6623850464820862\n",
      "510/3000 training loss: 0.6298310160636902, evaluation loss: 0.6612985730171204\n",
      "511/3000 training loss: 0.6287108659744263, evaluation loss: 0.660214364528656\n",
      "512/3000 training loss: 0.6275929808616638, evaluation loss: 0.6591323018074036\n",
      "513/3000 training loss: 0.6264773011207581, evaluation loss: 0.6580524444580078\n",
      "514/3000 training loss: 0.6253638863563538, evaluation loss: 0.6569747924804688\n",
      "515/3000 training loss: 0.6242527365684509, evaluation loss: 0.6558993458747864\n",
      "516/3000 training loss: 0.6231436729431152, evaluation loss: 0.6548259854316711\n",
      "517/3000 training loss: 0.622036874294281, evaluation loss: 0.6537547707557678\n",
      "518/3000 training loss: 0.620932400226593, evaluation loss: 0.6526857614517212\n",
      "519/3000 training loss: 0.6198300719261169, evaluation loss: 0.6516187787055969\n",
      "520/3000 training loss: 0.6187297701835632, evaluation loss: 0.6505540013313293\n",
      "521/3000 training loss: 0.6176317930221558, evaluation loss: 0.6494912505149841\n",
      "522/3000 training loss: 0.6165359020233154, evaluation loss: 0.6484306454658508\n",
      "523/3000 training loss: 0.6154420971870422, evaluation loss: 0.6473721265792847\n",
      "524/3000 training loss: 0.6143504977226257, evaluation loss: 0.6463155746459961\n",
      "525/3000 training loss: 0.6132610440254211, evaluation loss: 0.6452611684799194\n",
      "526/3000 training loss: 0.6121736764907837, evaluation loss: 0.6442087888717651\n",
      "527/3000 training loss: 0.6110884547233582, evaluation loss: 0.6431584358215332\n",
      "528/3000 training loss: 0.610005259513855, evaluation loss: 0.6421100497245789\n",
      "529/3000 training loss: 0.6089243292808533, evaluation loss: 0.6410637497901917\n",
      "530/3000 training loss: 0.6078453063964844, evaluation loss: 0.640019416809082\n",
      "531/3000 training loss: 0.6067683696746826, evaluation loss: 0.63897705078125\n",
      "532/3000 training loss: 0.6056935787200928, evaluation loss: 0.6379367113113403\n",
      "533/3000 training loss: 0.6046208143234253, evaluation loss: 0.6368982791900635\n",
      "534/3000 training loss: 0.6035500168800354, evaluation loss: 0.635861873626709\n",
      "535/3000 training loss: 0.6024813652038574, evaluation loss: 0.6348273754119873\n",
      "536/3000 training loss: 0.6014147400856018, evaluation loss: 0.6337948441505432\n",
      "537/3000 training loss: 0.6003501415252686, evaluation loss: 0.6327642798423767\n",
      "538/3000 training loss: 0.5992875099182129, evaluation loss: 0.6317355036735535\n",
      "539/3000 training loss: 0.5982268452644348, evaluation loss: 0.6307087540626526\n",
      "540/3000 training loss: 0.5971682667732239, evaluation loss: 0.6296838521957397\n",
      "541/3000 training loss: 0.5961116552352905, evaluation loss: 0.6286608576774597\n",
      "542/3000 training loss: 0.5950570106506348, evaluation loss: 0.6276397705078125\n",
      "543/3000 training loss: 0.5940043330192566, evaluation loss: 0.6266205310821533\n",
      "544/3000 training loss: 0.5929536819458008, evaluation loss: 0.6256031394004822\n",
      "545/3000 training loss: 0.5919049978256226, evaluation loss: 0.6245876550674438\n",
      "546/3000 training loss: 0.5908581614494324, evaluation loss: 0.6235740184783936\n",
      "547/3000 training loss: 0.5898133516311646, evaluation loss: 0.6225621700286865\n",
      "548/3000 training loss: 0.5887705087661743, evaluation loss: 0.6215521693229675\n",
      "549/3000 training loss: 0.5877295136451721, evaluation loss: 0.6205440163612366\n",
      "550/3000 training loss: 0.5866904854774475, evaluation loss: 0.6195377111434937\n",
      "551/3000 training loss: 0.5856534838676453, evaluation loss: 0.618533194065094\n",
      "552/3000 training loss: 0.5846182703971863, evaluation loss: 0.6175304651260376\n",
      "553/3000 training loss: 0.5835850238800049, evaluation loss: 0.6165294647216797\n",
      "554/3000 training loss: 0.5825536251068115, evaluation loss: 0.6155303120613098\n",
      "555/3000 training loss: 0.5815241932868958, evaluation loss: 0.6145329475402832\n",
      "556/3000 training loss: 0.5804965496063232, evaluation loss: 0.6135373711585999\n",
      "557/3000 training loss: 0.5794709324836731, evaluation loss: 0.612543523311615\n",
      "558/3000 training loss: 0.5784471035003662, evaluation loss: 0.6115514636039734\n",
      "559/3000 training loss: 0.5774251818656921, evaluation loss: 0.610561192035675\n",
      "560/3000 training loss: 0.5764051079750061, evaluation loss: 0.6095725297927856\n",
      "561/3000 training loss: 0.5753868818283081, evaluation loss: 0.6085857152938843\n",
      "562/3000 training loss: 0.5743705630302429, evaluation loss: 0.6076005697250366\n",
      "563/3000 training loss: 0.573356032371521, evaluation loss: 0.6066171526908875\n",
      "564/3000 training loss: 0.5723433494567871, evaluation loss: 0.6056355237960815\n",
      "565/3000 training loss: 0.5713325142860413, evaluation loss: 0.6046554446220398\n",
      "566/3000 training loss: 0.5703235864639282, evaluation loss: 0.6036772131919861\n",
      "567/3000 training loss: 0.5693163275718689, evaluation loss: 0.6027007102966309\n",
      "568/3000 training loss: 0.5683110356330872, evaluation loss: 0.6017257571220398\n",
      "569/3000 training loss: 0.5673074722290039, evaluation loss: 0.6007526516914368\n",
      "570/3000 training loss: 0.5663058161735535, evaluation loss: 0.5997810959815979\n",
      "571/3000 training loss: 0.5653058886528015, evaluation loss: 0.5988112688064575\n",
      "572/3000 training loss: 0.5643078684806824, evaluation loss: 0.5978431105613708\n",
      "573/3000 training loss: 0.5633115172386169, evaluation loss: 0.5968766808509827\n",
      "574/3000 training loss: 0.5623170137405396, evaluation loss: 0.5959117412567139\n",
      "575/3000 training loss: 0.5613242387771606, evaluation loss: 0.5949486494064331\n",
      "576/3000 training loss: 0.5603333711624146, evaluation loss: 0.5939870476722717\n",
      "577/3000 training loss: 0.5593442320823669, evaluation loss: 0.5930272340774536\n",
      "578/3000 training loss: 0.5583568215370178, evaluation loss: 0.5920689105987549\n",
      "579/3000 training loss: 0.5573711395263672, evaluation loss: 0.5911123156547546\n",
      "580/3000 training loss: 0.5563873648643494, evaluation loss: 0.5901572704315186\n",
      "581/3000 training loss: 0.5554052591323853, evaluation loss: 0.5892038345336914\n",
      "582/3000 training loss: 0.5544250011444092, evaluation loss: 0.5882521867752075\n",
      "583/3000 training loss: 0.553446352481842, evaluation loss: 0.5873019695281982\n",
      "584/3000 training loss: 0.5524696111679077, evaluation loss: 0.5863534212112427\n",
      "585/3000 training loss: 0.5514944791793823, evaluation loss: 0.5854065418243408\n",
      "586/3000 training loss: 0.5505211353302002, evaluation loss: 0.5844611525535583\n",
      "587/3000 training loss: 0.5495495796203613, evaluation loss: 0.5835174918174744\n",
      "588/3000 training loss: 0.5485796928405762, evaluation loss: 0.5825753211975098\n",
      "589/3000 training loss: 0.5476115942001343, evaluation loss: 0.5816347599029541\n",
      "590/3000 training loss: 0.5466451644897461, evaluation loss: 0.5806958079338074\n",
      "591/3000 training loss: 0.5456804037094116, evaluation loss: 0.57975834608078\n",
      "592/3000 training loss: 0.5447174906730652, evaluation loss: 0.5788224935531616\n",
      "593/3000 training loss: 0.5437561869621277, evaluation loss: 0.5778883099555969\n",
      "594/3000 training loss: 0.5427967309951782, evaluation loss: 0.5769556760787964\n",
      "595/3000 training loss: 0.5418388843536377, evaluation loss: 0.5760244727134705\n",
      "596/3000 training loss: 0.5408827066421509, evaluation loss: 0.5750949382781982\n",
      "597/3000 training loss: 0.5399283170700073, evaluation loss: 0.5741668939590454\n",
      "598/3000 training loss: 0.5389755964279175, evaluation loss: 0.5732404589653015\n",
      "599/3000 training loss: 0.5380244851112366, evaluation loss: 0.5723155736923218\n",
      "600/3000 training loss: 0.5370752215385437, evaluation loss: 0.5713922381401062\n",
      "601/3000 training loss: 0.536127507686615, evaluation loss: 0.57047039270401\n",
      "602/3000 training loss: 0.5351815223693848, evaluation loss: 0.569550096988678\n",
      "603/3000 training loss: 0.5342372059822083, evaluation loss: 0.5686314105987549\n",
      "604/3000 training loss: 0.533294677734375, evaluation loss: 0.5677140951156616\n",
      "605/3000 training loss: 0.5323536396026611, evaluation loss: 0.5667984485626221\n",
      "606/3000 training loss: 0.5314143896102905, evaluation loss: 0.5658843517303467\n",
      "607/3000 training loss: 0.5304767489433289, evaluation loss: 0.5649716258049011\n",
      "608/3000 training loss: 0.5295407772064209, evaluation loss: 0.564060628414154\n",
      "609/3000 training loss: 0.5286065340042114, evaluation loss: 0.5631510019302368\n",
      "610/3000 training loss: 0.5276739001274109, evaluation loss: 0.562242865562439\n",
      "611/3000 training loss: 0.5267428755760193, evaluation loss: 0.56133633852005\n",
      "612/3000 training loss: 0.525813639163971, evaluation loss: 0.5604312419891357\n",
      "613/3000 training loss: 0.524885892868042, evaluation loss: 0.5595276951789856\n",
      "614/3000 training loss: 0.5239598751068115, evaluation loss: 0.5586256384849548\n",
      "615/3000 training loss: 0.5230355262756348, evaluation loss: 0.5577250719070435\n",
      "616/3000 training loss: 0.5221127271652222, evaluation loss: 0.5568259954452515\n",
      "617/3000 training loss: 0.5211916565895081, evaluation loss: 0.5559284687042236\n",
      "618/3000 training loss: 0.5202721953392029, evaluation loss: 0.5550323724746704\n",
      "619/3000 training loss: 0.5193543434143066, evaluation loss: 0.5541377663612366\n",
      "620/3000 training loss: 0.5184381008148193, evaluation loss: 0.5532447695732117\n",
      "621/3000 training loss: 0.5175235271453857, evaluation loss: 0.5523530840873718\n",
      "622/3000 training loss: 0.5166105031967163, evaluation loss: 0.5514629483222961\n",
      "623/3000 training loss: 0.5156992077827454, evaluation loss: 0.5505742430686951\n",
      "624/3000 training loss: 0.5147894024848938, evaluation loss: 0.5496870875358582\n",
      "625/3000 training loss: 0.5138813257217407, evaluation loss: 0.5488014221191406\n",
      "626/3000 training loss: 0.5129747986793518, evaluation loss: 0.5479171872138977\n",
      "627/3000 training loss: 0.5120699405670166, evaluation loss: 0.5470344424247742\n",
      "628/3000 training loss: 0.5111666917800903, evaluation loss: 0.5461531281471252\n",
      "629/3000 training loss: 0.5102649927139282, evaluation loss: 0.5452733039855957\n",
      "630/3000 training loss: 0.509364902973175, evaluation loss: 0.5443949103355408\n",
      "631/3000 training loss: 0.508466362953186, evaluation loss: 0.54351806640625\n",
      "632/3000 training loss: 0.5075694918632507, evaluation loss: 0.5426426529884338\n",
      "633/3000 training loss: 0.5066742300987244, evaluation loss: 0.5417685508728027\n",
      "634/3000 training loss: 0.5057805180549622, evaluation loss: 0.5408959984779358\n",
      "635/3000 training loss: 0.5048883557319641, evaluation loss: 0.5400249361991882\n",
      "636/3000 training loss: 0.5039978623390198, evaluation loss: 0.5391553640365601\n",
      "637/3000 training loss: 0.5031088590621948, evaluation loss: 0.5382872223854065\n",
      "638/3000 training loss: 0.5022215247154236, evaluation loss: 0.537420392036438\n",
      "639/3000 training loss: 0.5013357400894165, evaluation loss: 0.5365551114082336\n",
      "640/3000 training loss: 0.5004515051841736, evaluation loss: 0.5356912016868591\n",
      "641/3000 training loss: 0.4995688498020172, evaluation loss: 0.5348289012908936\n",
      "642/3000 training loss: 0.4986877739429474, evaluation loss: 0.5339678525924683\n",
      "643/3000 training loss: 0.4978082776069641, evaluation loss: 0.5331082940101624\n",
      "644/3000 training loss: 0.4969303607940674, evaluation loss: 0.5322502255439758\n",
      "645/3000 training loss: 0.4960539937019348, evaluation loss: 0.5313935875892639\n",
      "646/3000 training loss: 0.4951792061328888, evaluation loss: 0.5305383205413818\n",
      "647/3000 training loss: 0.49430590867996216, evaluation loss: 0.5296844840049744\n",
      "648/3000 training loss: 0.49343422055244446, evaluation loss: 0.5288320779800415\n",
      "649/3000 training loss: 0.49256405234336853, evaluation loss: 0.5279811024665833\n",
      "650/3000 training loss: 0.49169546365737915, evaluation loss: 0.5271315574645996\n",
      "651/3000 training loss: 0.49082839488983154, evaluation loss: 0.5262834429740906\n",
      "652/3000 training loss: 0.48996293544769287, evaluation loss: 0.5254366993904114\n",
      "653/3000 training loss: 0.4890989661216736, evaluation loss: 0.5245913863182068\n",
      "654/3000 training loss: 0.48823651671409607, evaluation loss: 0.5237476229667664\n",
      "655/3000 training loss: 0.4873756766319275, evaluation loss: 0.5229051113128662\n",
      "656/3000 training loss: 0.4865163266658783, evaluation loss: 0.5220640897750854\n",
      "657/3000 training loss: 0.4856584668159485, evaluation loss: 0.5212244391441345\n",
      "658/3000 training loss: 0.4848021864891052, evaluation loss: 0.5203861594200134\n",
      "659/3000 training loss: 0.48394742608070374, evaluation loss: 0.5195493698120117\n",
      "660/3000 training loss: 0.4830942153930664, evaluation loss: 0.5187138915061951\n",
      "661/3000 training loss: 0.48224252462387085, evaluation loss: 0.5178799033164978\n",
      "662/3000 training loss: 0.4813922941684723, evaluation loss: 0.5170472860336304\n",
      "663/3000 training loss: 0.4805436432361603, evaluation loss: 0.5162160396575928\n",
      "664/3000 training loss: 0.47969651222229004, evaluation loss: 0.5153862237930298\n",
      "665/3000 training loss: 0.4788508415222168, evaluation loss: 0.5145577788352966\n",
      "666/3000 training loss: 0.4780067205429077, evaluation loss: 0.5137307643890381\n",
      "667/3000 training loss: 0.477164089679718, evaluation loss: 0.5129051208496094\n",
      "668/3000 training loss: 0.4763229787349701, evaluation loss: 0.5120808482170105\n",
      "669/3000 training loss: 0.47548341751098633, evaluation loss: 0.5112579464912415\n",
      "670/3000 training loss: 0.4746452569961548, evaluation loss: 0.5104364156723022\n",
      "671/3000 training loss: 0.4738086760044098, evaluation loss: 0.5096163749694824\n",
      "672/3000 training loss: 0.4729735553264618, evaluation loss: 0.5087976455688477\n",
      "673/3000 training loss: 0.47213995456695557, evaluation loss: 0.5079802870750427\n",
      "674/3000 training loss: 0.47130778431892395, evaluation loss: 0.5071642994880676\n",
      "675/3000 training loss: 0.4704771935939789, evaluation loss: 0.5063497424125671\n",
      "676/3000 training loss: 0.4696480333805084, evaluation loss: 0.5055364966392517\n",
      "677/3000 training loss: 0.46882036328315735, evaluation loss: 0.5047246813774109\n",
      "678/3000 training loss: 0.46799421310424805, evaluation loss: 0.5039141774177551\n",
      "679/3000 training loss: 0.46716949343681335, evaluation loss: 0.503105103969574\n",
      "680/3000 training loss: 0.46634626388549805, evaluation loss: 0.5022973418235779\n",
      "681/3000 training loss: 0.4655245244503021, evaluation loss: 0.5014909505844116\n",
      "682/3000 training loss: 0.4647042155265808, evaluation loss: 0.50068598985672\n",
      "683/3000 training loss: 0.46388542652130127, evaluation loss: 0.4998823404312134\n",
      "684/3000 training loss: 0.4630680978298187, evaluation loss: 0.49908003211021423\n",
      "685/3000 training loss: 0.4622521996498108, evaluation loss: 0.49827906489372253\n",
      "686/3000 training loss: 0.46143779158592224, evaluation loss: 0.49747946858406067\n",
      "687/3000 training loss: 0.4606248438358307, evaluation loss: 0.49668124318122864\n",
      "688/3000 training loss: 0.45981332659721375, evaluation loss: 0.49588438868522644\n",
      "689/3000 training loss: 0.4590032696723938, evaluation loss: 0.4950889050960541\n",
      "690/3000 training loss: 0.45819470286369324, evaluation loss: 0.494294673204422\n",
      "691/3000 training loss: 0.4573875069618225, evaluation loss: 0.4935018718242645\n",
      "692/3000 training loss: 0.45658183097839355, evaluation loss: 0.4927103817462921\n",
      "693/3000 training loss: 0.4557775557041168, evaluation loss: 0.49192023277282715\n",
      "694/3000 training loss: 0.4549747407436371, evaluation loss: 0.49113139510154724\n",
      "695/3000 training loss: 0.45417341589927673, evaluation loss: 0.49034392833709717\n",
      "696/3000 training loss: 0.4533734619617462, evaluation loss: 0.48955780267715454\n",
      "697/3000 training loss: 0.4525749385356903, evaluation loss: 0.488772988319397\n",
      "698/3000 training loss: 0.4517778754234314, evaluation loss: 0.48798954486846924\n",
      "699/3000 training loss: 0.4509822726249695, evaluation loss: 0.4872073829174042\n",
      "700/3000 training loss: 0.4501880407333374, evaluation loss: 0.48642656207084656\n",
      "701/3000 training loss: 0.4493952989578247, evaluation loss: 0.4856471121311188\n",
      "702/3000 training loss: 0.44860389828681946, evaluation loss: 0.4848689138889313\n",
      "703/3000 training loss: 0.4478139877319336, evaluation loss: 0.4840920865535736\n",
      "704/3000 training loss: 0.44702544808387756, evaluation loss: 0.483316570520401\n",
      "705/3000 training loss: 0.44623833894729614, evaluation loss: 0.48254233598709106\n",
      "706/3000 training loss: 0.44545263051986694, evaluation loss: 0.48176947236061096\n",
      "707/3000 training loss: 0.44466835260391235, evaluation loss: 0.48099786043167114\n",
      "708/3000 training loss: 0.4438855051994324, evaluation loss: 0.48022761940956116\n",
      "709/3000 training loss: 0.44310399889945984, evaluation loss: 0.47945863008499146\n",
      "710/3000 training loss: 0.4423239529132843, evaluation loss: 0.4786909520626068\n",
      "711/3000 training loss: 0.4415452480316162, evaluation loss: 0.4779246151447296\n",
      "712/3000 training loss: 0.44076797366142273, evaluation loss: 0.4771595597267151\n",
      "713/3000 training loss: 0.4399920701980591, evaluation loss: 0.4763958752155304\n",
      "714/3000 training loss: 0.43921756744384766, evaluation loss: 0.4756333827972412\n",
      "715/3000 training loss: 0.4384445250034332, evaluation loss: 0.4748722314834595\n",
      "716/3000 training loss: 0.43767282366752625, evaluation loss: 0.4741123616695404\n",
      "717/3000 training loss: 0.4369024634361267, evaluation loss: 0.473353773355484\n",
      "718/3000 training loss: 0.4361335039138794, evaluation loss: 0.47259652614593506\n",
      "719/3000 training loss: 0.4353659451007843, evaluation loss: 0.4718405306339264\n",
      "720/3000 training loss: 0.43459975719451904, evaluation loss: 0.4710858166217804\n",
      "721/3000 training loss: 0.4338349401950836, evaluation loss: 0.47033241391181946\n",
      "722/3000 training loss: 0.433071494102478, evaluation loss: 0.4695802628993988\n",
      "723/3000 training loss: 0.43230944871902466, evaluation loss: 0.4688293933868408\n",
      "724/3000 training loss: 0.43154874444007874, evaluation loss: 0.4680798053741455\n",
      "725/3000 training loss: 0.43078941106796265, evaluation loss: 0.4673314690589905\n",
      "726/3000 training loss: 0.4300314486026764, evaluation loss: 0.4665844440460205\n",
      "727/3000 training loss: 0.4292747974395752, evaluation loss: 0.46583864092826843\n",
      "728/3000 training loss: 0.42851951718330383, evaluation loss: 0.46509408950805664\n",
      "729/3000 training loss: 0.4277656674385071, evaluation loss: 0.4643509089946747\n",
      "730/3000 training loss: 0.4270130693912506, evaluation loss: 0.46360889077186584\n",
      "731/3000 training loss: 0.42626187205314636, evaluation loss: 0.46286818385124207\n",
      "732/3000 training loss: 0.42551204562187195, evaluation loss: 0.4621286988258362\n",
      "733/3000 training loss: 0.4247635304927826, evaluation loss: 0.4613904654979706\n",
      "734/3000 training loss: 0.4240163564682007, evaluation loss: 0.46065354347229004\n",
      "735/3000 training loss: 0.423270583152771, evaluation loss: 0.459917813539505\n",
      "736/3000 training loss: 0.4225260615348816, evaluation loss: 0.45918336510658264\n",
      "737/3000 training loss: 0.421782910823822, evaluation loss: 0.4584501385688782\n",
      "738/3000 training loss: 0.4210410714149475, evaluation loss: 0.4577181935310364\n",
      "739/3000 training loss: 0.42030057311058044, evaluation loss: 0.4569874405860901\n",
      "740/3000 training loss: 0.4195614457130432, evaluation loss: 0.45625796914100647\n",
      "741/3000 training loss: 0.41882359981536865, evaluation loss: 0.45552971959114075\n",
      "742/3000 training loss: 0.41808709502220154, evaluation loss: 0.4548026919364929\n",
      "743/3000 training loss: 0.4173519015312195, evaluation loss: 0.4540769159793854\n",
      "744/3000 training loss: 0.4166180193424225, evaluation loss: 0.4533523619174957\n",
      "745/3000 training loss: 0.4158855080604553, evaluation loss: 0.452629029750824\n",
      "746/3000 training loss: 0.41515421867370605, evaluation loss: 0.4519069492816925\n",
      "747/3000 training loss: 0.414424329996109, evaluation loss: 0.45118609070777893\n",
      "748/3000 training loss: 0.41369569301605225, evaluation loss: 0.4504663944244385\n",
      "749/3000 training loss: 0.41296839714050293, evaluation loss: 0.4497479796409607\n",
      "750/3000 training loss: 0.4122423827648163, evaluation loss: 0.4490307569503784\n",
      "751/3000 training loss: 0.4115176498889923, evaluation loss: 0.44831475615501404\n",
      "752/3000 training loss: 0.4107942283153534, evaluation loss: 0.44759994745254517\n",
      "753/3000 training loss: 0.41007211804389954, evaluation loss: 0.4468863904476166\n",
      "754/3000 training loss: 0.40935131907463074, evaluation loss: 0.4461739957332611\n",
      "755/3000 training loss: 0.4086317718029022, evaluation loss: 0.44546282291412354\n",
      "756/3000 training loss: 0.40791356563568115, evaluation loss: 0.44475284218788147\n",
      "757/3000 training loss: 0.40719664096832275, evaluation loss: 0.4440441131591797\n",
      "758/3000 training loss: 0.40648096799850464, evaluation loss: 0.44333648681640625\n",
      "759/3000 training loss: 0.4057665467262268, evaluation loss: 0.4426301419734955\n",
      "760/3000 training loss: 0.4050534665584564, evaluation loss: 0.44192495942115784\n",
      "761/3000 training loss: 0.4043416678905487, evaluation loss: 0.4412209391593933\n",
      "762/3000 training loss: 0.40363115072250366, evaluation loss: 0.44051817059516907\n",
      "763/3000 training loss: 0.4029218554496765, evaluation loss: 0.43981653451919556\n",
      "764/3000 training loss: 0.4022138714790344, evaluation loss: 0.43911609053611755\n",
      "765/3000 training loss: 0.40150710940361023, evaluation loss: 0.43841680884361267\n",
      "766/3000 training loss: 0.4008016586303711, evaluation loss: 0.43771877884864807\n",
      "767/3000 training loss: 0.40009748935699463, evaluation loss: 0.4370218515396118\n",
      "768/3000 training loss: 0.39939454197883606, evaluation loss: 0.43632611632347107\n",
      "769/3000 training loss: 0.39869290590286255, evaluation loss: 0.43563154339790344\n",
      "770/3000 training loss: 0.39799249172210693, evaluation loss: 0.4349382221698761\n",
      "771/3000 training loss: 0.3972933292388916, evaluation loss: 0.4342460036277771\n",
      "772/3000 training loss: 0.39659541845321655, evaluation loss: 0.43355491757392883\n",
      "773/3000 training loss: 0.3958987593650818, evaluation loss: 0.43286508321762085\n",
      "774/3000 training loss: 0.3952034115791321, evaluation loss: 0.4321763515472412\n",
      "775/3000 training loss: 0.3945092558860779, evaluation loss: 0.4314887821674347\n",
      "776/3000 training loss: 0.3938163220882416, evaluation loss: 0.4308023750782013\n",
      "777/3000 training loss: 0.39312463998794556, evaluation loss: 0.43011710047721863\n",
      "778/3000 training loss: 0.3924342393875122, evaluation loss: 0.4294329881668091\n",
      "779/3000 training loss: 0.39174503087997437, evaluation loss: 0.42875006794929504\n",
      "780/3000 training loss: 0.3910570740699768, evaluation loss: 0.42806825041770935\n",
      "781/3000 training loss: 0.39037036895751953, evaluation loss: 0.4273875653743744\n",
      "782/3000 training loss: 0.38968491554260254, evaluation loss: 0.42670807242393494\n",
      "783/3000 training loss: 0.38900062441825867, evaluation loss: 0.42602968215942383\n",
      "784/3000 training loss: 0.38831764459609985, evaluation loss: 0.42535242438316345\n",
      "785/3000 training loss: 0.38763582706451416, evaluation loss: 0.4246762990951538\n",
      "786/3000 training loss: 0.38695526123046875, evaluation loss: 0.4240013659000397\n",
      "787/3000 training loss: 0.38627591729164124, evaluation loss: 0.4233275055885315\n",
      "788/3000 training loss: 0.38559776544570923, evaluation loss: 0.42265480756759644\n",
      "789/3000 training loss: 0.3849208652973175, evaluation loss: 0.4219832122325897\n",
      "790/3000 training loss: 0.3842451870441437, evaluation loss: 0.42131277918815613\n",
      "791/3000 training loss: 0.38357070088386536, evaluation loss: 0.4206434488296509\n",
      "792/3000 training loss: 0.38289740681648254, evaluation loss: 0.419975221157074\n",
      "793/3000 training loss: 0.38222536444664, evaluation loss: 0.4193080961704254\n",
      "794/3000 training loss: 0.3815544843673706, evaluation loss: 0.41864213347435\n",
      "795/3000 training loss: 0.3808848261833191, evaluation loss: 0.41797730326652527\n",
      "796/3000 training loss: 0.3802163600921631, evaluation loss: 0.4173135459423065\n",
      "797/3000 training loss: 0.37954914569854736, evaluation loss: 0.4166508615016937\n",
      "798/3000 training loss: 0.37888309359550476, evaluation loss: 0.41598933935165405\n",
      "799/3000 training loss: 0.37821826338768005, evaluation loss: 0.41532889008522034\n",
      "800/3000 training loss: 0.37755462527275085, evaluation loss: 0.41466960310935974\n",
      "801/3000 training loss: 0.3768921494483948, evaluation loss: 0.4140114188194275\n",
      "802/3000 training loss: 0.3762308955192566, evaluation loss: 0.4133542776107788\n",
      "803/3000 training loss: 0.37557077407836914, evaluation loss: 0.4126982092857361\n",
      "804/3000 training loss: 0.3749118745326996, evaluation loss: 0.4120432734489441\n",
      "805/3000 training loss: 0.3742541968822479, evaluation loss: 0.41138944029808044\n",
      "806/3000 training loss: 0.3735976815223694, evaluation loss: 0.41073673963546753\n",
      "807/3000 training loss: 0.37294235825538635, evaluation loss: 0.4100850820541382\n",
      "808/3000 training loss: 0.37228819727897644, evaluation loss: 0.4094345271587372\n",
      "809/3000 training loss: 0.37163519859313965, evaluation loss: 0.40878498554229736\n",
      "810/3000 training loss: 0.370983362197876, evaluation loss: 0.40813660621643066\n",
      "811/3000 training loss: 0.3703327775001526, evaluation loss: 0.4074892997741699\n",
      "812/3000 training loss: 0.36968329548835754, evaluation loss: 0.40684303641319275\n",
      "813/3000 training loss: 0.369035005569458, evaluation loss: 0.4061978757381439\n",
      "814/3000 training loss: 0.3683878779411316, evaluation loss: 0.40555378794670105\n",
      "815/3000 training loss: 0.3677419424057007, evaluation loss: 0.40491077303886414\n",
      "816/3000 training loss: 0.3670971393585205, evaluation loss: 0.4042688310146332\n",
      "817/3000 training loss: 0.36645349860191345, evaluation loss: 0.4036279618740082\n",
      "818/3000 training loss: 0.3658110201358795, evaluation loss: 0.40298816561698914\n",
      "819/3000 training loss: 0.3651697039604187, evaluation loss: 0.40234941244125366\n",
      "820/3000 training loss: 0.3645295202732086, evaluation loss: 0.40171173214912415\n",
      "821/3000 training loss: 0.3638905882835388, evaluation loss: 0.4010751247406006\n",
      "822/3000 training loss: 0.3632526993751526, evaluation loss: 0.4004395604133606\n",
      "823/3000 training loss: 0.36261603236198425, evaluation loss: 0.3998050391674042\n",
      "824/3000 training loss: 0.36198049783706665, evaluation loss: 0.3991715908050537\n",
      "825/3000 training loss: 0.3613460659980774, evaluation loss: 0.3985392451286316\n",
      "826/3000 training loss: 0.36071282625198364, evaluation loss: 0.39790788292884827\n",
      "827/3000 training loss: 0.360080748796463, evaluation loss: 0.3972776234149933\n",
      "828/3000 training loss: 0.3594497740268707, evaluation loss: 0.3966483473777771\n",
      "829/3000 training loss: 0.3588199317455292, evaluation loss: 0.39602017402648926\n",
      "830/3000 training loss: 0.35819125175476074, evaluation loss: 0.3953930139541626\n",
      "831/3000 training loss: 0.35756370425224304, evaluation loss: 0.3947668969631195\n",
      "832/3000 training loss: 0.35693731904029846, evaluation loss: 0.3941418528556824\n",
      "833/3000 training loss: 0.35631200671195984, evaluation loss: 0.3935178220272064\n",
      "834/3000 training loss: 0.35568785667419434, evaluation loss: 0.3928948640823364\n",
      "835/3000 training loss: 0.35506483912467957, evaluation loss: 0.39227285981178284\n",
      "836/3000 training loss: 0.3544429838657379, evaluation loss: 0.39165198802948\n",
      "837/3000 training loss: 0.3538222014904022, evaluation loss: 0.39103206992149353\n",
      "838/3000 training loss: 0.35320255160331726, evaluation loss: 0.39041322469711304\n",
      "839/3000 training loss: 0.35258400440216064, evaluation loss: 0.3897953927516937\n",
      "840/3000 training loss: 0.35196661949157715, evaluation loss: 0.38917863368988037\n",
      "841/3000 training loss: 0.3513503670692444, evaluation loss: 0.3885628283023834\n",
      "842/3000 training loss: 0.35073521733283997, evaluation loss: 0.38794809579849243\n",
      "843/3000 training loss: 0.3501211106777191, evaluation loss: 0.3873343765735626\n",
      "844/3000 training loss: 0.34950822591781616, evaluation loss: 0.386721670627594\n",
      "845/3000 training loss: 0.34889641404151917, evaluation loss: 0.38611000776290894\n",
      "846/3000 training loss: 0.3482857048511505, evaluation loss: 0.38549932837486267\n",
      "847/3000 training loss: 0.3476760983467102, evaluation loss: 0.3848896622657776\n",
      "848/3000 training loss: 0.34706759452819824, evaluation loss: 0.3842810392379761\n",
      "849/3000 training loss: 0.3464601933956146, evaluation loss: 0.38367339968681335\n",
      "850/3000 training loss: 0.3458539545536041, evaluation loss: 0.3830667734146118\n",
      "851/3000 training loss: 0.3452487587928772, evaluation loss: 0.38246116042137146\n",
      "852/3000 training loss: 0.3446446657180786, evaluation loss: 0.3818565905094147\n",
      "853/3000 training loss: 0.34404170513153076, evaluation loss: 0.3812530040740967\n",
      "854/3000 training loss: 0.34343981742858887, evaluation loss: 0.3806504011154175\n",
      "855/3000 training loss: 0.34283900260925293, evaluation loss: 0.38004881143569946\n",
      "856/3000 training loss: 0.34223929047584534, evaluation loss: 0.379448264837265\n",
      "857/3000 training loss: 0.3416406810283661, evaluation loss: 0.3788486421108246\n",
      "858/3000 training loss: 0.3410432040691376, evaluation loss: 0.37825003266334534\n",
      "859/3000 training loss: 0.340446799993515, evaluation loss: 0.37765246629714966\n",
      "860/3000 training loss: 0.3398514688014984, evaluation loss: 0.3770558834075928\n",
      "861/3000 training loss: 0.3392571806907654, evaluation loss: 0.3764602839946747\n",
      "862/3000 training loss: 0.3386640250682831, evaluation loss: 0.3758656680583954\n",
      "863/3000 training loss: 0.33807191252708435, evaluation loss: 0.3752720355987549\n",
      "864/3000 training loss: 0.33748093247413635, evaluation loss: 0.3746793568134308\n",
      "865/3000 training loss: 0.3368909955024719, evaluation loss: 0.37408775091171265\n",
      "866/3000 training loss: 0.33630213141441345, evaluation loss: 0.3734970986843109\n",
      "867/3000 training loss: 0.33571434020996094, evaluation loss: 0.3729074001312256\n",
      "868/3000 training loss: 0.33512765169143677, evaluation loss: 0.37231874465942383\n",
      "869/3000 training loss: 0.33454203605651855, evaluation loss: 0.3717309832572937\n",
      "870/3000 training loss: 0.3339574635028839, evaluation loss: 0.37114429473876953\n",
      "871/3000 training loss: 0.3333739638328552, evaluation loss: 0.37055855989456177\n",
      "872/3000 training loss: 0.3327915668487549, evaluation loss: 0.3699737787246704\n",
      "873/3000 training loss: 0.3322102129459381, evaluation loss: 0.36938998103141785\n",
      "874/3000 training loss: 0.3316299021244049, evaluation loss: 0.36880719661712646\n",
      "875/3000 training loss: 0.33105069398880005, evaluation loss: 0.3682253062725067\n",
      "876/3000 training loss: 0.33047249913215637, evaluation loss: 0.36764442920684814\n",
      "877/3000 training loss: 0.32989537715911865, evaluation loss: 0.36706453561782837\n",
      "878/3000 training loss: 0.3293193578720093, evaluation loss: 0.3664855659008026\n",
      "879/3000 training loss: 0.3287443220615387, evaluation loss: 0.36590760946273804\n",
      "880/3000 training loss: 0.32817038893699646, evaluation loss: 0.36533060669898987\n",
      "881/3000 training loss: 0.3275975286960602, evaluation loss: 0.3647545874118805\n",
      "882/3000 training loss: 0.3270256817340851, evaluation loss: 0.3641795217990875\n",
      "883/3000 training loss: 0.32645487785339355, evaluation loss: 0.36360540986061096\n",
      "884/3000 training loss: 0.3258851170539856, evaluation loss: 0.3630322217941284\n",
      "885/3000 training loss: 0.3253164291381836, evaluation loss: 0.36246007680892944\n",
      "886/3000 training loss: 0.32474878430366516, evaluation loss: 0.3618888258934021\n",
      "887/3000 training loss: 0.3241821527481079, evaluation loss: 0.36131855845451355\n",
      "888/3000 training loss: 0.3236165940761566, evaluation loss: 0.360749214887619\n",
      "889/3000 training loss: 0.3230520188808441, evaluation loss: 0.3601808249950409\n",
      "890/3000 training loss: 0.32248857617378235, evaluation loss: 0.35961344838142395\n",
      "891/3000 training loss: 0.3219261169433594, evaluation loss: 0.3590470254421234\n",
      "892/3000 training loss: 0.32136470079421997, evaluation loss: 0.3584814667701721\n",
      "893/3000 training loss: 0.32080432772636414, evaluation loss: 0.3579168915748596\n",
      "894/3000 training loss: 0.3202449381351471, evaluation loss: 0.3573532998561859\n",
      "895/3000 training loss: 0.319686621427536, evaluation loss: 0.3567906320095062\n",
      "896/3000 training loss: 0.3191293776035309, evaluation loss: 0.35622888803482056\n",
      "897/3000 training loss: 0.31857308745384216, evaluation loss: 0.3556681275367737\n",
      "898/3000 training loss: 0.318017840385437, evaluation loss: 0.3551083207130432\n",
      "899/3000 training loss: 0.31746363639831543, evaluation loss: 0.3545494079589844\n",
      "900/3000 training loss: 0.3169104754924774, evaluation loss: 0.3539915084838867\n",
      "901/3000 training loss: 0.3163582682609558, evaluation loss: 0.3534344434738159\n",
      "902/3000 training loss: 0.31580713391304016, evaluation loss: 0.3528784215450287\n",
      "903/3000 training loss: 0.3152570128440857, evaluation loss: 0.3523232638835907\n",
      "904/3000 training loss: 0.31470787525177, evaluation loss: 0.3517690598964691\n",
      "905/3000 training loss: 0.3141597807407379, evaluation loss: 0.35121583938598633\n",
      "906/3000 training loss: 0.313612699508667, evaluation loss: 0.3506634533405304\n",
      "907/3000 training loss: 0.3130665719509125, evaluation loss: 0.35011205077171326\n",
      "908/3000 training loss: 0.3125215470790863, evaluation loss: 0.3495616018772125\n",
      "909/3000 training loss: 0.31197747588157654, evaluation loss: 0.3490120768547058\n",
      "910/3000 training loss: 0.31143441796302795, evaluation loss: 0.3484634459018707\n",
      "911/3000 training loss: 0.31089240312576294, evaluation loss: 0.34791579842567444\n",
      "912/3000 training loss: 0.3103513717651367, evaluation loss: 0.347368985414505\n",
      "913/3000 training loss: 0.3098112940788269, evaluation loss: 0.34682318568229675\n",
      "914/3000 training loss: 0.30927225947380066, evaluation loss: 0.3462783098220825\n",
      "915/3000 training loss: 0.3087342381477356, evaluation loss: 0.34573429822921753\n",
      "916/3000 training loss: 0.3081972002983093, evaluation loss: 0.34519121050834656\n",
      "917/3000 training loss: 0.30766114592552185, evaluation loss: 0.3446491062641144\n",
      "918/3000 training loss: 0.30712610483169556, evaluation loss: 0.34410789608955383\n",
      "919/3000 training loss: 0.3065919876098633, evaluation loss: 0.34356755018234253\n",
      "920/3000 training loss: 0.30605897307395935, evaluation loss: 0.3430282175540924\n",
      "921/3000 training loss: 0.3055269122123718, evaluation loss: 0.34248968958854675\n",
      "922/3000 training loss: 0.3049957752227783, evaluation loss: 0.3419521152973175\n",
      "923/3000 training loss: 0.30446571111679077, evaluation loss: 0.34141552448272705\n",
      "924/3000 training loss: 0.30393660068511963, evaluation loss: 0.34087976813316345\n",
      "925/3000 training loss: 0.30340850353240967, evaluation loss: 0.34034496545791626\n",
      "926/3000 training loss: 0.3028813302516937, evaluation loss: 0.3398110270500183\n",
      "927/3000 training loss: 0.3023551404476166, evaluation loss: 0.33927804231643677\n",
      "928/3000 training loss: 0.301829993724823, evaluation loss: 0.33874598145484924\n",
      "929/3000 training loss: 0.30130574107170105, evaluation loss: 0.33821478486061096\n",
      "930/3000 training loss: 0.3007825016975403, evaluation loss: 0.3376844823360443\n",
      "931/3000 training loss: 0.3002602458000183, evaluation loss: 0.33715513348579407\n",
      "932/3000 training loss: 0.29973897337913513, evaluation loss: 0.3366266191005707\n",
      "933/3000 training loss: 0.29921865463256836, evaluation loss: 0.33609911799430847\n",
      "934/3000 training loss: 0.29869934916496277, evaluation loss: 0.33557239174842834\n",
      "935/3000 training loss: 0.2981809377670288, evaluation loss: 0.3350466191768646\n",
      "936/3000 training loss: 0.29766350984573364, evaluation loss: 0.3345217704772949\n",
      "937/3000 training loss: 0.29714709520339966, evaluation loss: 0.33399781584739685\n",
      "938/3000 training loss: 0.2966316342353821, evaluation loss: 0.333474725484848\n",
      "939/3000 training loss: 0.2961170971393585, evaluation loss: 0.3329525887966156\n",
      "940/3000 training loss: 0.29560357332229614, evaluation loss: 0.33243125677108765\n",
      "941/3000 training loss: 0.2950909733772278, evaluation loss: 0.3319109082221985\n",
      "942/3000 training loss: 0.29457932710647583, evaluation loss: 0.3313913941383362\n",
      "943/3000 training loss: 0.29406866431236267, evaluation loss: 0.3308728039264679\n",
      "944/3000 training loss: 0.2935589849948883, evaluation loss: 0.33035507798194885\n",
      "945/3000 training loss: 0.29305019974708557, evaluation loss: 0.32983827590942383\n",
      "946/3000 training loss: 0.29254236817359924, evaluation loss: 0.32932233810424805\n",
      "947/3000 training loss: 0.2920355200767517, evaluation loss: 0.3288072645664215\n",
      "948/3000 training loss: 0.2915296256542206, evaluation loss: 0.3282930850982666\n",
      "949/3000 training loss: 0.29102465510368347, evaluation loss: 0.3277798593044281\n",
      "950/3000 training loss: 0.2905206084251404, evaluation loss: 0.32726743817329407\n",
      "951/3000 training loss: 0.29001757502555847, evaluation loss: 0.32675594091415405\n",
      "952/3000 training loss: 0.2895154654979706, evaluation loss: 0.3262452781200409\n",
      "953/3000 training loss: 0.2890142500400543, evaluation loss: 0.32573553919792175\n",
      "954/3000 training loss: 0.28851398825645447, evaluation loss: 0.32522666454315186\n",
      "955/3000 training loss: 0.288014680147171, evaluation loss: 0.3247186541557312\n",
      "956/3000 training loss: 0.287516325712204, evaluation loss: 0.32421156764030457\n",
      "957/3000 training loss: 0.28701889514923096, evaluation loss: 0.3237053155899048\n",
      "958/3000 training loss: 0.28652238845825195, evaluation loss: 0.32319995760917664\n",
      "959/3000 training loss: 0.28602686524391174, evaluation loss: 0.3226954936981201\n",
      "960/3000 training loss: 0.2855322062969208, evaluation loss: 0.32219186425209045\n",
      "961/3000 training loss: 0.2850385308265686, evaluation loss: 0.3216891288757324\n",
      "962/3000 training loss: 0.2845457196235657, evaluation loss: 0.32118725776672363\n",
      "963/3000 training loss: 0.28405386209487915, evaluation loss: 0.3206862807273865\n",
      "964/3000 training loss: 0.28356295824050903, evaluation loss: 0.32018613815307617\n",
      "965/3000 training loss: 0.28307297825813293, evaluation loss: 0.3196869194507599\n",
      "966/3000 training loss: 0.28258392214775085, evaluation loss: 0.3191884756088257\n",
      "967/3000 training loss: 0.2820957601070404, evaluation loss: 0.3186909556388855\n",
      "968/3000 training loss: 0.281608521938324, evaluation loss: 0.31819429993629456\n",
      "969/3000 training loss: 0.2811221778392792, evaluation loss: 0.31769850850105286\n",
      "970/3000 training loss: 0.28063684701919556, evaluation loss: 0.317203551530838\n",
      "971/3000 training loss: 0.2801523506641388, evaluation loss: 0.3167094886302948\n",
      "972/3000 training loss: 0.27966880798339844, evaluation loss: 0.31621626019477844\n",
      "973/3000 training loss: 0.2791861295700073, evaluation loss: 0.3157239258289337\n",
      "974/3000 training loss: 0.27870437502861023, evaluation loss: 0.31523242592811584\n",
      "975/3000 training loss: 0.27822357416152954, evaluation loss: 0.3147417902946472\n",
      "976/3000 training loss: 0.2777436375617981, evaluation loss: 0.31425201892852783\n",
      "977/3000 training loss: 0.27726465463638306, evaluation loss: 0.3137631118297577\n",
      "978/3000 training loss: 0.27678653597831726, evaluation loss: 0.313275009393692\n",
      "979/3000 training loss: 0.2763093411922455, evaluation loss: 0.3127877712249756\n",
      "980/3000 training loss: 0.2758330702781677, evaluation loss: 0.3123014271259308\n",
      "981/3000 training loss: 0.2753576636314392, evaluation loss: 0.31181591749191284\n",
      "982/3000 training loss: 0.2748831510543823, evaluation loss: 0.31133124232292175\n",
      "983/3000 training loss: 0.27440956234931946, evaluation loss: 0.3108474314212799\n",
      "984/3000 training loss: 0.2739368677139282, evaluation loss: 0.3103644549846649\n",
      "985/3000 training loss: 0.2734650671482086, evaluation loss: 0.30988234281539917\n",
      "986/3000 training loss: 0.27299413084983826, evaluation loss: 0.30940109491348267\n",
      "987/3000 training loss: 0.2725241482257843, evaluation loss: 0.30892065167427063\n",
      "988/3000 training loss: 0.2720550000667572, evaluation loss: 0.30844104290008545\n",
      "989/3000 training loss: 0.2715868055820465, evaluation loss: 0.3079623281955719\n",
      "990/3000 training loss: 0.27111944556236267, evaluation loss: 0.3074844181537628\n",
      "991/3000 training loss: 0.27065297961235046, evaluation loss: 0.307007372379303\n",
      "992/3000 training loss: 0.2701874375343323, evaluation loss: 0.30653116106987\n",
      "993/3000 training loss: 0.26972272992134094, evaluation loss: 0.30605578422546387\n",
      "994/3000 training loss: 0.26925894618034363, evaluation loss: 0.3055812120437622\n",
      "995/3000 training loss: 0.26879602670669556, evaluation loss: 0.3051075339317322\n",
      "996/3000 training loss: 0.26833397150039673, evaluation loss: 0.3046346604824066\n",
      "997/3000 training loss: 0.2678728401660919, evaluation loss: 0.3041626513004303\n",
      "998/3000 training loss: 0.2674125134944916, evaluation loss: 0.30369141697883606\n",
      "999/3000 training loss: 0.26695314049720764, evaluation loss: 0.30322107672691345\n",
      "1000/3000 training loss: 0.26649460196495056, evaluation loss: 0.3027515411376953\n",
      "1001/3000 training loss: 0.2660369575023651, evaluation loss: 0.30228284001350403\n",
      "1002/3000 training loss: 0.2655801475048065, evaluation loss: 0.3018149733543396\n",
      "1003/3000 training loss: 0.26512426137924194, evaluation loss: 0.301347941160202\n",
      "1004/3000 training loss: 0.2646692097187042, evaluation loss: 0.30088168382644653\n",
      "1005/3000 training loss: 0.26421502232551575, evaluation loss: 0.30041632056236267\n",
      "1006/3000 training loss: 0.2637617290019989, evaluation loss: 0.2999517321586609\n",
      "1007/3000 training loss: 0.2633092999458313, evaluation loss: 0.29948800802230835\n",
      "1008/3000 training loss: 0.26285773515701294, evaluation loss: 0.2990250587463379\n",
      "1009/3000 training loss: 0.26240700483322144, evaluation loss: 0.2985629737377167\n",
      "1010/3000 training loss: 0.2619571387767792, evaluation loss: 0.2981016933917999\n",
      "1011/3000 training loss: 0.26150816679000854, evaluation loss: 0.29764124751091003\n",
      "1012/3000 training loss: 0.2610599994659424, evaluation loss: 0.297181636095047\n",
      "1013/3000 training loss: 0.26061275601387024, evaluation loss: 0.29672279953956604\n",
      "1014/3000 training loss: 0.26016631722450256, evaluation loss: 0.2962648272514343\n",
      "1015/3000 training loss: 0.25972074270248413, evaluation loss: 0.2958076596260071\n",
      "1016/3000 training loss: 0.25927603244781494, evaluation loss: 0.2953512668609619\n",
      "1017/3000 training loss: 0.2588321566581726, evaluation loss: 0.2948957085609436\n",
      "1018/3000 training loss: 0.2583891749382019, evaluation loss: 0.29444095492362976\n",
      "1019/3000 training loss: 0.25794702768325806, evaluation loss: 0.2939870357513428\n",
      "1020/3000 training loss: 0.2575056850910187, evaluation loss: 0.29353389143943787\n",
      "1021/3000 training loss: 0.25706520676612854, evaluation loss: 0.2930815815925598\n",
      "1022/3000 training loss: 0.25662559270858765, evaluation loss: 0.29263007640838623\n",
      "1023/3000 training loss: 0.256186842918396, evaluation loss: 0.2921794354915619\n",
      "1024/3000 training loss: 0.2557488977909088, evaluation loss: 0.29172953963279724\n",
      "1025/3000 training loss: 0.2553118169307709, evaluation loss: 0.2912804186344147\n",
      "1026/3000 training loss: 0.2548755705356598, evaluation loss: 0.29083219170570374\n",
      "1027/3000 training loss: 0.25444015860557556, evaluation loss: 0.2903847098350525\n",
      "1028/3000 training loss: 0.2540055513381958, evaluation loss: 0.2899380624294281\n",
      "1029/3000 training loss: 0.25357183814048767, evaluation loss: 0.2894921600818634\n",
      "1030/3000 training loss: 0.2531388998031616, evaluation loss: 0.28904709219932556\n",
      "1031/3000 training loss: 0.2527068555355072, evaluation loss: 0.2886028587818146\n",
      "1032/3000 training loss: 0.25227564573287964, evaluation loss: 0.2881593406200409\n",
      "1033/3000 training loss: 0.25184521079063416, evaluation loss: 0.28771671652793884\n",
      "1034/3000 training loss: 0.2514156401157379, evaluation loss: 0.2872748374938965\n",
      "1035/3000 training loss: 0.25098690390586853, evaluation loss: 0.2868337631225586\n",
      "1036/3000 training loss: 0.2505589723587036, evaluation loss: 0.2863934636116028\n",
      "1037/3000 training loss: 0.25013187527656555, evaluation loss: 0.28595399856567383\n",
      "1038/3000 training loss: 0.24970558285713196, evaluation loss: 0.28551527857780457\n",
      "1039/3000 training loss: 0.2492801696062088, evaluation loss: 0.28507739305496216\n",
      "1040/3000 training loss: 0.24885553121566772, evaluation loss: 0.28464028239250183\n",
      "1041/3000 training loss: 0.2484317272901535, evaluation loss: 0.28420397639274597\n",
      "1042/3000 training loss: 0.24800874292850494, evaluation loss: 0.2837684154510498\n",
      "1043/3000 training loss: 0.24758657813072205, evaluation loss: 0.2833336591720581\n",
      "1044/3000 training loss: 0.24716521799564362, evaluation loss: 0.2828996777534485\n",
      "1045/3000 training loss: 0.24674469232559204, evaluation loss: 0.2824665307998657\n",
      "1046/3000 training loss: 0.24632495641708374, evaluation loss: 0.28203409910202026\n",
      "1047/3000 training loss: 0.2459060549736023, evaluation loss: 0.28160250186920166\n",
      "1048/3000 training loss: 0.2454879879951477, evaluation loss: 0.28117167949676514\n",
      "1049/3000 training loss: 0.2450706958770752, evaluation loss: 0.2807416021823883\n",
      "1050/3000 training loss: 0.24465420842170715, evaluation loss: 0.28031235933303833\n",
      "1051/3000 training loss: 0.24423855543136597, evaluation loss: 0.27988386154174805\n",
      "1052/3000 training loss: 0.24382369220256805, evaluation loss: 0.27945610880851746\n",
      "1053/3000 training loss: 0.2434096336364746, evaluation loss: 0.2790292203426361\n",
      "1054/3000 training loss: 0.24299639463424683, evaluation loss: 0.27860304713249207\n",
      "1055/3000 training loss: 0.24258393049240112, evaluation loss: 0.2781776487827301\n",
      "1056/3000 training loss: 0.24217228591442108, evaluation loss: 0.2777530550956726\n",
      "1057/3000 training loss: 0.2417614609003067, evaluation loss: 0.2773292064666748\n",
      "1058/3000 training loss: 0.2413514107465744, evaluation loss: 0.27690616250038147\n",
      "1059/3000 training loss: 0.24094216525554657, evaluation loss: 0.27648383378982544\n",
      "1060/3000 training loss: 0.2405337393283844, evaluation loss: 0.2760623097419739\n",
      "1061/3000 training loss: 0.2401260882616043, evaluation loss: 0.275641530752182\n",
      "1062/3000 training loss: 0.2397192120552063, evaluation loss: 0.2752215564250946\n",
      "1063/3000 training loss: 0.23931314051151276, evaluation loss: 0.2748023271560669\n",
      "1064/3000 training loss: 0.23890787363052368, evaluation loss: 0.2743838429450989\n",
      "1065/3000 training loss: 0.23850339651107788, evaluation loss: 0.2739661931991577\n",
      "1066/3000 training loss: 0.23809970915317535, evaluation loss: 0.27354922890663147\n",
      "1067/3000 training loss: 0.2376968115568161, evaluation loss: 0.2731330692768097\n",
      "1068/3000 training loss: 0.23729467391967773, evaluation loss: 0.2727176547050476\n",
      "1069/3000 training loss: 0.23689335584640503, evaluation loss: 0.2723029851913452\n",
      "1070/3000 training loss: 0.2364928126335144, evaluation loss: 0.2718890905380249\n",
      "1071/3000 training loss: 0.23609307408332825, evaluation loss: 0.2714759409427643\n",
      "1072/3000 training loss: 0.23569408059120178, evaluation loss: 0.27106359601020813\n",
      "1073/3000 training loss: 0.23529589176177979, evaluation loss: 0.2706519663333893\n",
      "1074/3000 training loss: 0.23489846289157867, evaluation loss: 0.2702410817146301\n",
      "1075/3000 training loss: 0.23450183868408203, evaluation loss: 0.26983100175857544\n",
      "1076/3000 training loss: 0.23410597443580627, evaluation loss: 0.26942166686058044\n",
      "1077/3000 training loss: 0.2337108850479126, evaluation loss: 0.26901307702064514\n",
      "1078/3000 training loss: 0.233316570520401, evaluation loss: 0.26860520243644714\n",
      "1079/3000 training loss: 0.23292306065559387, evaluation loss: 0.26819807291030884\n",
      "1080/3000 training loss: 0.23253028094768524, evaluation loss: 0.267791748046875\n",
      "1081/3000 training loss: 0.23213830590248108, evaluation loss: 0.26738616824150085\n",
      "1082/3000 training loss: 0.2317470908164978, evaluation loss: 0.2669812738895416\n",
      "1083/3000 training loss: 0.23135662078857422, evaluation loss: 0.2665771543979645\n",
      "1084/3000 training loss: 0.2309669554233551, evaluation loss: 0.2661738097667694\n",
      "1085/3000 training loss: 0.23057805001735687, evaluation loss: 0.26577115058898926\n",
      "1086/3000 training loss: 0.23018988966941833, evaluation loss: 0.2653692662715912\n",
      "1087/3000 training loss: 0.22980250418186188, evaluation loss: 0.2649681270122528\n",
      "1088/3000 training loss: 0.2294158786535263, evaluation loss: 0.2645677328109741\n",
      "1089/3000 training loss: 0.22903002798557281, evaluation loss: 0.26416805386543274\n",
      "1090/3000 training loss: 0.2286449372768402, evaluation loss: 0.26376914978027344\n",
      "1091/3000 training loss: 0.2282605916261673, evaluation loss: 0.26337099075317383\n",
      "1092/3000 training loss: 0.22787702083587646, evaluation loss: 0.26297351717948914\n",
      "1093/3000 training loss: 0.22749418020248413, evaluation loss: 0.2625768184661865\n",
      "1094/3000 training loss: 0.22711211442947388, evaluation loss: 0.26218080520629883\n",
      "1095/3000 training loss: 0.2267308235168457, evaluation loss: 0.2617855668067932\n",
      "1096/3000 training loss: 0.22635026276111603, evaluation loss: 0.2613910436630249\n",
      "1097/3000 training loss: 0.22597044706344604, evaluation loss: 0.2609972655773163\n",
      "1098/3000 training loss: 0.22559140622615814, evaluation loss: 0.26060420274734497\n",
      "1099/3000 training loss: 0.22521311044692993, evaluation loss: 0.26021188497543335\n",
      "1100/3000 training loss: 0.2248355597257614, evaluation loss: 0.25982028245925903\n",
      "1101/3000 training loss: 0.22445876896381378, evaluation loss: 0.259429395198822\n",
      "1102/3000 training loss: 0.22408270835876465, evaluation loss: 0.2590392827987671\n",
      "1103/3000 training loss: 0.2237073928117752, evaluation loss: 0.2586498558521271\n",
      "1104/3000 training loss: 0.22333282232284546, evaluation loss: 0.25826117396354675\n",
      "1105/3000 training loss: 0.2229590266942978, evaluation loss: 0.25787317752838135\n",
      "1106/3000 training loss: 0.22258594632148743, evaluation loss: 0.25748592615127563\n",
      "1107/3000 training loss: 0.22221361100673676, evaluation loss: 0.2570994198322296\n",
      "1108/3000 training loss: 0.22184203565120697, evaluation loss: 0.2567135989665985\n",
      "1109/3000 training loss: 0.2214711606502533, evaluation loss: 0.2563284933567047\n",
      "1110/3000 training loss: 0.2211010605096817, evaluation loss: 0.2559441030025482\n",
      "1111/3000 training loss: 0.2207316756248474, evaluation loss: 0.2555604577064514\n",
      "1112/3000 training loss: 0.22036303579807281, evaluation loss: 0.2551775574684143\n",
      "1113/3000 training loss: 0.21999511122703552, evaluation loss: 0.25479528307914734\n",
      "1114/3000 training loss: 0.21962794661521912, evaluation loss: 0.25441378355026245\n",
      "1115/3000 training loss: 0.21926149725914001, evaluation loss: 0.2540329694747925\n",
      "1116/3000 training loss: 0.2188957929611206, evaluation loss: 0.2536528706550598\n",
      "1117/3000 training loss: 0.2185307890176773, evaluation loss: 0.25327348709106445\n",
      "1118/3000 training loss: 0.2181665450334549, evaluation loss: 0.2528948187828064\n",
      "1119/3000 training loss: 0.21780303120613098, evaluation loss: 0.25251686573028564\n",
      "1120/3000 training loss: 0.21744021773338318, evaluation loss: 0.2521395981311798\n",
      "1121/3000 training loss: 0.21707814931869507, evaluation loss: 0.2517630457878113\n",
      "1122/3000 training loss: 0.21671681106090546, evaluation loss: 0.25138720870018005\n",
      "1123/3000 training loss: 0.21635618805885315, evaluation loss: 0.25101205706596375\n",
      "1124/3000 training loss: 0.21599628031253815, evaluation loss: 0.25063759088516235\n",
      "1125/3000 training loss: 0.21563708782196045, evaluation loss: 0.25026386976242065\n",
      "1126/3000 training loss: 0.21527861058712006, evaluation loss: 0.24989087879657745\n",
      "1127/3000 training loss: 0.21492089331150055, evaluation loss: 0.2495184987783432\n",
      "1128/3000 training loss: 0.21456384658813477, evaluation loss: 0.24914684891700745\n",
      "1129/3000 training loss: 0.21420753002166748, evaluation loss: 0.248775914311409\n",
      "1130/3000 training loss: 0.2138519436120987, evaluation loss: 0.24840568006038666\n",
      "1131/3000 training loss: 0.2134970724582672, evaluation loss: 0.24803613126277924\n",
      "1132/3000 training loss: 0.21314290165901184, evaluation loss: 0.24766729772090912\n",
      "1133/3000 training loss: 0.21278946101665497, evaluation loss: 0.24729911983013153\n",
      "1134/3000 training loss: 0.2124367207288742, evaluation loss: 0.24693164229393005\n",
      "1135/3000 training loss: 0.21208466589450836, evaluation loss: 0.24656489491462708\n",
      "1136/3000 training loss: 0.2117333561182022, evaluation loss: 0.24619878828525543\n",
      "1137/3000 training loss: 0.21138273179531097, evaluation loss: 0.2458333969116211\n",
      "1138/3000 training loss: 0.21103280782699585, evaluation loss: 0.24546866118907928\n",
      "1139/3000 training loss: 0.21068359911441803, evaluation loss: 0.24510464072227478\n",
      "1140/3000 training loss: 0.21033510565757751, evaluation loss: 0.2447413057088852\n",
      "1141/3000 training loss: 0.20998729765415192, evaluation loss: 0.24437865614891052\n",
      "1142/3000 training loss: 0.20964021980762482, evaluation loss: 0.24401669204235077\n",
      "1143/3000 training loss: 0.20929381251335144, evaluation loss: 0.24365539848804474\n",
      "1144/3000 training loss: 0.20894812047481537, evaluation loss: 0.24329480528831482\n",
      "1145/3000 training loss: 0.2086031138896942, evaluation loss: 0.24293488264083862\n",
      "1146/3000 training loss: 0.20825880765914917, evaluation loss: 0.24257566034793854\n",
      "1147/3000 training loss: 0.20791521668434143, evaluation loss: 0.2422170788049698\n",
      "1148/3000 training loss: 0.2075723111629486, evaluation loss: 0.24185919761657715\n",
      "1149/3000 training loss: 0.2072301059961319, evaluation loss: 0.24150198698043823\n",
      "1150/3000 training loss: 0.2068885862827301, evaluation loss: 0.24114546179771423\n",
      "1151/3000 training loss: 0.20654773712158203, evaluation loss: 0.24078959226608276\n",
      "1152/3000 training loss: 0.20620760321617126, evaluation loss: 0.2404344081878662\n",
      "1153/3000 training loss: 0.20586815476417542, evaluation loss: 0.24007989466190338\n",
      "1154/3000 training loss: 0.20552939176559448, evaluation loss: 0.23972606658935547\n",
      "1155/3000 training loss: 0.20519131422042847, evaluation loss: 0.2393728643655777\n",
      "1156/3000 training loss: 0.20485393702983856, evaluation loss: 0.23902034759521484\n",
      "1157/3000 training loss: 0.20451723039150238, evaluation loss: 0.2386685311794281\n",
      "1158/3000 training loss: 0.20418120920658112, evaluation loss: 0.2383173555135727\n",
      "1159/3000 training loss: 0.20384585857391357, evaluation loss: 0.237966850399971\n",
      "1160/3000 training loss: 0.20351120829582214, evaluation loss: 0.23761701583862305\n",
      "1161/3000 training loss: 0.20317724347114563, evaluation loss: 0.23726782202720642\n",
      "1162/3000 training loss: 0.20284394919872284, evaluation loss: 0.2369193136692047\n",
      "1163/3000 training loss: 0.20251134037971497, evaluation loss: 0.23657146096229553\n",
      "1164/3000 training loss: 0.20217938721179962, evaluation loss: 0.23622429370880127\n",
      "1165/3000 training loss: 0.201848104596138, evaluation loss: 0.23587775230407715\n",
      "1166/3000 training loss: 0.20151755213737488, evaluation loss: 0.23553188145160675\n",
      "1167/3000 training loss: 0.2011876255273819, evaluation loss: 0.2351866513490677\n",
      "1168/3000 training loss: 0.20085836946964264, evaluation loss: 0.23484207689762115\n",
      "1169/3000 training loss: 0.2005298137664795, evaluation loss: 0.23449818789958954\n",
      "1170/3000 training loss: 0.20020189881324768, evaluation loss: 0.23415493965148926\n",
      "1171/3000 training loss: 0.19987468421459198, evaluation loss: 0.2338123470544815\n",
      "1172/3000 training loss: 0.1995481252670288, evaluation loss: 0.23347041010856628\n",
      "1173/3000 training loss: 0.19922222197055817, evaluation loss: 0.2331290990114212\n",
      "1174/3000 training loss: 0.19889700412750244, evaluation loss: 0.23278844356536865\n",
      "1175/3000 training loss: 0.19857245683670044, evaluation loss: 0.23244845867156982\n",
      "1176/3000 training loss: 0.19824855029582977, evaluation loss: 0.23210908472537994\n",
      "1177/3000 training loss: 0.19792529940605164, evaluation loss: 0.23177039623260498\n",
      "1178/3000 training loss: 0.19760273396968842, evaluation loss: 0.23143236339092255\n",
      "1179/3000 training loss: 0.19728080928325653, evaluation loss: 0.23109494149684906\n",
      "1180/3000 training loss: 0.19695957005023956, evaluation loss: 0.2307581752538681\n",
      "1181/3000 training loss: 0.19663895666599274, evaluation loss: 0.23042206466197968\n",
      "1182/3000 training loss: 0.19631901383399963, evaluation loss: 0.230086550116539\n",
      "1183/3000 training loss: 0.19599972665309906, evaluation loss: 0.22975172102451324\n",
      "1184/3000 training loss: 0.19568109512329102, evaluation loss: 0.22941751778125763\n",
      "1185/3000 training loss: 0.1953631192445755, evaluation loss: 0.22908392548561096\n",
      "1186/3000 training loss: 0.19504579901695251, evaluation loss: 0.22875100374221802\n",
      "1187/3000 training loss: 0.19472913444042206, evaluation loss: 0.22841870784759521\n",
      "1188/3000 training loss: 0.19441309571266174, evaluation loss: 0.22808706760406494\n",
      "1189/3000 training loss: 0.19409774243831635, evaluation loss: 0.22775602340698242\n",
      "1190/3000 training loss: 0.1937830001115799, evaluation loss: 0.22742560505867004\n",
      "1191/3000 training loss: 0.19346891343593597, evaluation loss: 0.2270958572626114\n",
      "1192/3000 training loss: 0.19315551221370697, evaluation loss: 0.2267667055130005\n",
      "1193/3000 training loss: 0.19284272193908691, evaluation loss: 0.2264382243156433\n",
      "1194/3000 training loss: 0.1925305426120758, evaluation loss: 0.2261103093624115\n",
      "1195/3000 training loss: 0.19221904873847961, evaluation loss: 0.2257830798625946\n",
      "1196/3000 training loss: 0.19190819561481476, evaluation loss: 0.22545644640922546\n",
      "1197/3000 training loss: 0.19159796833992004, evaluation loss: 0.22513045370578766\n",
      "1198/3000 training loss: 0.19128838181495667, evaluation loss: 0.22480508685112\n",
      "1199/3000 training loss: 0.19097943603992462, evaluation loss: 0.22448033094406128\n",
      "1200/3000 training loss: 0.1906711459159851, evaluation loss: 0.2241561859846115\n",
      "1201/3000 training loss: 0.19036343693733215, evaluation loss: 0.22383268177509308\n",
      "1202/3000 training loss: 0.19005639851093292, evaluation loss: 0.2235097885131836\n",
      "1203/3000 training loss: 0.18975001573562622, evaluation loss: 0.22318749129772186\n",
      "1204/3000 training loss: 0.18944424390792847, evaluation loss: 0.22286586463451385\n",
      "1205/3000 training loss: 0.18913908302783966, evaluation loss: 0.2225448042154312\n",
      "1206/3000 training loss: 0.1888345628976822, evaluation loss: 0.2222243845462799\n",
      "1207/3000 training loss: 0.18853068351745605, evaluation loss: 0.22190457582473755\n",
      "1208/3000 training loss: 0.18822742998600006, evaluation loss: 0.22158534824848175\n",
      "1209/3000 training loss: 0.18792478740215302, evaluation loss: 0.22126679122447968\n",
      "1210/3000 training loss: 0.1876227855682373, evaluation loss: 0.22094880044460297\n",
      "1211/3000 training loss: 0.18732139468193054, evaluation loss: 0.2206314355134964\n",
      "1212/3000 training loss: 0.18702061474323273, evaluation loss: 0.22031466662883759\n",
      "1213/3000 training loss: 0.18672049045562744, evaluation loss: 0.21999850869178772\n",
      "1214/3000 training loss: 0.1864209771156311, evaluation loss: 0.2196829617023468\n",
      "1215/3000 training loss: 0.18612205982208252, evaluation loss: 0.21936799585819244\n",
      "1216/3000 training loss: 0.18582378327846527, evaluation loss: 0.21905367076396942\n",
      "1217/3000 training loss: 0.18552610278129578, evaluation loss: 0.21873992681503296\n",
      "1218/3000 training loss: 0.18522904813289642, evaluation loss: 0.21842677891254425\n",
      "1219/3000 training loss: 0.18493258953094482, evaluation loss: 0.21811425685882568\n",
      "1220/3000 training loss: 0.18463678658008575, evaluation loss: 0.21780233085155487\n",
      "1221/3000 training loss: 0.18434156477451324, evaluation loss: 0.21749097108840942\n",
      "1222/3000 training loss: 0.18404695391654968, evaluation loss: 0.21718023717403412\n",
      "1223/3000 training loss: 0.18375296890735626, evaluation loss: 0.21687009930610657\n",
      "1224/3000 training loss: 0.1834595948457718, evaluation loss: 0.21656054258346558\n",
      "1225/3000 training loss: 0.18316680192947388, evaluation loss: 0.21625158190727234\n",
      "1226/3000 training loss: 0.1828746348619461, evaluation loss: 0.21594321727752686\n",
      "1227/3000 training loss: 0.1825830638408661, evaluation loss: 0.21563541889190674\n",
      "1228/3000 training loss: 0.18229210376739502, evaluation loss: 0.21532826125621796\n",
      "1229/3000 training loss: 0.1820017546415329, evaluation loss: 0.21502165496349335\n",
      "1230/3000 training loss: 0.18171198666095734, evaluation loss: 0.2147156447172165\n",
      "1231/3000 training loss: 0.1814228594303131, evaluation loss: 0.2144102156162262\n",
      "1232/3000 training loss: 0.18113428354263306, evaluation loss: 0.21410538256168365\n",
      "1233/3000 training loss: 0.18084631860256195, evaluation loss: 0.21380113065242767\n",
      "1234/3000 training loss: 0.1805589348077774, evaluation loss: 0.21349744498729706\n",
      "1235/3000 training loss: 0.180272176861763, evaluation loss: 0.2131943702697754\n",
      "1236/3000 training loss: 0.17998600006103516, evaluation loss: 0.21289189159870148\n",
      "1237/3000 training loss: 0.17970041930675507, evaluation loss: 0.21258996427059174\n",
      "1238/3000 training loss: 0.17941543459892273, evaluation loss: 0.21228860318660736\n",
      "1239/3000 training loss: 0.17913104593753815, evaluation loss: 0.21198785305023193\n",
      "1240/3000 training loss: 0.17884723842144012, evaluation loss: 0.21168766915798187\n",
      "1241/3000 training loss: 0.17856401205062866, evaluation loss: 0.21138805150985718\n",
      "1242/3000 training loss: 0.17828138172626495, evaluation loss: 0.21108900010585785\n",
      "1243/3000 training loss: 0.1779993325471878, evaluation loss: 0.21079054474830627\n",
      "1244/3000 training loss: 0.17771786451339722, evaluation loss: 0.21049265563488007\n",
      "1245/3000 training loss: 0.17743699252605438, evaluation loss: 0.21019533276557922\n",
      "1246/3000 training loss: 0.1771567165851593, evaluation loss: 0.20989859104156494\n",
      "1247/3000 training loss: 0.1768769919872284, evaluation loss: 0.20960241556167603\n",
      "1248/3000 training loss: 0.17659786343574524, evaluation loss: 0.20930679142475128\n",
      "1249/3000 training loss: 0.17631931602954865, evaluation loss: 0.2090117186307907\n",
      "1250/3000 training loss: 0.17604131996631622, evaluation loss: 0.2087172418832779\n",
      "1251/3000 training loss: 0.17576394975185394, evaluation loss: 0.20842334628105164\n",
      "1252/3000 training loss: 0.17548711597919464, evaluation loss: 0.20813000202178955\n",
      "1253/3000 training loss: 0.1752108633518219, evaluation loss: 0.20783717930316925\n",
      "1254/3000 training loss: 0.17493519186973572, evaluation loss: 0.2075449675321579\n",
      "1255/3000 training loss: 0.1746601015329361, evaluation loss: 0.20725330710411072\n",
      "1256/3000 training loss: 0.17438556253910065, evaluation loss: 0.20696218311786652\n",
      "1257/3000 training loss: 0.17411158978939056, evaluation loss: 0.20667162537574768\n",
      "1258/3000 training loss: 0.17383819818496704, evaluation loss: 0.20638161897659302\n",
      "1259/3000 training loss: 0.1735653579235077, evaluation loss: 0.20609217882156372\n",
      "1260/3000 training loss: 0.1732930988073349, evaluation loss: 0.2058032751083374\n",
      "1261/3000 training loss: 0.17302140593528748, evaluation loss: 0.20551495254039764\n",
      "1262/3000 training loss: 0.17275024950504303, evaluation loss: 0.20522715151309967\n",
      "1263/3000 training loss: 0.17247970402240753, evaluation loss: 0.20493993163108826\n",
      "1264/3000 training loss: 0.172209694981575, evaluation loss: 0.20465323328971863\n",
      "1265/3000 training loss: 0.17194025218486786, evaluation loss: 0.20436711609363556\n",
      "1266/3000 training loss: 0.1716713309288025, evaluation loss: 0.20408153533935547\n",
      "1267/3000 training loss: 0.17140302062034607, evaluation loss: 0.20379649102687836\n",
      "1268/3000 training loss: 0.17113524675369263, evaluation loss: 0.20351198315620422\n",
      "1269/3000 training loss: 0.17086803913116455, evaluation loss: 0.20322801172733307\n",
      "1270/3000 training loss: 0.17060135304927826, evaluation loss: 0.20294460654258728\n",
      "1271/3000 training loss: 0.17033521831035614, evaluation loss: 0.20266176760196686\n",
      "1272/3000 training loss: 0.17006966471672058, evaluation loss: 0.20237943530082703\n",
      "1273/3000 training loss: 0.1698046624660492, evaluation loss: 0.20209763944149017\n",
      "1274/3000 training loss: 0.1695401817560196, evaluation loss: 0.2018163800239563\n",
      "1275/3000 training loss: 0.16927628219127655, evaluation loss: 0.2015356868505478\n",
      "1276/3000 training loss: 0.1690129041671753, evaluation loss: 0.20125548541545868\n",
      "1277/3000 training loss: 0.1687500923871994, evaluation loss: 0.20097585022449493\n",
      "1278/3000 training loss: 0.1684878021478653, evaluation loss: 0.20069675147533417\n",
      "1279/3000 training loss: 0.16822609305381775, evaluation loss: 0.20041817426681519\n",
      "1280/3000 training loss: 0.1679648756980896, evaluation loss: 0.20014013350009918\n",
      "1281/3000 training loss: 0.167704239487648, evaluation loss: 0.19986261427402496\n",
      "1282/3000 training loss: 0.1674441248178482, evaluation loss: 0.19958563148975372\n",
      "1283/3000 training loss: 0.16718453168869019, evaluation loss: 0.19930918514728546\n",
      "1284/3000 training loss: 0.16692550480365753, evaluation loss: 0.19903326034545898\n",
      "1285/3000 training loss: 0.16666701436042786, evaluation loss: 0.1987578570842743\n",
      "1286/3000 training loss: 0.16640904545783997, evaluation loss: 0.19848299026489258\n",
      "1287/3000 training loss: 0.16615161299705505, evaluation loss: 0.19820863008499146\n",
      "1288/3000 training loss: 0.16589471697807312, evaluation loss: 0.19793477654457092\n",
      "1289/3000 training loss: 0.16563835740089417, evaluation loss: 0.19766147434711456\n",
      "1290/3000 training loss: 0.1653825044631958, evaluation loss: 0.1973886787891388\n",
      "1291/3000 training loss: 0.1651272028684616, evaluation loss: 0.197116419672966\n",
      "1292/3000 training loss: 0.1648724228143692, evaluation loss: 0.1968446671962738\n",
      "1293/3000 training loss: 0.16461816430091858, evaluation loss: 0.1965734213590622\n",
      "1294/3000 training loss: 0.16436442732810974, evaluation loss: 0.19630271196365356\n",
      "1295/3000 training loss: 0.16411122679710388, evaluation loss: 0.19603249430656433\n",
      "1296/3000 training loss: 0.1638585478067398, evaluation loss: 0.19576281309127808\n",
      "1297/3000 training loss: 0.16360637545585632, evaluation loss: 0.19549360871315002\n",
      "1298/3000 training loss: 0.163354754447937, evaluation loss: 0.19522495567798615\n",
      "1299/3000 training loss: 0.1631036400794983, evaluation loss: 0.19495680928230286\n",
      "1300/3000 training loss: 0.16285303235054016, evaluation loss: 0.19468915462493896\n",
      "1301/3000 training loss: 0.16260294616222382, evaluation loss: 0.19442199170589447\n",
      "1302/3000 training loss: 0.16235336661338806, evaluation loss: 0.19415535032749176\n",
      "1303/3000 training loss: 0.16210432350635529, evaluation loss: 0.19388921558856964\n",
      "1304/3000 training loss: 0.1618557721376419, evaluation loss: 0.1936235874891281\n",
      "1305/3000 training loss: 0.1616077572107315, evaluation loss: 0.19335846602916718\n",
      "1306/3000 training loss: 0.1613602191209793, evaluation loss: 0.19309385120868683\n",
      "1307/3000 training loss: 0.1611132174730301, evaluation loss: 0.19282972812652588\n",
      "1308/3000 training loss: 0.16086673736572266, evaluation loss: 0.1925661265850067\n",
      "1309/3000 training loss: 0.16062073409557343, evaluation loss: 0.19230298697948456\n",
      "1310/3000 training loss: 0.16037525236606598, evaluation loss: 0.192040354013443\n",
      "1311/3000 training loss: 0.16013027727603912, evaluation loss: 0.19177822768688202\n",
      "1312/3000 training loss: 0.15988580882549286, evaluation loss: 0.19151660799980164\n",
      "1313/3000 training loss: 0.159641832113266, evaluation loss: 0.19125548005104065\n",
      "1314/3000 training loss: 0.15939834713935852, evaluation loss: 0.19099481403827667\n",
      "1315/3000 training loss: 0.15915538370609283, evaluation loss: 0.19073466956615448\n",
      "1316/3000 training loss: 0.15891289710998535, evaluation loss: 0.1904749870300293\n",
      "1317/3000 training loss: 0.15867094695568085, evaluation loss: 0.1902158260345459\n",
      "1318/3000 training loss: 0.15842947363853455, evaluation loss: 0.1899571418762207\n",
      "1319/3000 training loss: 0.15818847715854645, evaluation loss: 0.1896989643573761\n",
      "1320/3000 training loss: 0.15794801712036133, evaluation loss: 0.1894412338733673\n",
      "1321/3000 training loss: 0.15770801901817322, evaluation loss: 0.1891840100288391\n",
      "1322/3000 training loss: 0.1574685126543045, evaluation loss: 0.1889272779226303\n",
      "1323/3000 training loss: 0.1572294980287552, evaluation loss: 0.1886710226535797\n",
      "1324/3000 training loss: 0.15699097514152527, evaluation loss: 0.18841524422168732\n",
      "1325/3000 training loss: 0.15675295889377594, evaluation loss: 0.18815994262695312\n",
      "1326/3000 training loss: 0.15651541948318481, evaluation loss: 0.18790513277053833\n",
      "1327/3000 training loss: 0.1562783569097519, evaluation loss: 0.18765078485012054\n",
      "1328/3000 training loss: 0.15604178607463837, evaluation loss: 0.18739692866802216\n",
      "1329/3000 training loss: 0.15580569207668304, evaluation loss: 0.18714353442192078\n",
      "1330/3000 training loss: 0.1555701047182083, evaluation loss: 0.1868906319141388\n",
      "1331/3000 training loss: 0.1553349792957306, evaluation loss: 0.18663820624351501\n",
      "1332/3000 training loss: 0.15510031580924988, evaluation loss: 0.18638627231121063\n",
      "1333/3000 training loss: 0.15486617386341095, evaluation loss: 0.18613477051258087\n",
      "1334/3000 training loss: 0.15463249385356903, evaluation loss: 0.1858837753534317\n",
      "1335/3000 training loss: 0.15439929068088531, evaluation loss: 0.18563319742679596\n",
      "1336/3000 training loss: 0.1541665494441986, evaluation loss: 0.1853831261396408\n",
      "1337/3000 training loss: 0.1539342999458313, evaluation loss: 0.18513348698616028\n",
      "1338/3000 training loss: 0.1537025272846222, evaluation loss: 0.18488438427448273\n",
      "1339/3000 training loss: 0.1534712165594101, evaluation loss: 0.1846356987953186\n",
      "1340/3000 training loss: 0.153240367770195, evaluation loss: 0.1843874752521515\n",
      "1341/3000 training loss: 0.15300999581813812, evaluation loss: 0.18413972854614258\n",
      "1342/3000 training loss: 0.15278011560440063, evaluation loss: 0.18389247357845306\n",
      "1343/3000 training loss: 0.15255066752433777, evaluation loss: 0.18364562094211578\n",
      "1344/3000 training loss: 0.1523217260837555, evaluation loss: 0.1833992451429367\n",
      "1345/3000 training loss: 0.15209321677684784, evaluation loss: 0.18315334618091583\n",
      "1346/3000 training loss: 0.1518651843070984, evaluation loss: 0.18290789425373077\n",
      "1347/3000 training loss: 0.15163759887218475, evaluation loss: 0.18266288936138153\n",
      "1348/3000 training loss: 0.1514105200767517, evaluation loss: 0.1824183613061905\n",
      "1349/3000 training loss: 0.1511838585138321, evaluation loss: 0.18217425048351288\n",
      "1350/3000 training loss: 0.15095767378807068, evaluation loss: 0.18193064630031586\n",
      "1351/3000 training loss: 0.15073193609714508, evaluation loss: 0.18168747425079346\n",
      "1352/3000 training loss: 0.15050667524337769, evaluation loss: 0.18144474923610687\n",
      "1353/3000 training loss: 0.1502818614244461, evaluation loss: 0.1812024712562561\n",
      "1354/3000 training loss: 0.15005749464035034, evaluation loss: 0.18096064031124115\n",
      "1355/3000 training loss: 0.1498335897922516, evaluation loss: 0.180719256401062\n",
      "1356/3000 training loss: 0.14961014688014984, evaluation loss: 0.1804783195257187\n",
      "1357/3000 training loss: 0.1493871510028839, evaluation loss: 0.18023782968521118\n",
      "1358/3000 training loss: 0.1491645872592926, evaluation loss: 0.1799977868795395\n",
      "1359/3000 training loss: 0.1489425003528595, evaluation loss: 0.17975817620754242\n",
      "1360/3000 training loss: 0.148720845580101, evaluation loss: 0.17951904237270355\n",
      "1361/3000 training loss: 0.14849963784217834, evaluation loss: 0.1792803257703781\n",
      "1362/3000 training loss: 0.14827889204025269, evaluation loss: 0.1790420413017273\n",
      "1363/3000 training loss: 0.14805857837200165, evaluation loss: 0.17880423367023468\n",
      "1364/3000 training loss: 0.14783871173858643, evaluation loss: 0.1785668134689331\n",
      "1365/3000 training loss: 0.14761927723884583, evaluation loss: 0.17832988500595093\n",
      "1366/3000 training loss: 0.14740030467510223, evaluation loss: 0.17809335887432098\n",
      "1367/3000 training loss: 0.14718177914619446, evaluation loss: 0.17785727977752686\n",
      "1368/3000 training loss: 0.1469636708498001, evaluation loss: 0.17762161791324615\n",
      "1369/3000 training loss: 0.14674600958824158, evaluation loss: 0.17738640308380127\n",
      "1370/3000 training loss: 0.14652878046035767, evaluation loss: 0.177151620388031\n",
      "1371/3000 training loss: 0.14631201326847076, evaluation loss: 0.17691726982593536\n",
      "1372/3000 training loss: 0.1460956484079361, evaluation loss: 0.17668335139751434\n",
      "1373/3000 training loss: 0.14587973058223724, evaluation loss: 0.17644983530044556\n",
      "1374/3000 training loss: 0.145664244890213, evaluation loss: 0.17621676623821259\n",
      "1375/3000 training loss: 0.1454492062330246, evaluation loss: 0.17598415911197662\n",
      "1376/3000 training loss: 0.1452345848083496, evaluation loss: 0.1757519394159317\n",
      "1377/3000 training loss: 0.14502039551734924, evaluation loss: 0.1755201518535614\n",
      "1378/3000 training loss: 0.1448066234588623, evaluation loss: 0.17528875172138214\n",
      "1379/3000 training loss: 0.14459328353405, evaluation loss: 0.1750578135251999\n",
      "1380/3000 training loss: 0.1443803757429123, evaluation loss: 0.17482732236385345\n",
      "1381/3000 training loss: 0.14416790008544922, evaluation loss: 0.17459718883037567\n",
      "1382/3000 training loss: 0.14395582675933838, evaluation loss: 0.1743675321340561\n",
      "1383/3000 training loss: 0.14374420046806335, evaluation loss: 0.17413827776908875\n",
      "1384/3000 training loss: 0.14353299140930176, evaluation loss: 0.17390942573547363\n",
      "1385/3000 training loss: 0.1433221995830536, evaluation loss: 0.17368100583553314\n",
      "1386/3000 training loss: 0.14311183989048004, evaluation loss: 0.17345298826694489\n",
      "1387/3000 training loss: 0.14290188252925873, evaluation loss: 0.17322535812854767\n",
      "1388/3000 training loss: 0.14269234240055084, evaluation loss: 0.17299817502498627\n",
      "1389/3000 training loss: 0.14248321950435638, evaluation loss: 0.1727713942527771\n",
      "1390/3000 training loss: 0.14227451384067535, evaluation loss: 0.17254504561424255\n",
      "1391/3000 training loss: 0.14206624031066895, evaluation loss: 0.17231908440589905\n",
      "1392/3000 training loss: 0.14185835421085358, evaluation loss: 0.17209354043006897\n",
      "1393/3000 training loss: 0.14165088534355164, evaluation loss: 0.1718684285879135\n",
      "1394/3000 training loss: 0.14144383370876312, evaluation loss: 0.1716436743736267\n",
      "1395/3000 training loss: 0.14123719930648804, evaluation loss: 0.17141938209533691\n",
      "1396/3000 training loss: 0.14103096723556519, evaluation loss: 0.17119544744491577\n",
      "1397/3000 training loss: 0.14082515239715576, evaluation loss: 0.17097193002700806\n",
      "1398/3000 training loss: 0.14061972498893738, evaluation loss: 0.17074882984161377\n",
      "1399/3000 training loss: 0.14041472971439362, evaluation loss: 0.17052613198757172\n",
      "1400/3000 training loss: 0.1402101367712021, evaluation loss: 0.1703038364648819\n",
      "1401/3000 training loss: 0.1400059163570404, evaluation loss: 0.17008191347122192\n",
      "1402/3000 training loss: 0.13980214297771454, evaluation loss: 0.16986040771007538\n",
      "1403/3000 training loss: 0.1395987570285797, evaluation loss: 0.16963930428028107\n",
      "1404/3000 training loss: 0.13939575850963593, evaluation loss: 0.169418603181839\n",
      "1405/3000 training loss: 0.13919317722320557, evaluation loss: 0.16919828951358795\n",
      "1406/3000 training loss: 0.13899099826812744, evaluation loss: 0.16897836327552795\n",
      "1407/3000 training loss: 0.13878919184207916, evaluation loss: 0.1687588393688202\n",
      "1408/3000 training loss: 0.13858778774738312, evaluation loss: 0.16853970289230347\n",
      "1409/3000 training loss: 0.1383867859840393, evaluation loss: 0.16832096874713898\n",
      "1410/3000 training loss: 0.13818618655204773, evaluation loss: 0.16810265183448792\n",
      "1411/3000 training loss: 0.1379859745502472, evaluation loss: 0.16788466274738312\n",
      "1412/3000 training loss: 0.1377861499786377, evaluation loss: 0.16766710579395294\n",
      "1413/3000 training loss: 0.13758671283721924, evaluation loss: 0.1674499213695526\n",
      "1414/3000 training loss: 0.1373876929283142, evaluation loss: 0.16723312437534332\n",
      "1415/3000 training loss: 0.13718904554843903, evaluation loss: 0.16701672971248627\n",
      "1416/3000 training loss: 0.13699078559875488, evaluation loss: 0.16680069267749786\n",
      "1417/3000 training loss: 0.13679291307926178, evaluation loss: 0.1665850579738617\n",
      "1418/3000 training loss: 0.13659542798995972, evaluation loss: 0.16636981070041656\n",
      "1419/3000 training loss: 0.1363983303308487, evaluation loss: 0.16615493595600128\n",
      "1420/3000 training loss: 0.13620160520076752, evaluation loss: 0.16594044864177704\n",
      "1421/3000 training loss: 0.13600528240203857, evaluation loss: 0.16572631895542145\n",
      "1422/3000 training loss: 0.13580933213233948, evaluation loss: 0.1655125916004181\n",
      "1423/3000 training loss: 0.13561375439167023, evaluation loss: 0.16529925167560577\n",
      "1424/3000 training loss: 0.13541856408119202, evaluation loss: 0.1650862693786621\n",
      "1425/3000 training loss: 0.13522376120090485, evaluation loss: 0.16487367451190948\n",
      "1426/3000 training loss: 0.13502934575080872, evaluation loss: 0.1646614670753479\n",
      "1427/3000 training loss: 0.13483530282974243, evaluation loss: 0.16444960236549377\n",
      "1428/3000 training loss: 0.1346416175365448, evaluation loss: 0.1642381250858307\n",
      "1429/3000 training loss: 0.1344483345746994, evaluation loss: 0.16402702033519745\n",
      "1430/3000 training loss: 0.13425539433956146, evaluation loss: 0.16381631791591644\n",
      "1431/3000 training loss: 0.13406285643577576, evaluation loss: 0.1636059433221817\n",
      "1432/3000 training loss: 0.1338706761598587, evaluation loss: 0.1633959412574768\n",
      "1433/3000 training loss: 0.1336788684129715, evaluation loss: 0.16318632662296295\n",
      "1434/3000 training loss: 0.13348743319511414, evaluation loss: 0.16297709941864014\n",
      "1435/3000 training loss: 0.13329638540744781, evaluation loss: 0.1627682000398636\n",
      "1436/3000 training loss: 0.13310568034648895, evaluation loss: 0.16255968809127808\n",
      "1437/3000 training loss: 0.13291534781455994, evaluation loss: 0.1623515486717224\n",
      "1438/3000 training loss: 0.13272538781166077, evaluation loss: 0.1621437519788742\n",
      "1439/3000 training loss: 0.13253580033779144, evaluation loss: 0.16193629801273346\n",
      "1440/3000 training loss: 0.13234657049179077, evaluation loss: 0.16172923147678375\n",
      "1441/3000 training loss: 0.13215768337249756, evaluation loss: 0.1615225374698639\n",
      "1442/3000 training loss: 0.13196918368339539, evaluation loss: 0.16131621599197388\n",
      "1443/3000 training loss: 0.13178105652332306, evaluation loss: 0.16111022233963013\n",
      "1444/3000 training loss: 0.13159328699111938, evaluation loss: 0.16090461611747742\n",
      "1445/3000 training loss: 0.13140584528446198, evaluation loss: 0.16069933772087097\n",
      "1446/3000 training loss: 0.1312187910079956, evaluation loss: 0.16049443185329437\n",
      "1447/3000 training loss: 0.1310320794582367, evaluation loss: 0.16028986871242523\n",
      "1448/3000 training loss: 0.13084574043750763, evaluation loss: 0.16008564829826355\n",
      "1449/3000 training loss: 0.13065975904464722, evaluation loss: 0.1598818302154541\n",
      "1450/3000 training loss: 0.13047412037849426, evaluation loss: 0.15967832505702972\n",
      "1451/3000 training loss: 0.13028885424137115, evaluation loss: 0.1594752073287964\n",
      "1452/3000 training loss: 0.1301039159297943, evaluation loss: 0.15927241742610931\n",
      "1453/3000 training loss: 0.12991933524608612, evaluation loss: 0.1590699702501297\n",
      "1454/3000 training loss: 0.12973512709140778, evaluation loss: 0.15886786580085754\n",
      "1455/3000 training loss: 0.1295512467622757, evaluation loss: 0.15866614878177643\n",
      "1456/3000 training loss: 0.12936772406101227, evaluation loss: 0.15846474468708038\n",
      "1457/3000 training loss: 0.1291845440864563, evaluation loss: 0.158263698220253\n",
      "1458/3000 training loss: 0.12900172173976898, evaluation loss: 0.15806300938129425\n",
      "1459/3000 training loss: 0.12881924211978912, evaluation loss: 0.15786266326904297\n",
      "1460/3000 training loss: 0.12863712012767792, evaluation loss: 0.15766261518001556\n",
      "1461/3000 training loss: 0.12845531105995178, evaluation loss: 0.157462939620018\n",
      "1462/3000 training loss: 0.1282738745212555, evaluation loss: 0.1572636067867279\n",
      "1463/3000 training loss: 0.12809276580810547, evaluation loss: 0.15706466138362885\n",
      "1464/3000 training loss: 0.1279120147228241, evaluation loss: 0.15686599910259247\n",
      "1465/3000 training loss: 0.1277315765619278, evaluation loss: 0.15666767954826355\n",
      "1466/3000 training loss: 0.12755151093006134, evaluation loss: 0.15646971762180328\n",
      "1467/3000 training loss: 0.12737177312374115, evaluation loss: 0.1562720686197281\n",
      "1468/3000 training loss: 0.12719236314296722, evaluation loss: 0.15607477724552155\n",
      "1469/3000 training loss: 0.12701331079006195, evaluation loss: 0.15587782859802246\n",
      "1470/3000 training loss: 0.12683458626270294, evaluation loss: 0.15568119287490845\n",
      "1471/3000 training loss: 0.126656174659729, evaluation loss: 0.1554848998785019\n",
      "1472/3000 training loss: 0.12647812068462372, evaluation loss: 0.1552889496088028\n",
      "1473/3000 training loss: 0.1263003945350647, evaluation loss: 0.15509331226348877\n",
      "1474/3000 training loss: 0.12612301111221313, evaluation loss: 0.1548980325460434\n",
      "1475/3000 training loss: 0.12594594061374664, evaluation loss: 0.1547030806541443\n",
      "1476/3000 training loss: 0.1257692277431488, evaluation loss: 0.15450845658779144\n",
      "1477/3000 training loss: 0.12559282779693604, evaluation loss: 0.15431413054466248\n",
      "1478/3000 training loss: 0.12541675567626953, evaluation loss: 0.15412014722824097\n",
      "1479/3000 training loss: 0.12524102628231049, evaluation loss: 0.15392650663852692\n",
      "1480/3000 training loss: 0.1250656098127365, evaluation loss: 0.15373316407203674\n",
      "1481/3000 training loss: 0.1248905211687088, evaluation loss: 0.15354017913341522\n",
      "1482/3000 training loss: 0.12471576035022736, evaluation loss: 0.15334749221801758\n",
      "1483/3000 training loss: 0.12454132735729218, evaluation loss: 0.1531551480293274\n",
      "1484/3000 training loss: 0.12436721473932266, evaluation loss: 0.15296311676502228\n",
      "1485/3000 training loss: 0.12419342994689941, evaluation loss: 0.15277141332626343\n",
      "1486/3000 training loss: 0.12401995807886124, evaluation loss: 0.15258000791072845\n",
      "1487/3000 training loss: 0.12384681403636932, evaluation loss: 0.15238896012306213\n",
      "1488/3000 training loss: 0.12367398291826248, evaluation loss: 0.1521982103586197\n",
      "1489/3000 training loss: 0.1235014796257019, evaluation loss: 0.15200777351856232\n",
      "1490/3000 training loss: 0.1233292892575264, evaluation loss: 0.1518176645040512\n",
      "1491/3000 training loss: 0.12315741181373596, evaluation loss: 0.15162785351276398\n",
      "1492/3000 training loss: 0.1229858547449112, evaluation loss: 0.151438370347023\n",
      "1493/3000 training loss: 0.12281462550163269, evaluation loss: 0.15124918520450592\n",
      "1494/3000 training loss: 0.12264369428157806, evaluation loss: 0.15106035768985748\n",
      "1495/3000 training loss: 0.1224730983376503, evaluation loss: 0.15087179839611053\n",
      "1496/3000 training loss: 0.12230279296636581, evaluation loss: 0.15068358182907104\n",
      "1497/3000 training loss: 0.122132807970047, evaluation loss: 0.15049569308757782\n",
      "1498/3000 training loss: 0.12196315079927444, evaluation loss: 0.15030808746814728\n",
      "1499/3000 training loss: 0.12179379165172577, evaluation loss: 0.15012075006961823\n",
      "1500/3000 training loss: 0.12162473797798157, evaluation loss: 0.14993378520011902\n",
      "1501/3000 training loss: 0.12145599722862244, evaluation loss: 0.1497471034526825\n",
      "1502/3000 training loss: 0.12128756195306778, evaluation loss: 0.14956073462963104\n",
      "1503/3000 training loss: 0.1211194321513176, evaluation loss: 0.14937466382980347\n",
      "1504/3000 training loss: 0.12095160782337189, evaluation loss: 0.14918890595436096\n",
      "1505/3000 training loss: 0.12078408151865005, evaluation loss: 0.14900344610214233\n",
      "1506/3000 training loss: 0.12061687558889389, evaluation loss: 0.14881829917430878\n",
      "1507/3000 training loss: 0.1204499825835228, evaluation loss: 0.1486334502696991\n",
      "1508/3000 training loss: 0.12028336524963379, evaluation loss: 0.1484488993883133\n",
      "1509/3000 training loss: 0.12011707574129105, evaluation loss: 0.14826466143131256\n",
      "1510/3000 training loss: 0.11995106190443039, evaluation loss: 0.1480807065963745\n",
      "1511/3000 training loss: 0.1197853833436966, evaluation loss: 0.14789704978466034\n",
      "1512/3000 training loss: 0.11961996555328369, evaluation loss: 0.14771369099617004\n",
      "1513/3000 training loss: 0.11945486068725586, evaluation loss: 0.147530660033226\n",
      "1514/3000 training loss: 0.1192900612950325, evaluation loss: 0.14734789729118347\n",
      "1515/3000 training loss: 0.11912555992603302, evaluation loss: 0.1471654325723648\n",
      "1516/3000 training loss: 0.11896134167909622, evaluation loss: 0.14698326587677002\n",
      "1517/3000 training loss: 0.1187974214553833, evaluation loss: 0.1468014419078827\n",
      "1518/3000 training loss: 0.11863380670547485, evaluation loss: 0.14661984145641327\n",
      "1519/3000 training loss: 0.11847047507762909, evaluation loss: 0.1464385837316513\n",
      "1520/3000 training loss: 0.1183074414730072, evaluation loss: 0.14625757932662964\n",
      "1521/3000 training loss: 0.11814470589160919, evaluation loss: 0.14607690274715424\n",
      "1522/3000 training loss: 0.11798224598169327, evaluation loss: 0.14589646458625793\n",
      "1523/3000 training loss: 0.11782008409500122, evaluation loss: 0.1457163691520691\n",
      "1524/3000 training loss: 0.11765821278095245, evaluation loss: 0.14553652703762054\n",
      "1525/3000 training loss: 0.11749662458896637, evaluation loss: 0.14535699784755707\n",
      "1526/3000 training loss: 0.11733532696962357, evaluation loss: 0.14517775177955627\n",
      "1527/3000 training loss: 0.11717431247234344, evaluation loss: 0.14499878883361816\n",
      "1528/3000 training loss: 0.1170135885477066, evaluation loss: 0.14482010900974274\n",
      "1529/3000 training loss: 0.11685315519571304, evaluation loss: 0.1446416974067688\n",
      "1530/3000 training loss: 0.11669299006462097, evaluation loss: 0.14446356892585754\n",
      "1531/3000 training loss: 0.11653311550617218, evaluation loss: 0.14428575336933136\n",
      "1532/3000 training loss: 0.11637353897094727, evaluation loss: 0.14410822093486786\n",
      "1533/3000 training loss: 0.11621422320604324, evaluation loss: 0.14393095672130585\n",
      "1534/3000 training loss: 0.1160551980137825, evaluation loss: 0.14375393092632294\n",
      "1535/3000 training loss: 0.11589645594358444, evaluation loss: 0.14357726275920868\n",
      "1536/3000 training loss: 0.11573798954486847, evaluation loss: 0.14340084791183472\n",
      "1537/3000 training loss: 0.11557981371879578, evaluation loss: 0.14322468638420105\n",
      "1538/3000 training loss: 0.11542189866304398, evaluation loss: 0.14304882287979126\n",
      "1539/3000 training loss: 0.11526426672935486, evaluation loss: 0.14287324249744415\n",
      "1540/3000 training loss: 0.11510691046714783, evaluation loss: 0.14269791543483734\n",
      "1541/3000 training loss: 0.11494982242584229, evaluation loss: 0.1425228714942932\n",
      "1542/3000 training loss: 0.11479302495718002, evaluation loss: 0.14234811067581177\n",
      "1543/3000 training loss: 0.11463648080825806, evaluation loss: 0.1421736180782318\n",
      "1544/3000 training loss: 0.11448022723197937, evaluation loss: 0.14199940860271454\n",
      "1545/3000 training loss: 0.11432424187660217, evaluation loss: 0.14182545244693756\n",
      "1546/3000 training loss: 0.11416852474212646, evaluation loss: 0.14165176451206207\n",
      "1547/3000 training loss: 0.11401308327913284, evaluation loss: 0.14147835969924927\n",
      "1548/3000 training loss: 0.11385791748762131, evaluation loss: 0.14130523800849915\n",
      "1549/3000 training loss: 0.11370301246643066, evaluation loss: 0.1411323845386505\n",
      "1550/3000 training loss: 0.11354836821556091, evaluation loss: 0.14095978438854218\n",
      "1551/3000 training loss: 0.11339400708675385, evaluation loss: 0.14078743755817413\n",
      "1552/3000 training loss: 0.11323989927768707, evaluation loss: 0.14061537384986877\n",
      "1553/3000 training loss: 0.11308605968952179, evaluation loss: 0.1404435783624649\n",
      "1554/3000 training loss: 0.112932488322258, evaluation loss: 0.14027205109596252\n",
      "1555/3000 training loss: 0.1127791702747345, evaluation loss: 0.14010080695152283\n",
      "1556/3000 training loss: 0.11262613534927368, evaluation loss: 0.13992978632450104\n",
      "1557/3000 training loss: 0.11247335374355316, evaluation loss: 0.13975904881954193\n",
      "1558/3000 training loss: 0.11232082545757294, evaluation loss: 0.13958856463432312\n",
      "1559/3000 training loss: 0.1121685728430748, evaluation loss: 0.139418363571167\n",
      "1560/3000 training loss: 0.11201658844947815, evaluation loss: 0.13924840092658997\n",
      "1561/3000 training loss: 0.1118648424744606, evaluation loss: 0.13907872140407562\n",
      "1562/3000 training loss: 0.11171337217092514, evaluation loss: 0.13890928030014038\n",
      "1563/3000 training loss: 0.11156214028596878, evaluation loss: 0.13874009251594543\n",
      "1564/3000 training loss: 0.1114111840724945, evaluation loss: 0.13857118785381317\n",
      "1565/3000 training loss: 0.11126048117876053, evaluation loss: 0.13840252161026\n",
      "1566/3000 training loss: 0.11111003160476685, evaluation loss: 0.13823410868644714\n",
      "1567/3000 training loss: 0.11095983535051346, evaluation loss: 0.13806596398353577\n",
      "1568/3000 training loss: 0.11080989986658096, evaluation loss: 0.13789808750152588\n",
      "1569/3000 training loss: 0.11066020280122757, evaluation loss: 0.1377304494380951\n",
      "1570/3000 training loss: 0.11051078140735626, evaluation loss: 0.1375630646944046\n",
      "1571/3000 training loss: 0.11036159843206406, evaluation loss: 0.1373959183692932\n",
      "1572/3000 training loss: 0.11021266877651215, evaluation loss: 0.1372290402650833\n",
      "1573/3000 training loss: 0.11006399244070053, evaluation loss: 0.1370624303817749\n",
      "1574/3000 training loss: 0.10991556197404861, evaluation loss: 0.1368960589170456\n",
      "1575/3000 training loss: 0.1097673773765564, evaluation loss: 0.13672992587089539\n",
      "1576/3000 training loss: 0.10961944609880447, evaluation loss: 0.13656403124332428\n",
      "1577/3000 training loss: 0.10947176069021225, evaluation loss: 0.13639840483665466\n",
      "1578/3000 training loss: 0.10932432860136032, evaluation loss: 0.13623303174972534\n",
      "1579/3000 training loss: 0.1091771274805069, evaluation loss: 0.13606788218021393\n",
      "1580/3000 training loss: 0.10903019458055496, evaluation loss: 0.1359029859304428\n",
      "1581/3000 training loss: 0.10888348519802094, evaluation loss: 0.13573835790157318\n",
      "1582/3000 training loss: 0.1087370216846466, evaluation loss: 0.13557393848896027\n",
      "1583/3000 training loss: 0.10859081894159317, evaluation loss: 0.13540978729724884\n",
      "1584/3000 training loss: 0.10844484716653824, evaluation loss: 0.13524587452411652\n",
      "1585/3000 training loss: 0.10829911381006241, evaluation loss: 0.1350822150707245\n",
      "1586/3000 training loss: 0.10815362632274628, evaluation loss: 0.13491877913475037\n",
      "1587/3000 training loss: 0.10800837725400925, evaluation loss: 0.13475559651851654\n",
      "1588/3000 training loss: 0.10786336660385132, evaluation loss: 0.134592667222023\n",
      "1589/3000 training loss: 0.10771859437227249, evaluation loss: 0.1344299614429474\n",
      "1590/3000 training loss: 0.10757406800985336, evaluation loss: 0.13426750898361206\n",
      "1591/3000 training loss: 0.10742977261543274, evaluation loss: 0.13410528004169464\n",
      "1592/3000 training loss: 0.10728572309017181, evaluation loss: 0.13394328951835632\n",
      "1593/3000 training loss: 0.1071418970823288, evaluation loss: 0.1337815523147583\n",
      "1594/3000 training loss: 0.10699830949306488, evaluation loss: 0.13362003862857819\n",
      "1595/3000 training loss: 0.10685496032238007, evaluation loss: 0.13345876336097717\n",
      "1596/3000 training loss: 0.10671184957027435, evaluation loss: 0.13329774141311646\n",
      "1597/3000 training loss: 0.10656896978616714, evaluation loss: 0.13313691318035126\n",
      "1598/3000 training loss: 0.10642631351947784, evaluation loss: 0.13297635316848755\n",
      "1599/3000 training loss: 0.10628388822078705, evaluation loss: 0.13281604647636414\n",
      "1600/3000 training loss: 0.10614170134067535, evaluation loss: 0.13265591859817505\n",
      "1601/3000 training loss: 0.10599975287914276, evaluation loss: 0.13249607384204865\n",
      "1602/3000 training loss: 0.10585802793502808, evaluation loss: 0.13233642280101776\n",
      "1603/3000 training loss: 0.1057165265083313, evaluation loss: 0.13217702507972717\n",
      "1604/3000 training loss: 0.10557527840137482, evaluation loss: 0.1320178508758545\n",
      "1605/3000 training loss: 0.10543423146009445, evaluation loss: 0.1318589150905609\n",
      "1606/3000 training loss: 0.10529342293739319, evaluation loss: 0.13170021772384644\n",
      "1607/3000 training loss: 0.10515284538269043, evaluation loss: 0.13154172897338867\n",
      "1608/3000 training loss: 0.10501249134540558, evaluation loss: 0.13138347864151\n",
      "1609/3000 training loss: 0.10487235337495804, evaluation loss: 0.13122545182704926\n",
      "1610/3000 training loss: 0.104732446372509, evaluation loss: 0.1310676634311676\n",
      "1611/3000 training loss: 0.10459278523921967, evaluation loss: 0.13091011345386505\n",
      "1612/3000 training loss: 0.10445331782102585, evaluation loss: 0.1307527720928192\n",
      "1613/3000 training loss: 0.10431408882141113, evaluation loss: 0.13059565424919128\n",
      "1614/3000 training loss: 0.10417507588863373, evaluation loss: 0.13043877482414246\n",
      "1615/3000 training loss: 0.10403628647327423, evaluation loss: 0.13028213381767273\n",
      "1616/3000 training loss: 0.10389771312475204, evaluation loss: 0.13012567162513733\n",
      "1617/3000 training loss: 0.10375937074422836, evaluation loss: 0.12996944785118103\n",
      "1618/3000 training loss: 0.10362125188112259, evaluation loss: 0.12981346249580383\n",
      "1619/3000 training loss: 0.10348334163427353, evaluation loss: 0.12965771555900574\n",
      "1620/3000 training loss: 0.10334565490484238, evaluation loss: 0.12950214743614197\n",
      "1621/3000 training loss: 0.10320818424224854, evaluation loss: 0.1293468177318573\n",
      "1622/3000 training loss: 0.1030709370970726, evaluation loss: 0.12919172644615173\n",
      "1623/3000 training loss: 0.10293388366699219, evaluation loss: 0.12903684377670288\n",
      "1624/3000 training loss: 0.10279709100723267, evaluation loss: 0.12888218462467194\n",
      "1625/3000 training loss: 0.10266047716140747, evaluation loss: 0.1287277489900589\n",
      "1626/3000 training loss: 0.10252408683300018, evaluation loss: 0.12857352197170258\n",
      "1627/3000 training loss: 0.1023879200220108, evaluation loss: 0.12841951847076416\n",
      "1628/3000 training loss: 0.10225196182727814, evaluation loss: 0.12826573848724365\n",
      "1629/3000 training loss: 0.10211621969938278, evaluation loss: 0.12811218202114105\n",
      "1630/3000 training loss: 0.10198069363832474, evaluation loss: 0.12795883417129517\n",
      "1631/3000 training loss: 0.10184537619352341, evaluation loss: 0.12780572474002838\n",
      "1632/3000 training loss: 0.10171026736497879, evaluation loss: 0.12765279412269592\n",
      "1633/3000 training loss: 0.10157538950443268, evaluation loss: 0.12750011682510376\n",
      "1634/3000 training loss: 0.10144070535898209, evaluation loss: 0.12734763324260712\n",
      "1635/3000 training loss: 0.1013062372803688, evaluation loss: 0.12719537317752838\n",
      "1636/3000 training loss: 0.10117197781801224, evaluation loss: 0.12704332172870636\n",
      "1637/3000 training loss: 0.10103793442249298, evaluation loss: 0.12689147889614105\n",
      "1638/3000 training loss: 0.10090409219264984, evaluation loss: 0.12673987448215485\n",
      "1639/3000 training loss: 0.10077046602964401, evaluation loss: 0.12658847868442535\n",
      "1640/3000 training loss: 0.1006370335817337, evaluation loss: 0.12643729150295258\n",
      "1641/3000 training loss: 0.1005038321018219, evaluation loss: 0.1262863129377365\n",
      "1642/3000 training loss: 0.10037081688642502, evaluation loss: 0.12613557279109955\n",
      "1643/3000 training loss: 0.10023801773786545, evaluation loss: 0.1259850114583969\n",
      "1644/3000 training loss: 0.10010542720556259, evaluation loss: 0.12583467364311218\n",
      "1645/3000 training loss: 0.09997304528951645, evaluation loss: 0.12568454444408417\n",
      "1646/3000 training loss: 0.09984087198972702, evaluation loss: 0.12553462386131287\n",
      "1647/3000 training loss: 0.09970889240503311, evaluation loss: 0.12538494169712067\n",
      "1648/3000 training loss: 0.09957711398601532, evaluation loss: 0.12523546814918518\n",
      "1649/3000 training loss: 0.09944556653499603, evaluation loss: 0.12508618831634521\n",
      "1650/3000 training loss: 0.09931419789791107, evaluation loss: 0.12493713945150375\n",
      "1651/3000 training loss: 0.09918304532766342, evaluation loss: 0.12478826940059662\n",
      "1652/3000 training loss: 0.09905210137367249, evaluation loss: 0.12463963776826859\n",
      "1653/3000 training loss: 0.09892133623361588, evaluation loss: 0.12449120730161667\n",
      "1654/3000 training loss: 0.09879080206155777, evaluation loss: 0.12434297800064087\n",
      "1655/3000 training loss: 0.09866045415401459, evaluation loss: 0.12419498711824417\n",
      "1656/3000 training loss: 0.09853030741214752, evaluation loss: 0.1240471825003624\n",
      "1657/3000 training loss: 0.09840036183595657, evaluation loss: 0.12389960139989853\n",
      "1658/3000 training loss: 0.09827063232660294, evaluation loss: 0.12375221401453018\n",
      "1659/3000 training loss: 0.09814108908176422, evaluation loss: 0.12360504269599915\n",
      "1660/3000 training loss: 0.09801176190376282, evaluation loss: 0.12345808744430542\n",
      "1661/3000 training loss: 0.09788262099027634, evaluation loss: 0.12331132590770721\n",
      "1662/3000 training loss: 0.09775368124246597, evaluation loss: 0.12316478043794632\n",
      "1663/3000 training loss: 0.09762495011091232, evaluation loss: 0.12301845103502274\n",
      "1664/3000 training loss: 0.09749642014503479, evaluation loss: 0.12287233024835587\n",
      "1665/3000 training loss: 0.09736808389425278, evaluation loss: 0.12272641807794571\n",
      "1666/3000 training loss: 0.09723994135856628, evaluation loss: 0.12258069962263107\n",
      "1667/3000 training loss: 0.09711199253797531, evaluation loss: 0.12243519723415375\n",
      "1668/3000 training loss: 0.09698424488306046, evaluation loss: 0.12228991091251373\n",
      "1669/3000 training loss: 0.09685669839382172, evaluation loss: 0.12214482575654984\n",
      "1670/3000 training loss: 0.09672936797142029, evaluation loss: 0.12199994921684265\n",
      "1671/3000 training loss: 0.09660221636295319, evaluation loss: 0.12185526639223099\n",
      "1672/3000 training loss: 0.0964752659201622, evaluation loss: 0.12171080708503723\n",
      "1673/3000 training loss: 0.09634849429130554, evaluation loss: 0.12156654894351959\n",
      "1674/3000 training loss: 0.0962219387292862, evaluation loss: 0.12142249941825867\n",
      "1675/3000 training loss: 0.09609558433294296, evaluation loss: 0.12127867341041565\n",
      "1676/3000 training loss: 0.09596940875053406, evaluation loss: 0.12113502621650696\n",
      "1677/3000 training loss: 0.09584344923496246, evaluation loss: 0.12099158763885498\n",
      "1678/3000 training loss: 0.0957176685333252, evaluation loss: 0.1208483874797821\n",
      "1679/3000 training loss: 0.09559209644794464, evaluation loss: 0.12070535868406296\n",
      "1680/3000 training loss: 0.09546671062707901, evaluation loss: 0.12056256085634232\n",
      "1681/3000 training loss: 0.09534153342247009, evaluation loss: 0.12041997164487839\n",
      "1682/3000 training loss: 0.0952165424823761, evaluation loss: 0.12027758359909058\n",
      "1683/3000 training loss: 0.09509175270795822, evaluation loss: 0.12013540416955948\n",
      "1684/3000 training loss: 0.09496714174747467, evaluation loss: 0.1199934333562851\n",
      "1685/3000 training loss: 0.09484273195266724, evaluation loss: 0.11985167860984802\n",
      "1686/3000 training loss: 0.09471852332353592, evaluation loss: 0.11971011757850647\n",
      "1687/3000 training loss: 0.09459450840950012, evaluation loss: 0.11956877261400223\n",
      "1688/3000 training loss: 0.09447067975997925, evaluation loss: 0.11942761391401291\n",
      "1689/3000 training loss: 0.09434706717729568, evaluation loss: 0.11928669363260269\n",
      "1690/3000 training loss: 0.09422362595796585, evaluation loss: 0.1191459596157074\n",
      "1691/3000 training loss: 0.09410038590431213, evaluation loss: 0.11900544911623001\n",
      "1692/3000 training loss: 0.09397734701633453, evaluation loss: 0.11886516213417053\n",
      "1693/3000 training loss: 0.09385448694229126, evaluation loss: 0.11872503906488419\n",
      "1694/3000 training loss: 0.0937318280339241, evaluation loss: 0.11858515441417694\n",
      "1695/3000 training loss: 0.09360936284065247, evaluation loss: 0.11844545602798462\n",
      "1696/3000 training loss: 0.09348709136247635, evaluation loss: 0.1183059960603714\n",
      "1697/3000 training loss: 0.09336501359939575, evaluation loss: 0.1181667372584343\n",
      "1698/3000 training loss: 0.09324313700199127, evaluation loss: 0.11802767217159271\n",
      "1699/3000 training loss: 0.09312143921852112, evaluation loss: 0.11788881570100784\n",
      "1700/3000 training loss: 0.09299995005130768, evaluation loss: 0.11775016039609909\n",
      "1701/3000 training loss: 0.09287864714860916, evaluation loss: 0.11761173605918884\n",
      "1702/3000 training loss: 0.09275753796100616, evaluation loss: 0.11747352033853531\n",
      "1703/3000 training loss: 0.09263662248849869, evaluation loss: 0.1173354983329773\n",
      "1704/3000 training loss: 0.09251590073108673, evaluation loss: 0.11719770729541779\n",
      "1705/3000 training loss: 0.0923953726887703, evaluation loss: 0.1170601099729538\n",
      "1706/3000 training loss: 0.09227503091096878, evaluation loss: 0.11692272126674652\n",
      "1707/3000 training loss: 0.09215489029884338, evaluation loss: 0.11678554862737656\n",
      "1708/3000 training loss: 0.09203493595123291, evaluation loss: 0.11664857715368271\n",
      "1709/3000 training loss: 0.09191518276929855, evaluation loss: 0.11651184409856796\n",
      "1710/3000 training loss: 0.09179562330245972, evaluation loss: 0.11637529730796814\n",
      "1711/3000 training loss: 0.0916762501001358, evaluation loss: 0.11623896658420563\n",
      "1712/3000 training loss: 0.091557078063488, evaluation loss: 0.11610284447669983\n",
      "1713/3000 training loss: 0.09143808484077454, evaluation loss: 0.11596694588661194\n",
      "1714/3000 training loss: 0.09131929278373718, evaluation loss: 0.11583124101161957\n",
      "1715/3000 training loss: 0.09120069444179535, evaluation loss: 0.11569575220346451\n",
      "1716/3000 training loss: 0.09108228981494904, evaluation loss: 0.11556047201156616\n",
      "1717/3000 training loss: 0.09096407145261765, evaluation loss: 0.11542541533708572\n",
      "1718/3000 training loss: 0.09084604680538177, evaluation loss: 0.1152905598282814\n",
      "1719/3000 training loss: 0.09072822332382202, evaluation loss: 0.11515592783689499\n",
      "1720/3000 training loss: 0.09061059355735779, evaluation loss: 0.11502149701118469\n",
      "1721/3000 training loss: 0.09049316495656967, evaluation loss: 0.1148872897028923\n",
      "1722/3000 training loss: 0.09037592262029648, evaluation loss: 0.11475329101085663\n",
      "1723/3000 training loss: 0.09025886654853821, evaluation loss: 0.11461950838565826\n",
      "1724/3000 training loss: 0.09014199674129486, evaluation loss: 0.11448594182729721\n",
      "1725/3000 training loss: 0.09002533555030823, evaluation loss: 0.11435258388519287\n",
      "1726/3000 training loss: 0.08990886062383652, evaluation loss: 0.11421942710876465\n",
      "1727/3000 training loss: 0.08979257941246033, evaluation loss: 0.11408648639917374\n",
      "1728/3000 training loss: 0.08967648446559906, evaluation loss: 0.11395378410816193\n",
      "1729/3000 training loss: 0.0895606055855751, evaluation loss: 0.11382128298282623\n",
      "1730/3000 training loss: 0.08944489061832428, evaluation loss: 0.11368899047374725\n",
      "1731/3000 training loss: 0.08932938426733017, evaluation loss: 0.11355690658092499\n",
      "1732/3000 training loss: 0.08921407163143158, evaluation loss: 0.11342506110668182\n",
      "1733/3000 training loss: 0.08909895271062851, evaluation loss: 0.11329340189695358\n",
      "1734/3000 training loss: 0.08898402005434036, evaluation loss: 0.11316198855638504\n",
      "1735/3000 training loss: 0.08886928856372833, evaluation loss: 0.11303076148033142\n",
      "1736/3000 training loss: 0.08875475078821182, evaluation loss: 0.11289975792169571\n",
      "1737/3000 training loss: 0.08864040672779083, evaluation loss: 0.1127690002322197\n",
      "1738/3000 training loss: 0.08852625638246536, evaluation loss: 0.11263841390609741\n",
      "1739/3000 training loss: 0.08841229975223541, evaluation loss: 0.11250805854797363\n",
      "1740/3000 training loss: 0.08829852938652039, evaluation loss: 0.11237793415784836\n",
      "1741/3000 training loss: 0.08818496018648148, evaluation loss: 0.1122480034828186\n",
      "1742/3000 training loss: 0.08807157725095749, evaluation loss: 0.11211829632520676\n",
      "1743/3000 training loss: 0.08795841038227081, evaluation loss: 0.11198882758617401\n",
      "1744/3000 training loss: 0.08784541487693787, evaluation loss: 0.11185955256223679\n",
      "1745/3000 training loss: 0.08773261308670044, evaluation loss: 0.11173050105571747\n",
      "1746/3000 training loss: 0.08762001246213913, evaluation loss: 0.11160168796777725\n",
      "1747/3000 training loss: 0.08750761300325394, evaluation loss: 0.11147306859493256\n",
      "1748/3000 training loss: 0.08739539235830307, evaluation loss: 0.11134467273950577\n",
      "1749/3000 training loss: 0.08728336542844772, evaluation loss: 0.11121650040149689\n",
      "1750/3000 training loss: 0.08717156201601028, evaluation loss: 0.11108854413032532\n",
      "1751/3000 training loss: 0.08705992251634598, evaluation loss: 0.11096078902482986\n",
      "1752/3000 training loss: 0.08694848418235779, evaluation loss: 0.11083326488733292\n",
      "1753/3000 training loss: 0.08683723211288452, evaluation loss: 0.11070594936609268\n",
      "1754/3000 training loss: 0.08672619611024857, evaluation loss: 0.11057886481285095\n",
      "1755/3000 training loss: 0.08661534637212753, evaluation loss: 0.11045201122760773\n",
      "1756/3000 training loss: 0.08650466799736023, evaluation loss: 0.11032533645629883\n",
      "1757/3000 training loss: 0.08639419823884964, evaluation loss: 0.11019890010356903\n",
      "1758/3000 training loss: 0.08628392964601517, evaluation loss: 0.11007270961999893\n",
      "1759/3000 training loss: 0.08617385476827621, evaluation loss: 0.10994668304920197\n",
      "1760/3000 training loss: 0.08606395870447159, evaluation loss: 0.1098209097981453\n",
      "1761/3000 training loss: 0.08595426380634308, evaluation loss: 0.10969536006450653\n",
      "1762/3000 training loss: 0.08584476262331009, evaluation loss: 0.10957002639770508\n",
      "1763/3000 training loss: 0.08573545515537262, evaluation loss: 0.10944487899541855\n",
      "1764/3000 training loss: 0.08562634140253067, evaluation loss: 0.10931998491287231\n",
      "1765/3000 training loss: 0.08551742881536484, evaluation loss: 0.1091952994465828\n",
      "1766/3000 training loss: 0.08540870249271393, evaluation loss: 0.10907083004713058\n",
      "1767/3000 training loss: 0.08530016988515854, evaluation loss: 0.10894658416509628\n",
      "1768/3000 training loss: 0.08519183844327927, evaluation loss: 0.10882255434989929\n",
      "1769/3000 training loss: 0.08508368581533432, evaluation loss: 0.10869874060153961\n",
      "1770/3000 training loss: 0.08497574180364609, evaluation loss: 0.10857515782117844\n",
      "1771/3000 training loss: 0.08486797660589218, evaluation loss: 0.10845176875591278\n",
      "1772/3000 training loss: 0.08476042002439499, evaluation loss: 0.10832861810922623\n",
      "1773/3000 training loss: 0.08465304970741272, evaluation loss: 0.10820569097995758\n",
      "1774/3000 training loss: 0.08454588055610657, evaluation loss: 0.10808297246694565\n",
      "1775/3000 training loss: 0.08443888276815414, evaluation loss: 0.10796047747135162\n",
      "1776/3000 training loss: 0.08433210104703903, evaluation loss: 0.10783819854259491\n",
      "1777/3000 training loss: 0.08422549813985825, evaluation loss: 0.10771612823009491\n",
      "1778/3000 training loss: 0.08411908149719238, evaluation loss: 0.10759428888559341\n",
      "1779/3000 training loss: 0.08401288092136383, evaluation loss: 0.10747266560792923\n",
      "1780/3000 training loss: 0.08390685170888901, evaluation loss: 0.10735125839710236\n",
      "1781/3000 training loss: 0.0838010236620903, evaluation loss: 0.10723007470369339\n",
      "1782/3000 training loss: 0.08369538933038712, evaluation loss: 0.10710911452770233\n",
      "1783/3000 training loss: 0.08358994871377945, evaluation loss: 0.1069883480668068\n",
      "1784/3000 training loss: 0.0834846943616867, evaluation loss: 0.10686782002449036\n",
      "1785/3000 training loss: 0.08337964117527008, evaluation loss: 0.10674750059843063\n",
      "1786/3000 training loss: 0.08327477425336838, evaluation loss: 0.10662740468978882\n",
      "1787/3000 training loss: 0.0831700935959816, evaluation loss: 0.10650750994682312\n",
      "1788/3000 training loss: 0.08306561410427094, evaluation loss: 0.10638786107301712\n",
      "1789/3000 training loss: 0.0829613208770752, evaluation loss: 0.10626840591430664\n",
      "1790/3000 training loss: 0.08285722881555557, evaluation loss: 0.10614918172359467\n",
      "1791/3000 training loss: 0.08275331556797028, evaluation loss: 0.10603017359972\n",
      "1792/3000 training loss: 0.0826495960354805, evaluation loss: 0.10591137409210205\n",
      "1793/3000 training loss: 0.08254607021808624, evaluation loss: 0.10579279065132141\n",
      "1794/3000 training loss: 0.08244273066520691, evaluation loss: 0.10567442327737808\n",
      "1795/3000 training loss: 0.0823395848274231, evaluation loss: 0.10555627197027206\n",
      "1796/3000 training loss: 0.0822366327047348, evaluation loss: 0.10543834418058395\n",
      "1797/3000 training loss: 0.08213385939598083, evaluation loss: 0.10532062500715256\n",
      "1798/3000 training loss: 0.08203128725290298, evaluation loss: 0.10520312935113907\n",
      "1799/3000 training loss: 0.08192890137434006, evaluation loss: 0.10508584976196289\n",
      "1800/3000 training loss: 0.08182670176029205, evaluation loss: 0.10496876388788223\n",
      "1801/3000 training loss: 0.08172470331192017, evaluation loss: 0.10485190898180008\n",
      "1802/3000 training loss: 0.08162287622690201, evaluation loss: 0.10473526269197464\n",
      "1803/3000 training loss: 0.08152125030755997, evaluation loss: 0.10461883246898651\n",
      "1804/3000 training loss: 0.08141981065273285, evaluation loss: 0.1045026183128357\n",
      "1805/3000 training loss: 0.08131856471300125, evaluation loss: 0.10438662022352219\n",
      "1806/3000 training loss: 0.08121750503778458, evaluation loss: 0.10427083820104599\n",
      "1807/3000 training loss: 0.08111663907766342, evaluation loss: 0.10415524244308472\n",
      "1808/3000 training loss: 0.081015944480896, evaluation loss: 0.10403988510370255\n",
      "1809/3000 training loss: 0.0809154361486435, evaluation loss: 0.10392474383115768\n",
      "1810/3000 training loss: 0.0808151364326477, evaluation loss: 0.10380978882312775\n",
      "1811/3000 training loss: 0.08071502298116684, evaluation loss: 0.10369507223367691\n",
      "1812/3000 training loss: 0.0806150883436203, evaluation loss: 0.1035805493593216\n",
      "1813/3000 training loss: 0.08051533252000809, evaluation loss: 0.10346625000238419\n",
      "1814/3000 training loss: 0.08041577786207199, evaluation loss: 0.10335215926170349\n",
      "1815/3000 training loss: 0.08031639456748962, evaluation loss: 0.10323826968669891\n",
      "1816/3000 training loss: 0.08021721243858337, evaluation loss: 0.10312458872795105\n",
      "1817/3000 training loss: 0.08011820912361145, evaluation loss: 0.1030111312866211\n",
      "1818/3000 training loss: 0.08001939207315445, evaluation loss: 0.10289788246154785\n",
      "1819/3000 training loss: 0.07992075383663177, evaluation loss: 0.10278483480215073\n",
      "1820/3000 training loss: 0.07982231676578522, evaluation loss: 0.10267197340726852\n",
      "1821/3000 training loss: 0.07972405850887299, evaluation loss: 0.10255935788154602\n",
      "1822/3000 training loss: 0.07962597906589508, evaluation loss: 0.10244693607091904\n",
      "1823/3000 training loss: 0.0795280858874321, evaluation loss: 0.10233471542596817\n",
      "1824/3000 training loss: 0.07943038642406464, evaluation loss: 0.10222271829843521\n",
      "1825/3000 training loss: 0.0793328657746315, evaluation loss: 0.10211090743541718\n",
      "1826/3000 training loss: 0.0792355164885521, evaluation loss: 0.10199930518865585\n",
      "1827/3000 training loss: 0.0791383683681488, evaluation loss: 0.10188792645931244\n",
      "1828/3000 training loss: 0.07904139906167984, evaluation loss: 0.10177674144506454\n",
      "1829/3000 training loss: 0.0789446011185646, evaluation loss: 0.10166575759649277\n",
      "1830/3000 training loss: 0.07884799689054489, evaluation loss: 0.1015549749135971\n",
      "1831/3000 training loss: 0.0787515714764595, evaluation loss: 0.10144439339637756\n",
      "1832/3000 training loss: 0.07865533232688904, evaluation loss: 0.10133403539657593\n",
      "1833/3000 training loss: 0.0785592645406723, evaluation loss: 0.10122387856245041\n",
      "1834/3000 training loss: 0.07846338301897049, evaluation loss: 0.10111390799283981\n",
      "1835/3000 training loss: 0.078367680311203, evaluation loss: 0.10100414603948593\n",
      "1836/3000 training loss: 0.07827216386795044, evaluation loss: 0.10089460015296936\n",
      "1837/3000 training loss: 0.0781768262386322, evaluation loss: 0.10078524798154831\n",
      "1838/3000 training loss: 0.07808167487382889, evaluation loss: 0.10067608207464218\n",
      "1839/3000 training loss: 0.0779866948723793, evaluation loss: 0.10056712478399277\n",
      "1840/3000 training loss: 0.07789189368486404, evaluation loss: 0.10045836865901947\n",
      "1841/3000 training loss: 0.07779727876186371, evaluation loss: 0.10034981369972229\n",
      "1842/3000 training loss: 0.0777028426527977, evaluation loss: 0.10024145990610123\n",
      "1843/3000 training loss: 0.07760857790708542, evaluation loss: 0.10013329982757568\n",
      "1844/3000 training loss: 0.07751450687646866, evaluation loss: 0.10002534836530685\n",
      "1845/3000 training loss: 0.07742059230804443, evaluation loss: 0.09991757571697235\n",
      "1846/3000 training loss: 0.07732687145471573, evaluation loss: 0.09981001168489456\n",
      "1847/3000 training loss: 0.07723332196474075, evaluation loss: 0.09970264881849289\n",
      "1848/3000 training loss: 0.0771399512887001, evaluation loss: 0.09959547221660614\n",
      "1849/3000 training loss: 0.07704675197601318, evaluation loss: 0.0994885042309761\n",
      "1850/3000 training loss: 0.07695374637842178, evaluation loss: 0.09938173741102219\n",
      "1851/3000 training loss: 0.07686089724302292, evaluation loss: 0.099275141954422\n",
      "1852/3000 training loss: 0.07676822692155838, evaluation loss: 0.09916874766349792\n",
      "1853/3000 training loss: 0.07667573541402817, evaluation loss: 0.09906256198883057\n",
      "1854/3000 training loss: 0.07658342272043228, evaluation loss: 0.09895656257867813\n",
      "1855/3000 training loss: 0.07649129629135132, evaluation loss: 0.09885074943304062\n",
      "1856/3000 training loss: 0.07639932632446289, evaluation loss: 0.09874514490365982\n",
      "1857/3000 training loss: 0.07630754262208939, evaluation loss: 0.09863971918821335\n",
      "1858/3000 training loss: 0.07621593028306961, evaluation loss: 0.098534494638443\n",
      "1859/3000 training loss: 0.07612448185682297, evaluation loss: 0.09842944890260696\n",
      "1860/3000 training loss: 0.07603321969509125, evaluation loss: 0.09832459688186646\n",
      "1861/3000 training loss: 0.07594212144613266, evaluation loss: 0.09821993112564087\n",
      "1862/3000 training loss: 0.0758511945605278, evaluation loss: 0.0981154665350914\n",
      "1863/3000 training loss: 0.07576044648885727, evaluation loss: 0.09801118820905685\n",
      "1864/3000 training loss: 0.07566986978054047, evaluation loss: 0.09790708869695663\n",
      "1865/3000 training loss: 0.0755794569849968, evaluation loss: 0.09780317544937134\n",
      "1866/3000 training loss: 0.07548923045396805, evaluation loss: 0.09769947081804276\n",
      "1867/3000 training loss: 0.07539916783571243, evaluation loss: 0.0975959450006485\n",
      "1868/3000 training loss: 0.07530926913022995, evaluation loss: 0.09749259054660797\n",
      "1869/3000 training loss: 0.0752195417881012, evaluation loss: 0.09738943725824356\n",
      "1870/3000 training loss: 0.07513000071048737, evaluation loss: 0.09728645533323288\n",
      "1871/3000 training loss: 0.07504061609506607, evaluation loss: 0.09718368202447891\n",
      "1872/3000 training loss: 0.0749514028429985, evaluation loss: 0.09708108007907867\n",
      "1873/3000 training loss: 0.07486235350370407, evaluation loss: 0.09697866439819336\n",
      "1874/3000 training loss: 0.07477348297834396, evaluation loss: 0.09687642753124237\n",
      "1875/3000 training loss: 0.07468476891517639, evaluation loss: 0.0967743769288063\n",
      "1876/3000 training loss: 0.07459623366594315, evaluation loss: 0.09667252004146576\n",
      "1877/3000 training loss: 0.07450786232948303, evaluation loss: 0.09657083451747894\n",
      "1878/3000 training loss: 0.07441966235637665, evaluation loss: 0.09646932780742645\n",
      "1879/3000 training loss: 0.0743316113948822, evaluation loss: 0.09636801481246948\n",
      "1880/3000 training loss: 0.07424375414848328, evaluation loss: 0.09626687318086624\n",
      "1881/3000 training loss: 0.07415604591369629, evaluation loss: 0.09616589546203613\n",
      "1882/3000 training loss: 0.07406850904226303, evaluation loss: 0.09606511145830154\n",
      "1883/3000 training loss: 0.0739811360836029, evaluation loss: 0.09596451371908188\n",
      "1884/3000 training loss: 0.07389392703771591, evaluation loss: 0.09586408734321594\n",
      "1885/3000 training loss: 0.07380688190460205, evaluation loss: 0.09576385468244553\n",
      "1886/3000 training loss: 0.07372001558542252, evaluation loss: 0.09566378593444824\n",
      "1887/3000 training loss: 0.07363330572843552, evaluation loss: 0.09556388854980469\n",
      "1888/3000 training loss: 0.07354675978422165, evaluation loss: 0.09546417742967606\n",
      "1889/3000 training loss: 0.07346037030220032, evaluation loss: 0.09536465257406235\n",
      "1890/3000 training loss: 0.07337415963411331, evaluation loss: 0.09526528418064117\n",
      "1891/3000 training loss: 0.07328809797763824, evaluation loss: 0.09516610205173492\n",
      "1892/3000 training loss: 0.0732022076845169, evaluation loss: 0.0950670912861824\n",
      "1893/3000 training loss: 0.0731164738535881, evaluation loss: 0.09496825188398361\n",
      "1894/3000 training loss: 0.07303091138601303, evaluation loss: 0.09486959874629974\n",
      "1895/3000 training loss: 0.0729454979300499, evaluation loss: 0.09477110952138901\n",
      "1896/3000 training loss: 0.07286025583744049, evaluation loss: 0.09467282146215439\n",
      "1897/3000 training loss: 0.07277517020702362, evaluation loss: 0.09457466751337051\n",
      "1898/3000 training loss: 0.07269024848937988, evaluation loss: 0.09447668492794037\n",
      "1899/3000 training loss: 0.07260547578334808, evaluation loss: 0.09437888860702515\n",
      "1900/3000 training loss: 0.07252088189125061, evaluation loss: 0.09428126364946365\n",
      "1901/3000 training loss: 0.07243643701076508, evaluation loss: 0.09418381750583649\n",
      "1902/3000 training loss: 0.07235215604305267, evaluation loss: 0.09408653527498245\n",
      "1903/3000 training loss: 0.07226802408695221, evaluation loss: 0.09398941695690155\n",
      "1904/3000 training loss: 0.07218404859304428, evaluation loss: 0.09389247745275497\n",
      "1905/3000 training loss: 0.07210025191307068, evaluation loss: 0.09379569441080093\n",
      "1906/3000 training loss: 0.07201658934354782, evaluation loss: 0.09369908273220062\n",
      "1907/3000 training loss: 0.07193310558795929, evaluation loss: 0.09360264241695404\n",
      "1908/3000 training loss: 0.0718497708439827, evaluation loss: 0.0935063511133194\n",
      "1909/3000 training loss: 0.07176660001277924, evaluation loss: 0.09341026842594147\n",
      "1910/3000 training loss: 0.07168357819318771, evaluation loss: 0.09331432729959488\n",
      "1911/3000 training loss: 0.07160071283578873, evaluation loss: 0.09321853518486023\n",
      "1912/3000 training loss: 0.07151799649000168, evaluation loss: 0.0931229367852211\n",
      "1913/3000 training loss: 0.07143545150756836, evaluation loss: 0.0930274948477745\n",
      "1914/3000 training loss: 0.07135305553674698, evaluation loss: 0.09293221682310104\n",
      "1915/3000 training loss: 0.07127080112695694, evaluation loss: 0.09283711016178131\n",
      "1916/3000 training loss: 0.07118872553110123, evaluation loss: 0.09274215251207352\n",
      "1917/3000 training loss: 0.07110678404569626, evaluation loss: 0.09264736622571945\n",
      "1918/3000 training loss: 0.07102501392364502, evaluation loss: 0.09255274385213852\n",
      "1919/3000 training loss: 0.07094338536262512, evaluation loss: 0.09245829284191132\n",
      "1920/3000 training loss: 0.07086192071437836, evaluation loss: 0.09236398339271545\n",
      "1921/3000 training loss: 0.07078059762716293, evaluation loss: 0.09226985275745392\n",
      "1922/3000 training loss: 0.07069943100214005, evaluation loss: 0.09217587858438492\n",
      "1923/3000 training loss: 0.07061842083930969, evaluation loss: 0.09208204597234726\n",
      "1924/3000 training loss: 0.07053756713867188, evaluation loss: 0.09198839217424393\n",
      "1925/3000 training loss: 0.0704568475484848, evaluation loss: 0.09189490228891373\n",
      "1926/3000 training loss: 0.07037629932165146, evaluation loss: 0.09180156886577606\n",
      "1927/3000 training loss: 0.07029588520526886, evaluation loss: 0.09170838445425034\n",
      "1928/3000 training loss: 0.0702156350016594, evaluation loss: 0.09161537140607834\n",
      "1929/3000 training loss: 0.07013552635908127, evaluation loss: 0.09152250736951828\n",
      "1930/3000 training loss: 0.07005557417869568, evaluation loss: 0.09142980724573135\n",
      "1931/3000 training loss: 0.06997577100992203, evaluation loss: 0.09133727103471756\n",
      "1932/3000 training loss: 0.06989610940217972, evaluation loss: 0.09124487638473511\n",
      "1933/3000 training loss: 0.06981661170721054, evaluation loss: 0.09115265309810638\n",
      "1934/3000 training loss: 0.0697372555732727, evaluation loss: 0.091060571372509\n",
      "1935/3000 training loss: 0.06965804845094681, evaluation loss: 0.09096865355968475\n",
      "1936/3000 training loss: 0.06957898288965225, evaluation loss: 0.09087689220905304\n",
      "1937/3000 training loss: 0.06950008124113083, evaluation loss: 0.09078527241945267\n",
      "1938/3000 training loss: 0.06942132115364075, evaluation loss: 0.09069382399320602\n",
      "1939/3000 training loss: 0.0693427100777626, evaluation loss: 0.09060253947973251\n",
      "1940/3000 training loss: 0.0692642405629158, evaluation loss: 0.09051139652729034\n",
      "1941/3000 training loss: 0.06918591260910034, evaluation loss: 0.09042040258646011\n",
      "1942/3000 training loss: 0.06910774111747742, evaluation loss: 0.09032956510782242\n",
      "1943/3000 training loss: 0.06902971863746643, evaluation loss: 0.09023886919021606\n",
      "1944/3000 training loss: 0.06895182281732559, evaluation loss: 0.09014833718538284\n",
      "1945/3000 training loss: 0.06887408345937729, evaluation loss: 0.09005796164274216\n",
      "1946/3000 training loss: 0.06879650056362152, evaluation loss: 0.08996772021055222\n",
      "1947/3000 training loss: 0.0687190443277359, evaluation loss: 0.0898776426911354\n",
      "1948/3000 training loss: 0.06864174455404282, evaluation loss: 0.08978772163391113\n",
      "1949/3000 training loss: 0.06856458634138107, evaluation loss: 0.0896979495882988\n",
      "1950/3000 training loss: 0.06848757714033127, evaluation loss: 0.08960830420255661\n",
      "1951/3000 training loss: 0.06841070204973221, evaluation loss: 0.08951882272958755\n",
      "1952/3000 training loss: 0.06833396852016449, evaluation loss: 0.08942948281764984\n",
      "1953/3000 training loss: 0.06825738400220871, evaluation loss: 0.08934031426906586\n",
      "1954/3000 training loss: 0.06818095594644547, evaluation loss: 0.08925128728151321\n",
      "1955/3000 training loss: 0.06810465455055237, evaluation loss: 0.08916239440441132\n",
      "1956/3000 training loss: 0.06802848726511002, evaluation loss: 0.08907365798950195\n",
      "1957/3000 training loss: 0.0679524764418602, evaluation loss: 0.08898506313562393\n",
      "1958/3000 training loss: 0.06787659227848053, evaluation loss: 0.08889662474393845\n",
      "1959/3000 training loss: 0.06780087202787399, evaluation loss: 0.0888083204627037\n",
      "1960/3000 training loss: 0.067725270986557, evaluation loss: 0.0887201726436615\n",
      "1961/3000 training loss: 0.06764982640743256, evaluation loss: 0.08863216638565063\n",
      "1962/3000 training loss: 0.06757450848817825, evaluation loss: 0.08854428678750992\n",
      "1963/3000 training loss: 0.06749933958053589, evaluation loss: 0.08845659345388412\n",
      "1964/3000 training loss: 0.06742430478334427, evaluation loss: 0.08836902678012848\n",
      "1965/3000 training loss: 0.06734941154718399, evaluation loss: 0.08828159421682358\n",
      "1966/3000 training loss: 0.06727465987205505, evaluation loss: 0.08819431066513062\n",
      "1967/3000 training loss: 0.06720004975795746, evaluation loss: 0.088107168674469\n",
      "1968/3000 training loss: 0.06712556630373001, evaluation loss: 0.08802018314599991\n",
      "1969/3000 training loss: 0.0670512244105339, evaluation loss: 0.08793333172798157\n",
      "1970/3000 training loss: 0.06697702407836914, evaluation loss: 0.08784662187099457\n",
      "1971/3000 training loss: 0.06690296530723572, evaluation loss: 0.08776003867387772\n",
      "1972/3000 training loss: 0.06682904064655304, evaluation loss: 0.087673619389534\n",
      "1973/3000 training loss: 0.0667552500963211, evaluation loss: 0.08758733421564102\n",
      "1974/3000 training loss: 0.06668160855770111, evaluation loss: 0.08750119060277939\n",
      "1975/3000 training loss: 0.06660808622837067, evaluation loss: 0.0874151960015297\n",
      "1976/3000 training loss: 0.06653469800949097, evaluation loss: 0.08732932806015015\n",
      "1977/3000 training loss: 0.0664614588022232, evaluation loss: 0.08724360167980194\n",
      "1978/3000 training loss: 0.06638834625482559, evaluation loss: 0.08715803176164627\n",
      "1979/3000 training loss: 0.06631537526845932, evaluation loss: 0.08707258850336075\n",
      "1980/3000 training loss: 0.0662425309419632, evaluation loss: 0.08698727935552597\n",
      "1981/3000 training loss: 0.06616983562707901, evaluation loss: 0.08690211921930313\n",
      "1982/3000 training loss: 0.06609725952148438, evaluation loss: 0.08681709319353104\n",
      "1983/3000 training loss: 0.06602482497692108, evaluation loss: 0.08673220872879028\n",
      "1984/3000 training loss: 0.06595252454280853, evaluation loss: 0.08664745092391968\n",
      "1985/3000 training loss: 0.06588036566972733, evaluation loss: 0.08656284213066101\n",
      "1986/3000 training loss: 0.06580831855535507, evaluation loss: 0.08647838234901428\n",
      "1987/3000 training loss: 0.06573642045259476, evaluation loss: 0.08639402687549591\n",
      "1988/3000 training loss: 0.06566464900970459, evaluation loss: 0.08630983531475067\n",
      "1989/3000 training loss: 0.06559299677610397, evaluation loss: 0.08622577041387558\n",
      "1990/3000 training loss: 0.06552150100469589, evaluation loss: 0.08614183962345123\n",
      "1991/3000 training loss: 0.06545011699199677, evaluation loss: 0.08605805039405823\n",
      "1992/3000 training loss: 0.06537888944149017, evaluation loss: 0.08597439527511597\n",
      "1993/3000 training loss: 0.06530777364969254, evaluation loss: 0.08589088171720505\n",
      "1994/3000 training loss: 0.06523679196834564, evaluation loss: 0.08580749481916428\n",
      "1995/3000 training loss: 0.0651659369468689, evaluation loss: 0.08572423458099365\n",
      "1996/3000 training loss: 0.0650952160358429, evaluation loss: 0.08564111590385437\n",
      "1997/3000 training loss: 0.06502462923526764, evaluation loss: 0.08555812388658524\n",
      "1998/3000 training loss: 0.06495416164398193, evaluation loss: 0.08547528088092804\n",
      "1999/3000 training loss: 0.06488383561372757, evaluation loss: 0.0853925570845604\n",
      "2000/3000 training loss: 0.06481362879276276, evaluation loss: 0.0853099673986435\n",
      "2001/3000 training loss: 0.06474355608224869, evaluation loss: 0.08522752672433853\n",
      "2002/3000 training loss: 0.06467361003160477, evaluation loss: 0.08514519035816193\n",
      "2003/3000 training loss: 0.064603790640831, evaluation loss: 0.08506301045417786\n",
      "2004/3000 training loss: 0.06453410536050797, evaluation loss: 0.08498094230890274\n",
      "2005/3000 training loss: 0.06446453183889389, evaluation loss: 0.08489902317523956\n",
      "2006/3000 training loss: 0.06439509987831116, evaluation loss: 0.08481723070144653\n",
      "2007/3000 training loss: 0.06432579457759857, evaluation loss: 0.08473555743694305\n",
      "2008/3000 training loss: 0.06425660848617554, evaluation loss: 0.08465401828289032\n",
      "2009/3000 training loss: 0.06418756395578384, evaluation loss: 0.08457261323928833\n",
      "2010/3000 training loss: 0.0641186311841011, evaluation loss: 0.08449134975671768\n",
      "2011/3000 training loss: 0.06404983252286911, evaluation loss: 0.08441020548343658\n",
      "2012/3000 training loss: 0.06398114562034607, evaluation loss: 0.08432918041944504\n",
      "2013/3000 training loss: 0.06391260027885437, evaluation loss: 0.08424829691648483\n",
      "2014/3000 training loss: 0.06384417414665222, evaluation loss: 0.08416754752397537\n",
      "2015/3000 training loss: 0.06377587467432022, evaluation loss: 0.08408690989017487\n",
      "2016/3000 training loss: 0.06370769441127777, evaluation loss: 0.0840064138174057\n",
      "2017/3000 training loss: 0.06363964080810547, evaluation loss: 0.08392603695392609\n",
      "2018/3000 training loss: 0.06357170641422272, evaluation loss: 0.08384579420089722\n",
      "2019/3000 training loss: 0.06350390613079071, evaluation loss: 0.0837656706571579\n",
      "2020/3000 training loss: 0.06343622505664825, evaluation loss: 0.08368568867444992\n",
      "2021/3000 training loss: 0.06336866319179535, evaluation loss: 0.0836058109998703\n",
      "2022/3000 training loss: 0.06330124288797379, evaluation loss: 0.08352609723806381\n",
      "2023/3000 training loss: 0.06323391944169998, evaluation loss: 0.08344647288322449\n",
      "2024/3000 training loss: 0.06316673010587692, evaluation loss: 0.0833669900894165\n",
      "2025/3000 training loss: 0.06309965997934341, evaluation loss: 0.08328762650489807\n",
      "2026/3000 training loss: 0.06303270906209946, evaluation loss: 0.08320838958024979\n",
      "2027/3000 training loss: 0.06296588480472565, evaluation loss: 0.08312930166721344\n",
      "2028/3000 training loss: 0.06289917975664139, evaluation loss: 0.08305031061172485\n",
      "2029/3000 training loss: 0.06283260136842728, evaluation loss: 0.08297145366668701\n",
      "2030/3000 training loss: 0.06276614218950272, evaluation loss: 0.08289271593093872\n",
      "2031/3000 training loss: 0.0626998022198677, evaluation loss: 0.08281409740447998\n",
      "2032/3000 training loss: 0.06263358145952225, evaluation loss: 0.08273561298847198\n",
      "2033/3000 training loss: 0.06256747245788574, evaluation loss: 0.08265724778175354\n",
      "2034/3000 training loss: 0.06250149756669998, evaluation loss: 0.08257900923490524\n",
      "2035/3000 training loss: 0.062435634434223175, evaluation loss: 0.0825008898973465\n",
      "2036/3000 training loss: 0.062369897961616516, evaluation loss: 0.0824228972196579\n",
      "2037/3000 training loss: 0.06230427324771881, evaluation loss: 0.08234502375125885\n",
      "2038/3000 training loss: 0.06223876029253006, evaluation loss: 0.08226728439331055\n",
      "2039/3000 training loss: 0.062173373997211456, evaluation loss: 0.0821896493434906\n",
      "2040/3000 training loss: 0.062108106911182404, evaluation loss: 0.0821121409535408\n",
      "2041/3000 training loss: 0.0620429553091526, evaluation loss: 0.08203475177288055\n",
      "2042/3000 training loss: 0.06197792664170265, evaluation loss: 0.08195748925209045\n",
      "2043/3000 training loss: 0.061913009732961655, evaluation loss: 0.0818803459405899\n",
      "2044/3000 training loss: 0.06184820458292961, evaluation loss: 0.0818033292889595\n",
      "2045/3000 training loss: 0.06178351864218712, evaluation loss: 0.08172643184661865\n",
      "2046/3000 training loss: 0.061718959361314774, evaluation loss: 0.08164963871240616\n",
      "2047/3000 training loss: 0.061654504388570786, evaluation loss: 0.081572987139225\n",
      "2048/3000 training loss: 0.06159017235040665, evaluation loss: 0.08149643242359161\n",
      "2049/3000 training loss: 0.06152594834566116, evaluation loss: 0.08142000436782837\n",
      "2050/3000 training loss: 0.061461854726076126, evaluation loss: 0.08134370297193527\n",
      "2051/3000 training loss: 0.061397869139909744, evaluation loss: 0.08126752823591232\n",
      "2052/3000 training loss: 0.06133399158716202, evaluation loss: 0.08119145780801773\n",
      "2053/3000 training loss: 0.06127023324370384, evaluation loss: 0.08111549913883209\n",
      "2054/3000 training loss: 0.06120658665895462, evaluation loss: 0.0810396671295166\n",
      "2055/3000 training loss: 0.06114306300878525, evaluation loss: 0.08096396178007126\n",
      "2056/3000 training loss: 0.061079639941453934, evaluation loss: 0.08088836073875427\n",
      "2057/3000 training loss: 0.06101634353399277, evaluation loss: 0.08081287890672684\n",
      "2058/3000 training loss: 0.060953158885240555, evaluation loss: 0.08073751628398895\n",
      "2059/3000 training loss: 0.0608900748193264, evaluation loss: 0.08066228032112122\n",
      "2060/3000 training loss: 0.060827113687992096, evaluation loss: 0.08058715611696243\n",
      "2061/3000 training loss: 0.06076426804065704, evaluation loss: 0.0805121436715126\n",
      "2062/3000 training loss: 0.060701530426740646, evaluation loss: 0.08043725043535233\n",
      "2063/3000 training loss: 0.060638900846242905, evaluation loss: 0.0803624615073204\n",
      "2064/3000 training loss: 0.060576386749744415, evaluation loss: 0.08028781414031982\n",
      "2065/3000 training loss: 0.06051399186253548, evaluation loss: 0.080213263630867\n",
      "2066/3000 training loss: 0.060451701283454895, evaluation loss: 0.08013883233070374\n",
      "2067/3000 training loss: 0.06038951873779297, evaluation loss: 0.08006452023983002\n",
      "2068/3000 training loss: 0.06032745540142059, evaluation loss: 0.07999031245708466\n",
      "2069/3000 training loss: 0.06026550382375717, evaluation loss: 0.07991623133420944\n",
      "2070/3000 training loss: 0.06020364910364151, evaluation loss: 0.07984225451946259\n",
      "2071/3000 training loss: 0.0601419135928154, evaluation loss: 0.07976839691400528\n",
      "2072/3000 training loss: 0.06008029729127884, evaluation loss: 0.07969464361667633\n",
      "2073/3000 training loss: 0.06001877784729004, evaluation loss: 0.07962101697921753\n",
      "2074/3000 training loss: 0.059957366436719894, evaluation loss: 0.07954750210046768\n",
      "2075/3000 training loss: 0.059896070510149, evaluation loss: 0.07947409898042679\n",
      "2076/3000 training loss: 0.05983487889170647, evaluation loss: 0.07940080016851425\n",
      "2077/3000 training loss: 0.059773799031972885, evaluation loss: 0.07932761311531067\n",
      "2078/3000 training loss: 0.05971282348036766, evaluation loss: 0.07925454527139664\n",
      "2079/3000 training loss: 0.05965195968747139, evaluation loss: 0.07918160408735275\n",
      "2080/3000 training loss: 0.059591203927993774, evaluation loss: 0.07910875976085663\n",
      "2081/3000 training loss: 0.05953056365251541, evaluation loss: 0.07903602719306946\n",
      "2082/3000 training loss: 0.05947002023458481, evaluation loss: 0.07896341383457184\n",
      "2083/3000 training loss: 0.05940958485007286, evaluation loss: 0.07889091223478317\n",
      "2084/3000 training loss: 0.05934926122426987, evaluation loss: 0.07881850749254227\n",
      "2085/3000 training loss: 0.05928903445601463, evaluation loss: 0.07874621450901031\n",
      "2086/3000 training loss: 0.05922892689704895, evaluation loss: 0.07867404073476791\n",
      "2087/3000 training loss: 0.059168923646211624, evaluation loss: 0.07860197871923447\n",
      "2088/3000 training loss: 0.05910902097821236, evaluation loss: 0.07853001356124878\n",
      "2089/3000 training loss: 0.059049222618341446, evaluation loss: 0.07845817506313324\n",
      "2090/3000 training loss: 0.05898953229188919, evaluation loss: 0.07838642597198486\n",
      "2091/3000 training loss: 0.05892995372414589, evaluation loss: 0.07831481099128723\n",
      "2092/3000 training loss: 0.058870475739240646, evaluation loss: 0.07824330031871796\n",
      "2093/3000 training loss: 0.05881110578775406, evaluation loss: 0.07817189395427704\n",
      "2094/3000 training loss: 0.05875183269381523, evaluation loss: 0.07810059934854507\n",
      "2095/3000 training loss: 0.05869267135858536, evaluation loss: 0.07802940905094147\n",
      "2096/3000 training loss: 0.05863361060619354, evaluation loss: 0.07795833796262741\n",
      "2097/3000 training loss: 0.058574654161930084, evaluation loss: 0.07788736373186111\n",
      "2098/3000 training loss: 0.05851580202579498, evaluation loss: 0.07781647890806198\n",
      "2099/3000 training loss: 0.05845705419778824, evaluation loss: 0.077745720744133\n",
      "2100/3000 training loss: 0.05839840695261955, evaluation loss: 0.07767508178949356\n",
      "2101/3000 training loss: 0.05833986774086952, evaluation loss: 0.07760453224182129\n",
      "2102/3000 training loss: 0.058281417936086655, evaluation loss: 0.07753409445285797\n",
      "2103/3000 training loss: 0.05822308361530304, evaluation loss: 0.07746376097202301\n",
      "2104/3000 training loss: 0.058164846152067184, evaluation loss: 0.0773935467004776\n",
      "2105/3000 training loss: 0.05810672417283058, evaluation loss: 0.07732343673706055\n",
      "2106/3000 training loss: 0.058048687875270844, evaluation loss: 0.07725342363119125\n",
      "2107/3000 training loss: 0.05799075588583946, evaluation loss: 0.07718352228403091\n",
      "2108/3000 training loss: 0.057932931929826736, evaluation loss: 0.07711372524499893\n",
      "2109/3000 training loss: 0.057875197380781174, evaluation loss: 0.0770440325140953\n",
      "2110/3000 training loss: 0.057817574590444565, evaluation loss: 0.07697445154190063\n",
      "2111/3000 training loss: 0.057760048657655716, evaluation loss: 0.07690498232841492\n",
      "2112/3000 training loss: 0.057702623307704926, evaluation loss: 0.07683559507131577\n",
      "2113/3000 training loss: 0.05764530599117279, evaluation loss: 0.07676632702350616\n",
      "2114/3000 training loss: 0.05758807808160782, evaluation loss: 0.07669716328382492\n",
      "2115/3000 training loss: 0.057530954480171204, evaluation loss: 0.07662809640169144\n",
      "2116/3000 training loss: 0.05747393146157265, evaluation loss: 0.0765591412782669\n",
      "2117/3000 training loss: 0.057416997849941254, evaluation loss: 0.07649029791355133\n",
      "2118/3000 training loss: 0.057360175997018814, evaluation loss: 0.07642155885696411\n",
      "2119/3000 training loss: 0.05730345472693443, evaluation loss: 0.07635290920734406\n",
      "2120/3000 training loss: 0.057246822863817215, evaluation loss: 0.07628435641527176\n",
      "2121/3000 training loss: 0.05719028785824776, evaluation loss: 0.07621592283248901\n",
      "2122/3000 training loss: 0.05713386833667755, evaluation loss: 0.07614757865667343\n",
      "2123/3000 training loss: 0.05707752704620361, evaluation loss: 0.076079361140728\n",
      "2124/3000 training loss: 0.05702129378914833, evaluation loss: 0.07601122558116913\n",
      "2125/3000 training loss: 0.05696515366435051, evaluation loss: 0.07594319432973862\n",
      "2126/3000 training loss: 0.05690911412239075, evaluation loss: 0.07587526738643646\n",
      "2127/3000 training loss: 0.056853171437978745, evaluation loss: 0.07580745220184326\n",
      "2128/3000 training loss: 0.056797321885824203, evaluation loss: 0.07573971897363663\n",
      "2129/3000 training loss: 0.056741565465927124, evaluation loss: 0.07567209750413895\n",
      "2130/3000 training loss: 0.056685920804739, evaluation loss: 0.07560458779335022\n",
      "2131/3000 training loss: 0.05663035809993744, evaluation loss: 0.07553716003894806\n",
      "2132/3000 training loss: 0.05657489597797394, evaluation loss: 0.07546984404325485\n",
      "2133/3000 training loss: 0.056519534438848495, evaluation loss: 0.0754026249051094\n",
      "2134/3000 training loss: 0.05646425858139992, evaluation loss: 0.07533550262451172\n",
      "2135/3000 training loss: 0.0564090870320797, evaluation loss: 0.07526849955320358\n",
      "2136/3000 training loss: 0.05635400488972664, evaluation loss: 0.07520157843828201\n",
      "2137/3000 training loss: 0.05629901960492134, evaluation loss: 0.0751347541809082\n",
      "2138/3000 training loss: 0.0562441311776638, evaluation loss: 0.07506805658340454\n",
      "2139/3000 training loss: 0.05618933588266373, evaluation loss: 0.07500144839286804\n",
      "2140/3000 training loss: 0.05613463744521141, evaluation loss: 0.07493492215871811\n",
      "2141/3000 training loss: 0.056080032140016556, evaluation loss: 0.07486849278211594\n",
      "2142/3000 training loss: 0.05602552369236946, evaluation loss: 0.07480218261480331\n",
      "2143/3000 training loss: 0.05597110092639923, evaluation loss: 0.07473595440387726\n",
      "2144/3000 training loss: 0.05591677874326706, evaluation loss: 0.07466983795166016\n",
      "2145/3000 training loss: 0.05586254224181175, evaluation loss: 0.07460381835699081\n",
      "2146/3000 training loss: 0.055808402597904205, evaluation loss: 0.07453789561986923\n",
      "2147/3000 training loss: 0.055754367262125015, evaluation loss: 0.07447207719087601\n",
      "2148/3000 training loss: 0.05570041015744209, evaluation loss: 0.07440635561943054\n",
      "2149/3000 training loss: 0.05564654618501663, evaluation loss: 0.07434071600437164\n",
      "2150/3000 training loss: 0.05559277534484863, evaluation loss: 0.0742751881480217\n",
      "2151/3000 training loss: 0.055539101362228394, evaluation loss: 0.07420974969863892\n",
      "2152/3000 training loss: 0.05548551678657532, evaluation loss: 0.0741444081068039\n",
      "2153/3000 training loss: 0.0554320253431797, evaluation loss: 0.07407917082309723\n",
      "2154/3000 training loss: 0.05537861958146095, evaluation loss: 0.07401402294635773\n",
      "2155/3000 training loss: 0.05532531067728996, evaluation loss: 0.07394897192716599\n",
      "2156/3000 training loss: 0.055272091180086136, evaluation loss: 0.0738840326666832\n",
      "2157/3000 training loss: 0.05521896108984947, evaluation loss: 0.07381917536258698\n",
      "2158/3000 training loss: 0.05516593158245087, evaluation loss: 0.07375441491603851\n",
      "2159/3000 training loss: 0.05511297658085823, evaluation loss: 0.07368975132703781\n",
      "2160/3000 training loss: 0.055060118436813354, evaluation loss: 0.07362518459558487\n",
      "2161/3000 training loss: 0.05500735715031624, evaluation loss: 0.07356071472167969\n",
      "2162/3000 training loss: 0.05495467782020569, evaluation loss: 0.07349633425474167\n",
      "2163/3000 training loss: 0.0549020953476429, evaluation loss: 0.0734320655465126\n",
      "2164/3000 training loss: 0.054849591106176376, evaluation loss: 0.07336787134408951\n",
      "2165/3000 training loss: 0.054797179996967316, evaluation loss: 0.07330378144979477\n",
      "2166/3000 training loss: 0.054744865745306015, evaluation loss: 0.0732397735118866\n",
      "2167/3000 training loss: 0.05469262972474098, evaluation loss: 0.07317588478326797\n",
      "2168/3000 training loss: 0.05464048311114311, evaluation loss: 0.07311207056045532\n",
      "2169/3000 training loss: 0.054588433355093, evaluation loss: 0.07304836064577103\n",
      "2170/3000 training loss: 0.05453645810484886, evaluation loss: 0.0729847326874733\n",
      "2171/3000 training loss: 0.05448458716273308, evaluation loss: 0.07292122393846512\n",
      "2172/3000 training loss: 0.05443279445171356, evaluation loss: 0.07285778224468231\n",
      "2173/3000 training loss: 0.05438109487295151, evaluation loss: 0.07279442995786667\n",
      "2174/3000 training loss: 0.05432947725057602, evaluation loss: 0.07273118942975998\n",
      "2175/3000 training loss: 0.05427795276045799, evaluation loss: 0.07266803085803986\n",
      "2176/3000 training loss: 0.05422651022672653, evaluation loss: 0.07260497659444809\n",
      "2177/3000 training loss: 0.054175153374671936, evaluation loss: 0.07254201173782349\n",
      "2178/3000 training loss: 0.054123882204294205, evaluation loss: 0.07247913628816605\n",
      "2179/3000 training loss: 0.054072700440883636, evaluation loss: 0.07241634279489517\n",
      "2180/3000 training loss: 0.05402160435914993, evaluation loss: 0.07235365360975266\n",
      "2181/3000 training loss: 0.05397059768438339, evaluation loss: 0.0722910463809967\n",
      "2182/3000 training loss: 0.05391967296600342, evaluation loss: 0.07222855091094971\n",
      "2183/3000 training loss: 0.05386883392930031, evaluation loss: 0.07216612994670868\n",
      "2184/3000 training loss: 0.05381808057427406, evaluation loss: 0.07210379838943481\n",
      "2185/3000 training loss: 0.05376741662621498, evaluation loss: 0.07204156368970871\n",
      "2186/3000 training loss: 0.05371682718396187, evaluation loss: 0.07197941839694977\n",
      "2187/3000 training loss: 0.05366633087396622, evaluation loss: 0.07191736251115799\n",
      "2188/3000 training loss: 0.05361591652035713, evaluation loss: 0.07185541093349457\n",
      "2189/3000 training loss: 0.05356559157371521, evaluation loss: 0.07179352641105652\n",
      "2190/3000 training loss: 0.053515348583459854, evaluation loss: 0.07173175364732742\n",
      "2191/3000 training loss: 0.05346519127488136, evaluation loss: 0.0716700553894043\n",
      "2192/3000 training loss: 0.053415101021528244, evaluation loss: 0.07160845398902893\n",
      "2193/3000 training loss: 0.05336511507630348, evaluation loss: 0.07154694199562073\n",
      "2194/3000 training loss: 0.05331520736217499, evaluation loss: 0.07148552685976028\n",
      "2195/3000 training loss: 0.05326537787914276, evaluation loss: 0.07142419368028641\n",
      "2196/3000 training loss: 0.0532156340777874, evaluation loss: 0.0713629424571991\n",
      "2197/3000 training loss: 0.0531659796833992, evaluation loss: 0.07130179554224014\n",
      "2198/3000 training loss: 0.05311640352010727, evaluation loss: 0.07124073058366776\n",
      "2199/3000 training loss: 0.0530669130384922, evaluation loss: 0.07117975503206253\n",
      "2200/3000 training loss: 0.053017497062683105, evaluation loss: 0.07111886143684387\n",
      "2201/3000 training loss: 0.05296816676855087, evaluation loss: 0.07105807214975357\n",
      "2202/3000 training loss: 0.052918918430805206, evaluation loss: 0.07099736481904984\n",
      "2203/3000 training loss: 0.052869752049446106, evaluation loss: 0.07093673944473267\n",
      "2204/3000 training loss: 0.05282066389918327, evaluation loss: 0.07087619602680206\n",
      "2205/3000 training loss: 0.052771665155887604, evaluation loss: 0.07081574946641922\n",
      "2206/3000 training loss: 0.0527227409183979, evaluation loss: 0.07075538486242294\n",
      "2207/3000 training loss: 0.05267389863729477, evaluation loss: 0.07069511711597443\n",
      "2208/3000 training loss: 0.0526251383125782, evaluation loss: 0.07063493132591248\n",
      "2209/3000 training loss: 0.052576467394828796, evaluation loss: 0.07057484239339828\n",
      "2210/3000 training loss: 0.052527863532304764, evaluation loss: 0.07051482796669006\n",
      "2211/3000 training loss: 0.052479349076747894, evaluation loss: 0.070454902946949\n",
      "2212/3000 training loss: 0.05243091285228729, evaluation loss: 0.07039506733417511\n",
      "2213/3000 training loss: 0.05238255485892296, evaluation loss: 0.07033530622720718\n",
      "2214/3000 training loss: 0.05233427509665489, evaluation loss: 0.07027564197778702\n",
      "2215/3000 training loss: 0.05228608101606369, evaluation loss: 0.07021606713533401\n",
      "2216/3000 training loss: 0.05223796144127846, evaluation loss: 0.07015657424926758\n",
      "2217/3000 training loss: 0.05218992382287979, evaluation loss: 0.0700971782207489\n",
      "2218/3000 training loss: 0.052141956984996796, evaluation loss: 0.0700378566980362\n",
      "2219/3000 training loss: 0.05209407955408096, evaluation loss: 0.06997860968112946\n",
      "2220/3000 training loss: 0.0520462803542614, evaluation loss: 0.06991947442293167\n",
      "2221/3000 training loss: 0.0519985556602478, evaluation loss: 0.06986039876937866\n",
      "2222/3000 training loss: 0.051950909197330475, evaluation loss: 0.06980141997337341\n",
      "2223/3000 training loss: 0.05190334469079971, evaluation loss: 0.06974252313375473\n",
      "2224/3000 training loss: 0.05185585096478462, evaluation loss: 0.06968371570110321\n",
      "2225/3000 training loss: 0.0518084354698658, evaluation loss: 0.06962499767541885\n",
      "2226/3000 training loss: 0.05176110938191414, evaluation loss: 0.06956633925437927\n",
      "2227/3000 training loss: 0.05171384662389755, evaluation loss: 0.06950778514146805\n",
      "2228/3000 training loss: 0.05166666954755783, evaluation loss: 0.06944931298494339\n",
      "2229/3000 training loss: 0.05161957070231438, evaluation loss: 0.0693909302353859\n",
      "2230/3000 training loss: 0.051572542637586594, evaluation loss: 0.06933261454105377\n",
      "2231/3000 training loss: 0.051525600254535675, evaluation loss: 0.0692744106054306\n",
      "2232/3000 training loss: 0.05147872865200043, evaluation loss: 0.06921625882387161\n",
      "2233/3000 training loss: 0.05143193528056145, evaluation loss: 0.06915820389986038\n",
      "2234/3000 training loss: 0.051385216414928436, evaluation loss: 0.06910023838281631\n",
      "2235/3000 training loss: 0.05133857578039169, evaluation loss: 0.06904233992099762\n",
      "2236/3000 training loss: 0.05129201337695122, evaluation loss: 0.06898454576730728\n",
      "2237/3000 training loss: 0.051245518028736115, evaluation loss: 0.06892682611942291\n",
      "2238/3000 training loss: 0.05119910463690758, evaluation loss: 0.06886918097734451\n",
      "2239/3000 training loss: 0.05115276947617531, evaluation loss: 0.06881162524223328\n",
      "2240/3000 training loss: 0.05110650137066841, evaluation loss: 0.06875413656234741\n",
      "2241/3000 training loss: 0.05106031894683838, evaluation loss: 0.0686967521905899\n",
      "2242/3000 training loss: 0.05101419612765312, evaluation loss: 0.06863944232463837\n",
      "2243/3000 training loss: 0.05096816271543503, evaluation loss: 0.0685821995139122\n",
      "2244/3000 training loss: 0.05092219263315201, evaluation loss: 0.0685250535607338\n",
      "2245/3000 training loss: 0.05087630823254585, evaluation loss: 0.06846798956394196\n",
      "2246/3000 training loss: 0.05083048716187477, evaluation loss: 0.06841100007295609\n",
      "2247/3000 training loss: 0.050784748047590256, evaluation loss: 0.06835409998893738\n",
      "2248/3000 training loss: 0.05073908343911171, evaluation loss: 0.06829726696014404\n",
      "2249/3000 training loss: 0.050693489611148834, evaluation loss: 0.06824052333831787\n",
      "2250/3000 training loss: 0.05064796656370163, evaluation loss: 0.06818386167287827\n",
      "2251/3000 training loss: 0.050602518022060394, evaluation loss: 0.06812726706266403\n",
      "2252/3000 training loss: 0.050557155162096024, evaluation loss: 0.06807078421115875\n",
      "2253/3000 training loss: 0.05051184818148613, evaluation loss: 0.06801435351371765\n",
      "2254/3000 training loss: 0.050466619431972504, evaluation loss: 0.06795801222324371\n",
      "2255/3000 training loss: 0.050421468913555145, evaluation loss: 0.06790175288915634\n",
      "2256/3000 training loss: 0.05037638545036316, evaluation loss: 0.06784555315971375\n",
      "2257/3000 training loss: 0.050331372767686844, evaluation loss: 0.0677894726395607\n",
      "2258/3000 training loss: 0.050286438316106796, evaluation loss: 0.06773345172405243\n",
      "2259/3000 training loss: 0.05024157464504242, evaluation loss: 0.06767750531435013\n",
      "2260/3000 training loss: 0.050196778029203415, evaluation loss: 0.0676216259598732\n",
      "2261/3000 training loss: 0.05015205591917038, evaluation loss: 0.06756584346294403\n",
      "2262/3000 training loss: 0.050107404589653015, evaluation loss: 0.06751012802124023\n",
      "2263/3000 training loss: 0.05006282776594162, evaluation loss: 0.0674545094370842\n",
      "2264/3000 training loss: 0.0500183179974556, evaluation loss: 0.06739895790815353\n",
      "2265/3000 training loss: 0.04997388646006584, evaluation loss: 0.06734348088502884\n",
      "2266/3000 training loss: 0.04992951825261116, evaluation loss: 0.06728808581829071\n",
      "2267/3000 training loss: 0.04988522082567215, evaluation loss: 0.06723277270793915\n",
      "2268/3000 training loss: 0.04984099790453911, evaluation loss: 0.06717754155397415\n",
      "2269/3000 training loss: 0.04979684576392174, evaluation loss: 0.06712237745523453\n",
      "2270/3000 training loss: 0.04975275695323944, evaluation loss: 0.06706728786230087\n",
      "2271/3000 training loss: 0.04970875009894371, evaluation loss: 0.06701228767633438\n",
      "2272/3000 training loss: 0.049664802849292755, evaluation loss: 0.06695735454559326\n",
      "2273/3000 training loss: 0.04962092638015747, evaluation loss: 0.0669025108218193\n",
      "2274/3000 training loss: 0.049577128142118454, evaluation loss: 0.06684773415327072\n",
      "2275/3000 training loss: 0.04953339695930481, evaluation loss: 0.0667930394411087\n",
      "2276/3000 training loss: 0.04948973283171654, evaluation loss: 0.06673841923475266\n",
      "2277/3000 training loss: 0.04944613575935364, evaluation loss: 0.06668388098478317\n",
      "2278/3000 training loss: 0.049402616918087006, evaluation loss: 0.06662941724061966\n",
      "2279/3000 training loss: 0.04935916140675545, evaluation loss: 0.06657502800226212\n",
      "2280/3000 training loss: 0.04931577295064926, evaluation loss: 0.06652072072029114\n",
      "2281/3000 training loss: 0.04927244782447815, evaluation loss: 0.06646647304296494\n",
      "2282/3000 training loss: 0.049229204654693604, evaluation loss: 0.06641232222318649\n",
      "2283/3000 training loss: 0.04918602108955383, evaluation loss: 0.06635823100805283\n",
      "2284/3000 training loss: 0.049142904579639435, evaluation loss: 0.06630421429872513\n",
      "2285/3000 training loss: 0.049099862575531006, evaluation loss: 0.06625029444694519\n",
      "2286/3000 training loss: 0.04905688762664795, evaluation loss: 0.06619643419981003\n",
      "2287/3000 training loss: 0.049013979732990265, evaluation loss: 0.06614264100790024\n",
      "2288/3000 training loss: 0.04897113889455795, evaluation loss: 0.06608894467353821\n",
      "2289/3000 training loss: 0.048928357660770416, evaluation loss: 0.06603530049324036\n",
      "2290/3000 training loss: 0.04888565093278885, evaluation loss: 0.06598174571990967\n",
      "2291/3000 training loss: 0.048843007534742355, evaluation loss: 0.06592825800180435\n",
      "2292/3000 training loss: 0.04880043491721153, evaluation loss: 0.0658748522400856\n",
      "2293/3000 training loss: 0.04875793308019638, evaluation loss: 0.06582152098417282\n",
      "2294/3000 training loss: 0.0487154945731163, evaluation loss: 0.06576825678348541\n",
      "2295/3000 training loss: 0.0486731231212616, evaluation loss: 0.06571506708860397\n",
      "2296/3000 training loss: 0.048630811274051666, evaluation loss: 0.0656619593501091\n",
      "2297/3000 training loss: 0.048588573932647705, evaluation loss: 0.0656089186668396\n",
      "2298/3000 training loss: 0.04854639247059822, evaluation loss: 0.06555595993995667\n",
      "2299/3000 training loss: 0.048504285514354706, evaluation loss: 0.0655030682682991\n",
      "2300/3000 training loss: 0.048462238162755966, evaluation loss: 0.06545025110244751\n",
      "2301/3000 training loss: 0.0484202615916729, evaluation loss: 0.06539750844240189\n",
      "2302/3000 training loss: 0.0483783558011055, evaluation loss: 0.06534484028816223\n",
      "2303/3000 training loss: 0.04833650961518288, evaluation loss: 0.06529223173856735\n",
      "2304/3000 training loss: 0.04829472675919533, evaluation loss: 0.06523970514535904\n",
      "2305/3000 training loss: 0.04825301468372345, evaluation loss: 0.0651872530579567\n",
      "2306/3000 training loss: 0.04821135476231575, evaluation loss: 0.06513487547636032\n",
      "2307/3000 training loss: 0.04816977679729462, evaluation loss: 0.06508257985115051\n",
      "2308/3000 training loss: 0.04812825843691826, evaluation loss: 0.06503035128116608\n",
      "2309/3000 training loss: 0.04808679595589638, evaluation loss: 0.06497818231582642\n",
      "2310/3000 training loss: 0.04804540053009987, evaluation loss: 0.06492609530687332\n",
      "2311/3000 training loss: 0.04800407588481903, evaluation loss: 0.064874067902565\n",
      "2312/3000 training loss: 0.04796280711889267, evaluation loss: 0.06482212990522385\n",
      "2313/3000 training loss: 0.04792160168290138, evaluation loss: 0.06477025896310806\n",
      "2314/3000 training loss: 0.047880470752716064, evaluation loss: 0.06471844762563705\n",
      "2315/3000 training loss: 0.04783939942717552, evaluation loss: 0.06466671079397202\n",
      "2316/3000 training loss: 0.047798387706279755, evaluation loss: 0.06461506336927414\n",
      "2317/3000 training loss: 0.04775744676589966, evaluation loss: 0.06456346809864044\n",
      "2318/3000 training loss: 0.04771655797958374, evaluation loss: 0.06451193988323212\n",
      "2319/3000 training loss: 0.04767573997378349, evaluation loss: 0.06446050107479095\n",
      "2320/3000 training loss: 0.04763497784733772, evaluation loss: 0.06440913677215576\n",
      "2321/3000 training loss: 0.047594282776117325, evaluation loss: 0.06435782462358475\n",
      "2322/3000 training loss: 0.0475536547601223, evaluation loss: 0.0643065944314003\n",
      "2323/3000 training loss: 0.04751308634877205, evaluation loss: 0.06425543874502182\n",
      "2324/3000 training loss: 0.04747258126735687, evaluation loss: 0.06420434266328812\n",
      "2325/3000 training loss: 0.04743213579058647, evaluation loss: 0.06415332108736038\n",
      "2326/3000 training loss: 0.04739176109433174, evaluation loss: 0.06410236656665802\n",
      "2327/3000 training loss: 0.04735143855214119, evaluation loss: 0.06405147910118103\n",
      "2328/3000 training loss: 0.04731118306517601, evaluation loss: 0.06400066614151001\n",
      "2329/3000 training loss: 0.047270987182855606, evaluation loss: 0.06394992768764496\n",
      "2330/3000 training loss: 0.04723085090517998, evaluation loss: 0.06389924883842468\n",
      "2331/3000 training loss: 0.04719077795743942, evaluation loss: 0.06384865939617157\n",
      "2332/3000 training loss: 0.04715076833963394, evaluation loss: 0.06379810720682144\n",
      "2333/3000 training loss: 0.04711081460118294, evaluation loss: 0.06374764442443848\n",
      "2334/3000 training loss: 0.04707092419266701, evaluation loss: 0.06369724124670029\n",
      "2335/3000 training loss: 0.04703109711408615, evaluation loss: 0.06364692002534866\n",
      "2336/3000 training loss: 0.04699132964015007, evaluation loss: 0.06359665840864182\n",
      "2337/3000 training loss: 0.046951621770858765, evaluation loss: 0.06354646384716034\n",
      "2338/3000 training loss: 0.04691197723150253, evaluation loss: 0.06349634379148483\n",
      "2339/3000 training loss: 0.04687238857150078, evaluation loss: 0.0634462982416153\n",
      "2340/3000 training loss: 0.0468328632414341, evaluation loss: 0.06339630484580994\n",
      "2341/3000 training loss: 0.04679339379072189, evaluation loss: 0.06334639340639114\n",
      "2342/3000 training loss: 0.04675399512052536, evaluation loss: 0.06329654157161713\n",
      "2343/3000 training loss: 0.04671464487910271, evaluation loss: 0.06324675679206848\n",
      "2344/3000 training loss: 0.046675365418195724, evaluation loss: 0.0631970465183258\n",
      "2345/3000 training loss: 0.04663613438606262, evaluation loss: 0.0631474032998085\n",
      "2346/3000 training loss: 0.04659697785973549, evaluation loss: 0.06309782713651657\n",
      "2347/3000 training loss: 0.04655786603689194, evaluation loss: 0.06304831802845001\n",
      "2348/3000 training loss: 0.04651881381869316, evaluation loss: 0.06299887597560883\n",
      "2349/3000 training loss: 0.04647982493042946, evaluation loss: 0.06294950097799301\n",
      "2350/3000 training loss: 0.04644089192152023, evaluation loss: 0.06290018558502197\n",
      "2351/3000 training loss: 0.04640202224254608, evaluation loss: 0.0628509372472763\n",
      "2352/3000 training loss: 0.04636320844292641, evaluation loss: 0.0628017708659172\n",
      "2353/3000 training loss: 0.046324457973241806, evaluation loss: 0.06275266408920288\n",
      "2354/3000 training loss: 0.046285759657621384, evaluation loss: 0.06270362436771393\n",
      "2355/3000 training loss: 0.046247128397226334, evaluation loss: 0.06265465170145035\n",
      "2356/3000 training loss: 0.046208545565605164, evaluation loss: 0.06260573118925095\n",
      "2357/3000 training loss: 0.04617002606391907, evaluation loss: 0.06255690008401871\n",
      "2358/3000 training loss: 0.04613155871629715, evaluation loss: 0.06250812858343124\n",
      "2359/3000 training loss: 0.046093154698610306, evaluation loss: 0.06245940923690796\n",
      "2360/3000 training loss: 0.04605481028556824, evaluation loss: 0.06241076812148094\n",
      "2361/3000 training loss: 0.04601651802659035, evaluation loss: 0.0623621866106987\n",
      "2362/3000 training loss: 0.04597828909754753, evaluation loss: 0.06231366842985153\n",
      "2363/3000 training loss: 0.04594011232256889, evaluation loss: 0.062265221029520035\n",
      "2364/3000 training loss: 0.04590199515223503, evaluation loss: 0.062216851860284805\n",
      "2365/3000 training loss: 0.045863937586545944, evaluation loss: 0.062168534845113754\n",
      "2366/3000 training loss: 0.045825932174921036, evaluation loss: 0.06212027743458748\n",
      "2367/3000 training loss: 0.045787982642650604, evaluation loss: 0.06207209452986717\n",
      "2368/3000 training loss: 0.04575009271502495, evaluation loss: 0.06202397122979164\n",
      "2369/3000 training loss: 0.04571226239204407, evaluation loss: 0.06197591498494148\n",
      "2370/3000 training loss: 0.045674487948417664, evaluation loss: 0.0619279183447361\n",
      "2371/3000 training loss: 0.04563676565885544, evaluation loss: 0.061879999935626984\n",
      "2372/3000 training loss: 0.04559910297393799, evaluation loss: 0.061832133680582047\n",
      "2373/3000 training loss: 0.04556148871779442, evaluation loss: 0.06178433448076248\n",
      "2374/3000 training loss: 0.04552394151687622, evaluation loss: 0.06173660606145859\n",
      "2375/3000 training loss: 0.045486439019441605, evaluation loss: 0.061688922345638275\n",
      "2376/3000 training loss: 0.04544900357723236, evaluation loss: 0.06164131313562393\n",
      "2377/3000 training loss: 0.045411616563797, evaluation loss: 0.06159377843141556\n",
      "2378/3000 training loss: 0.04537428915500641, evaluation loss: 0.06154630333185196\n",
      "2379/3000 training loss: 0.04533701390028, evaluation loss: 0.06149888038635254\n",
      "2380/3000 training loss: 0.045299794524908066, evaluation loss: 0.06145153567194939\n",
      "2381/3000 training loss: 0.04526263475418091, evaluation loss: 0.06140423193573952\n",
      "2382/3000 training loss: 0.04522552341222763, evaluation loss: 0.06135701388120651\n",
      "2383/3000 training loss: 0.04518846794962883, evaluation loss: 0.06130985543131828\n",
      "2384/3000 training loss: 0.045151472091674805, evaluation loss: 0.06126276031136513\n",
      "2385/3000 training loss: 0.04511452838778496, evaluation loss: 0.061215732246637344\n",
      "2386/3000 training loss: 0.04507764056324959, evaluation loss: 0.06116875261068344\n",
      "2387/3000 training loss: 0.0450408048927784, evaluation loss: 0.06112184375524521\n",
      "2388/3000 training loss: 0.04500402510166168, evaluation loss: 0.06107499822974205\n",
      "2389/3000 training loss: 0.044967301189899445, evaluation loss: 0.061028219759464264\n",
      "2390/3000 training loss: 0.04493062570691109, evaluation loss: 0.060981497168540955\n",
      "2391/3000 training loss: 0.044894009828567505, evaluation loss: 0.06093483418226242\n",
      "2392/3000 training loss: 0.0448574423789978, evaluation loss: 0.06088824197649956\n",
      "2393/3000 training loss: 0.04482093080878258, evaluation loss: 0.06084170192480087\n",
      "2394/3000 training loss: 0.04478447884321213, evaluation loss: 0.06079523265361786\n",
      "2395/3000 training loss: 0.04474807158112526, evaluation loss: 0.06074882298707962\n",
      "2396/3000 training loss: 0.044711723923683167, evaluation loss: 0.06070247292518616\n",
      "2397/3000 training loss: 0.044675424695014954, evaluation loss: 0.06065617501735687\n",
      "2398/3000 training loss: 0.04463918134570122, evaluation loss: 0.060609955340623856\n",
      "2399/3000 training loss: 0.04460298642516136, evaluation loss: 0.06056378781795502\n",
      "2400/3000 training loss: 0.04456685855984688, evaluation loss: 0.06051768735051155\n",
      "2401/3000 training loss: 0.04453076794743538, evaluation loss: 0.06047164276242256\n",
      "2402/3000 training loss: 0.044494740664958954, evaluation loss: 0.060425661504268646\n",
      "2403/3000 training loss: 0.04445876181125641, evaluation loss: 0.060379743576049805\n",
      "2404/3000 training loss: 0.044422831386327744, evaluation loss: 0.06033388897776604\n",
      "2405/3000 training loss: 0.044386960566043854, evaluation loss: 0.06028807908296585\n",
      "2406/3000 training loss: 0.044351134449243546, evaluation loss: 0.060242343693971634\n",
      "2407/3000 training loss: 0.04431536793708801, evaluation loss: 0.06019667163491249\n",
      "2408/3000 training loss: 0.04427964985370636, evaluation loss: 0.060151055455207825\n",
      "2409/3000 training loss: 0.044243987649679184, evaluation loss: 0.060105495154857635\n",
      "2410/3000 training loss: 0.04420837014913559, evaluation loss: 0.06005999818444252\n",
      "2411/3000 training loss: 0.04417280852794647, evaluation loss: 0.06001456081867218\n",
      "2412/3000 training loss: 0.04413729906082153, evaluation loss: 0.059969183057546616\n",
      "2413/3000 training loss: 0.044101838022470474, evaluation loss: 0.05992385372519493\n",
      "2414/3000 training loss: 0.04406643286347389, evaluation loss: 0.059878598898649216\n",
      "2415/3000 training loss: 0.04403107240796089, evaluation loss: 0.05983341112732887\n",
      "2416/3000 training loss: 0.04399577155709267, evaluation loss: 0.05978827178478241\n",
      "2417/3000 training loss: 0.043960511684417725, evaluation loss: 0.059743188321590424\n",
      "2418/3000 training loss: 0.04392531141638756, evaluation loss: 0.05969817563891411\n",
      "2419/3000 training loss: 0.04389015957713127, evaluation loss: 0.05965321883559227\n",
      "2420/3000 training loss: 0.043855056166648865, evaluation loss: 0.05960831791162491\n",
      "2421/3000 training loss: 0.043820008635520935, evaluation loss: 0.059563469141721725\n",
      "2422/3000 training loss: 0.043785009533166885, evaluation loss: 0.05951869115233421\n",
      "2423/3000 training loss: 0.043750058859586716, evaluation loss: 0.059473976492881775\n",
      "2424/3000 training loss: 0.04371516406536102, evaluation loss: 0.05942930653691292\n",
      "2425/3000 training loss: 0.04368031397461891, evaluation loss: 0.05938469618558884\n",
      "2426/3000 training loss: 0.04364551603794098, evaluation loss: 0.05934015288949013\n",
      "2427/3000 training loss: 0.043610770255327225, evaluation loss: 0.059295665472745895\n",
      "2428/3000 training loss: 0.04357607290148735, evaluation loss: 0.05925123766064644\n",
      "2429/3000 training loss: 0.04354142025113106, evaluation loss: 0.05920686200261116\n",
      "2430/3000 training loss: 0.04350682348012924, evaluation loss: 0.059162549674510956\n",
      "2431/3000 training loss: 0.043472275137901306, evaluation loss: 0.05911828950047493\n",
      "2432/3000 training loss: 0.04343777522444725, evaluation loss: 0.05907409265637398\n",
      "2433/3000 training loss: 0.04340332746505737, evaluation loss: 0.059029944241046906\n",
      "2434/3000 training loss: 0.04336892440915108, evaluation loss: 0.0589858703315258\n",
      "2435/3000 training loss: 0.04333456978201866, evaluation loss: 0.05894184112548828\n",
      "2436/3000 training loss: 0.043300267308950424, evaluation loss: 0.058897871524095535\n",
      "2437/3000 training loss: 0.04326601326465607, evaluation loss: 0.058853961527347565\n",
      "2438/3000 training loss: 0.04323181137442589, evaluation loss: 0.05881010740995407\n",
      "2439/3000 training loss: 0.04319765418767929, evaluation loss: 0.05876632407307625\n",
      "2440/3000 training loss: 0.04316355288028717, evaluation loss: 0.05872257798910141\n",
      "2441/3000 training loss: 0.04312949627637863, evaluation loss: 0.05867888033390045\n",
      "2442/3000 training loss: 0.043095480650663376, evaluation loss: 0.05863526836037636\n",
      "2443/3000 training loss: 0.043061524629592896, evaluation loss: 0.05859169736504555\n",
      "2444/3000 training loss: 0.0430276095867157, evaluation loss: 0.05854818597435951\n",
      "2445/3000 training loss: 0.04299374297261238, evaluation loss: 0.05850473418831825\n",
      "2446/3000 training loss: 0.04295992851257324, evaluation loss: 0.05846133828163147\n",
      "2447/3000 training loss: 0.042926155030727386, evaluation loss: 0.05841799080371857\n",
      "2448/3000 training loss: 0.04289243742823601, evaluation loss: 0.05837470665574074\n",
      "2449/3000 training loss: 0.04285876452922821, evaluation loss: 0.05833147466182709\n",
      "2450/3000 training loss: 0.042825136333703995, evaluation loss: 0.05828830599784851\n",
      "2451/3000 training loss: 0.04279156029224396, evaluation loss: 0.05824518948793411\n",
      "2452/3000 training loss: 0.0427580326795578, evaluation loss: 0.05820212885737419\n",
      "2453/3000 training loss: 0.042724549770355225, evaluation loss: 0.05815912038087845\n",
      "2454/3000 training loss: 0.04269111156463623, evaluation loss: 0.05811616778373718\n",
      "2455/3000 training loss: 0.042657725512981415, evaluation loss: 0.05807327479124069\n",
      "2456/3000 training loss: 0.04262438789010048, evaluation loss: 0.05803043767809868\n",
      "2457/3000 training loss: 0.042591094970703125, evaluation loss: 0.057987652719020844\n",
      "2458/3000 training loss: 0.04255784675478935, evaluation loss: 0.05794493481516838\n",
      "2459/3000 training loss: 0.04252464696764946, evaluation loss: 0.0579022653400898\n",
      "2460/3000 training loss: 0.042491499334573746, evaluation loss: 0.057859644293785095\n",
      "2461/3000 training loss: 0.042458392679691315, evaluation loss: 0.05781708285212517\n",
      "2462/3000 training loss: 0.042425334453582764, evaluation loss: 0.057774581015110016\n",
      "2463/3000 training loss: 0.042392317205667496, evaluation loss: 0.05773211643099785\n",
      "2464/3000 training loss: 0.04235934466123581, evaluation loss: 0.05768971890211105\n",
      "2465/3000 training loss: 0.0423264242708683, evaluation loss: 0.05764738470315933\n",
      "2466/3000 training loss: 0.04229355603456497, evaluation loss: 0.057605087757110596\n",
      "2467/3000 training loss: 0.04226072505116463, evaluation loss: 0.057562850415706635\n",
      "2468/3000 training loss: 0.04222794249653816, evaluation loss: 0.05752066522836685\n",
      "2469/3000 training loss: 0.04219520092010498, evaluation loss: 0.057478539645671844\n",
      "2470/3000 training loss: 0.042162515223026276, evaluation loss: 0.057436466217041016\n",
      "2471/3000 training loss: 0.042129870504140854, evaluation loss: 0.05739445239305496\n",
      "2472/3000 training loss: 0.042097270488739014, evaluation loss: 0.05735248327255249\n",
      "2473/3000 training loss: 0.04206471145153046, evaluation loss: 0.05731057748198509\n",
      "2474/3000 training loss: 0.04203220456838608, evaluation loss: 0.057268716394901276\n",
      "2475/3000 training loss: 0.04199974611401558, evaluation loss: 0.05722692608833313\n",
      "2476/3000 training loss: 0.041967328637838364, evaluation loss: 0.05718517303466797\n",
      "2477/3000 training loss: 0.04193495586514473, evaluation loss: 0.057143475860357285\n",
      "2478/3000 training loss: 0.04190262779593468, evaluation loss: 0.05710183456540108\n",
      "2479/3000 training loss: 0.041870344430208206, evaluation loss: 0.057060256600379944\n",
      "2480/3000 training loss: 0.04183810204267502, evaluation loss: 0.0570187047123909\n",
      "2481/3000 training loss: 0.04180591180920601, evaluation loss: 0.05697723105549812\n",
      "2482/3000 training loss: 0.04177376627922058, evaluation loss: 0.05693579465150833\n",
      "2483/3000 training loss: 0.041741661727428436, evaluation loss: 0.05689442530274391\n",
      "2484/3000 training loss: 0.04170960187911987, evaluation loss: 0.056853100657463074\n",
      "2485/3000 training loss: 0.04167758673429489, evaluation loss: 0.056811824440956116\n",
      "2486/3000 training loss: 0.04164561629295349, evaluation loss: 0.05677061527967453\n",
      "2487/3000 training loss: 0.04161369055509567, evaluation loss: 0.05672944337129593\n",
      "2488/3000 training loss: 0.041581809520721436, evaluation loss: 0.0566883310675621\n",
      "2489/3000 training loss: 0.04154997318983078, evaluation loss: 0.056647270917892456\n",
      "2490/3000 training loss: 0.041518181562423706, evaluation loss: 0.056606266647577286\n",
      "2491/3000 training loss: 0.041486434638500214, evaluation loss: 0.0565653070807457\n",
      "2492/3000 training loss: 0.041454724967479706, evaluation loss: 0.056524403393268585\n",
      "2493/3000 training loss: 0.04142305999994278, evaluation loss: 0.05648355185985565\n",
      "2494/3000 training loss: 0.04139144718647003, evaluation loss: 0.0564427487552166\n",
      "2495/3000 training loss: 0.04135987162590027, evaluation loss: 0.05640200152993202\n",
      "2496/3000 training loss: 0.04132833704352379, evaluation loss: 0.056361302733421326\n",
      "2497/3000 training loss: 0.04129684716463089, evaluation loss: 0.056320663541555405\n",
      "2498/3000 training loss: 0.04126540198922157, evaluation loss: 0.05628005787730217\n",
      "2499/3000 training loss: 0.04123400151729584, evaluation loss: 0.056239522993564606\n",
      "2500/3000 training loss: 0.041202645748853683, evaluation loss: 0.056199025362730026\n",
      "2501/3000 training loss: 0.041171323508024216, evaluation loss: 0.05615858733654022\n",
      "2502/3000 training loss: 0.041140053421258926, evaluation loss: 0.0561182014644146\n",
      "2503/3000 training loss: 0.04110882431268692, evaluation loss: 0.05607786774635315\n",
      "2504/3000 training loss: 0.041077639907598495, evaluation loss: 0.056037575006484985\n",
      "2505/3000 training loss: 0.041046496480703354, evaluation loss: 0.055997345596551895\n",
      "2506/3000 training loss: 0.0410153903067112, evaluation loss: 0.05595715716481209\n",
      "2507/3000 training loss: 0.04098433628678322, evaluation loss: 0.05591702088713646\n",
      "2508/3000 training loss: 0.040953315794467926, evaluation loss: 0.055876944214105606\n",
      "2509/3000 training loss: 0.040922340005636215, evaluation loss: 0.05583690106868744\n",
      "2510/3000 training loss: 0.040891408920288086, evaluation loss: 0.05579692870378494\n",
      "2511/3000 training loss: 0.04086051508784294, evaluation loss: 0.055756986141204834\n",
      "2512/3000 training loss: 0.040829673409461975, evaluation loss: 0.0557171069085598\n",
      "2513/3000 training loss: 0.040798865258693695, evaluation loss: 0.055677272379398346\n",
      "2514/3000 training loss: 0.0407680943608284, evaluation loss: 0.05563749000430107\n",
      "2515/3000 training loss: 0.04073737561702728, evaluation loss: 0.055597756057977676\n",
      "2516/3000 training loss: 0.04070669785141945, evaluation loss: 0.05555807799100876\n",
      "2517/3000 training loss: 0.0406760536134243, evaluation loss: 0.055518440902233124\n",
      "2518/3000 training loss: 0.04064546152949333, evaluation loss: 0.055478863418102264\n",
      "2519/3000 training loss: 0.04061489924788475, evaluation loss: 0.05543932691216469\n",
      "2520/3000 training loss: 0.04058438912034035, evaluation loss: 0.05539984256029129\n",
      "2521/3000 training loss: 0.04055391252040863, evaluation loss: 0.05536041036248207\n",
      "2522/3000 training loss: 0.04052348434925079, evaluation loss: 0.05532102659344673\n",
      "2523/3000 training loss: 0.04049309715628624, evaluation loss: 0.055281687527894974\n",
      "2524/3000 training loss: 0.040462739765644073, evaluation loss: 0.05524241179227829\n",
      "2525/3000 training loss: 0.040432434529066086, evaluation loss: 0.055203165858983994\n",
      "2526/3000 training loss: 0.04040216654539108, evaluation loss: 0.055163975805044174\n",
      "2527/3000 training loss: 0.040371935814619064, evaluation loss: 0.05512483790516853\n",
      "2528/3000 training loss: 0.040341753512620926, evaluation loss: 0.05508573725819588\n",
      "2529/3000 training loss: 0.040311604738235474, evaluation loss: 0.05504670739173889\n",
      "2530/3000 training loss: 0.0402815043926239, evaluation loss: 0.055007707327604294\n",
      "2531/3000 training loss: 0.04025143012404442, evaluation loss: 0.05496876686811447\n",
      "2532/3000 training loss: 0.04022141173481941, evaluation loss: 0.05492986738681793\n",
      "2533/3000 training loss: 0.04019142687320709, evaluation loss: 0.05489102005958557\n",
      "2534/3000 training loss: 0.04016147926449776, evaluation loss: 0.054852209985256195\n",
      "2535/3000 training loss: 0.0401315800845623, evaluation loss: 0.05481346696615219\n",
      "2536/3000 training loss: 0.04010171443223953, evaluation loss: 0.05477476119995117\n",
      "2537/3000 training loss: 0.040071889758110046, evaluation loss: 0.05473611503839493\n",
      "2538/3000 training loss: 0.04004210978746414, evaluation loss: 0.05469750240445137\n",
      "2539/3000 training loss: 0.04001236706972122, evaluation loss: 0.05465893819928169\n",
      "2540/3000 training loss: 0.03998265787959099, evaluation loss: 0.05462042987346649\n",
      "2541/3000 training loss: 0.039952997118234634, evaluation loss: 0.05458196625113487\n",
      "2542/3000 training loss: 0.03992336988449097, evaluation loss: 0.05454355105757713\n",
      "2543/3000 training loss: 0.03989379107952118, evaluation loss: 0.05450518801808357\n",
      "2544/3000 training loss: 0.03986424207687378, evaluation loss: 0.054466865956783295\n",
      "2545/3000 training loss: 0.03983473405241966, evaluation loss: 0.054428581148386\n",
      "2546/3000 training loss: 0.03980526700615883, evaluation loss: 0.054390355944633484\n",
      "2547/3000 training loss: 0.03977583721280098, evaluation loss: 0.05435217171907425\n",
      "2548/3000 training loss: 0.039746448397636414, evaluation loss: 0.05431404337286949\n",
      "2549/3000 training loss: 0.03971710056066513, evaluation loss: 0.05427595227956772\n",
      "2550/3000 training loss: 0.03968778997659683, evaluation loss: 0.05423791706562042\n",
      "2551/3000 training loss: 0.03965852037072182, evaluation loss: 0.05419992655515671\n",
      "2552/3000 training loss: 0.03962928429245949, evaluation loss: 0.05416197329759598\n",
      "2553/3000 training loss: 0.03960008919239044, evaluation loss: 0.05412407964468002\n",
      "2554/3000 training loss: 0.03957093507051468, evaluation loss: 0.05408623069524765\n",
      "2555/3000 training loss: 0.0395418144762516, evaluation loss: 0.05404841899871826\n",
      "2556/3000 training loss: 0.039512742310762405, evaluation loss: 0.05401064455509186\n",
      "2557/3000 training loss: 0.0394836962223053, evaluation loss: 0.053972940891981125\n",
      "2558/3000 training loss: 0.03945469483733177, evaluation loss: 0.053935278207063675\n",
      "2559/3000 training loss: 0.03942573070526123, evaluation loss: 0.05389765277504921\n",
      "2560/3000 training loss: 0.03939680755138397, evaluation loss: 0.053860075771808624\n",
      "2561/3000 training loss: 0.0393679141998291, evaluation loss: 0.05382254347205162\n",
      "2562/3000 training loss: 0.03933907300233841, evaluation loss: 0.0537850558757782\n",
      "2563/3000 training loss: 0.03931025415658951, evaluation loss: 0.05374762415885925\n",
      "2564/3000 training loss: 0.03928148001432419, evaluation loss: 0.05371022969484329\n",
      "2565/3000 training loss: 0.03925274312496185, evaluation loss: 0.05367287993431091\n",
      "2566/3000 training loss: 0.0392240546643734, evaluation loss: 0.05363558977842331\n",
      "2567/3000 training loss: 0.03919538855552673, evaluation loss: 0.05359833315014839\n",
      "2568/3000 training loss: 0.03916676342487335, evaluation loss: 0.05356111750006676\n",
      "2569/3000 training loss: 0.039138175547122955, evaluation loss: 0.0535239540040493\n",
      "2570/3000 training loss: 0.03910963237285614, evaluation loss: 0.05348683521151543\n",
      "2571/3000 training loss: 0.03908112645149231, evaluation loss: 0.053449761122465134\n",
      "2572/3000 training loss: 0.039052657783031464, evaluation loss: 0.05341273546218872\n",
      "2573/3000 training loss: 0.039024218916893005, evaluation loss: 0.05337574705481529\n",
      "2574/3000 training loss: 0.03899581730365753, evaluation loss: 0.05333881452679634\n",
      "2575/3000 training loss: 0.03896745666861534, evaluation loss: 0.053301915526390076\n",
      "2576/3000 training loss: 0.038939133286476135, evaluation loss: 0.05326507240533829\n",
      "2577/3000 training loss: 0.038910843431949615, evaluation loss: 0.05322827026247978\n",
      "2578/3000 training loss: 0.03888259455561638, evaluation loss: 0.05319151282310486\n",
      "2579/3000 training loss: 0.038854386657476425, evaluation loss: 0.053154800087213516\n",
      "2580/3000 training loss: 0.03882620483636856, evaluation loss: 0.05311812087893486\n",
      "2581/3000 training loss: 0.038798071444034576, evaluation loss: 0.05308149382472038\n",
      "2582/3000 training loss: 0.03876996412873268, evaluation loss: 0.053044915199279785\n",
      "2583/3000 training loss: 0.038741905242204666, evaluation loss: 0.05300837755203247\n",
      "2584/3000 training loss: 0.03871387243270874, evaluation loss: 0.05297188088297844\n",
      "2585/3000 training loss: 0.0386858768761158, evaluation loss: 0.05293543264269829\n",
      "2586/3000 training loss: 0.038657914847135544, evaluation loss: 0.05289902910590172\n",
      "2587/3000 training loss: 0.03862999007105827, evaluation loss: 0.05286266654729843\n",
      "2588/3000 training loss: 0.038602109998464584, evaluation loss: 0.05282634124159813\n",
      "2589/3000 training loss: 0.038574256002902985, evaluation loss: 0.05279006436467171\n",
      "2590/3000 training loss: 0.03854644298553467, evaluation loss: 0.05275383219122887\n",
      "2591/3000 training loss: 0.03851866349577904, evaluation loss: 0.05271764472126961\n",
      "2592/3000 training loss: 0.03849092498421669, evaluation loss: 0.05268149450421333\n",
      "2593/3000 training loss: 0.03846322000026703, evaluation loss: 0.05264539644122124\n",
      "2594/3000 training loss: 0.038435548543930054, evaluation loss: 0.05260934308171272\n",
      "2595/3000 training loss: 0.03840791434049606, evaluation loss: 0.052573319524526596\n",
      "2596/3000 training loss: 0.03838031738996506, evaluation loss: 0.052537351846694946\n",
      "2597/3000 training loss: 0.03835275396704674, evaluation loss: 0.05250141769647598\n",
      "2598/3000 training loss: 0.0383252277970314, evaluation loss: 0.052465539425611496\n",
      "2599/3000 training loss: 0.03829773887991905, evaluation loss: 0.0524296872317791\n",
      "2600/3000 training loss: 0.03827028349041939, evaluation loss: 0.05239388719201088\n",
      "2601/3000 training loss: 0.03824286162853241, evaluation loss: 0.05235813558101654\n",
      "2602/3000 training loss: 0.03821547329425812, evaluation loss: 0.052322421222925186\n",
      "2603/3000 training loss: 0.03818811848759651, evaluation loss: 0.052286744117736816\n",
      "2604/3000 training loss: 0.03816080093383789, evaluation loss: 0.052251119166612625\n",
      "2605/3000 training loss: 0.038133516907691956, evaluation loss: 0.05221553519368172\n",
      "2606/3000 training loss: 0.038106273859739304, evaluation loss: 0.052179984748363495\n",
      "2607/3000 training loss: 0.03807906433939934, evaluation loss: 0.052144479006528854\n",
      "2608/3000 training loss: 0.03805188462138176, evaluation loss: 0.052109017968177795\n",
      "2609/3000 training loss: 0.038024742156267166, evaluation loss: 0.052073605358600616\n",
      "2610/3000 training loss: 0.03799763694405556, evaluation loss: 0.052038222551345825\n",
      "2611/3000 training loss: 0.037970561534166336, evaluation loss: 0.052002884447574615\n",
      "2612/3000 training loss: 0.0379435271024704, evaluation loss: 0.051967594772577286\n",
      "2613/3000 training loss: 0.03791651874780655, evaluation loss: 0.05193233862519264\n",
      "2614/3000 training loss: 0.037889547646045685, evaluation loss: 0.051897138357162476\n",
      "2615/3000 training loss: 0.037862613797187805, evaluation loss: 0.0518619678914547\n",
      "2616/3000 training loss: 0.03783571347594261, evaluation loss: 0.051826849579811096\n",
      "2617/3000 training loss: 0.0378088504076004, evaluation loss: 0.05179176107048988\n",
      "2618/3000 training loss: 0.03778201341629028, evaluation loss: 0.05175672098994255\n",
      "2619/3000 training loss: 0.03775521367788315, evaluation loss: 0.051721710711717606\n",
      "2620/3000 training loss: 0.037728451192379, evaluation loss: 0.05168675258755684\n",
      "2621/3000 training loss: 0.037701722234487534, evaluation loss: 0.05165183171629906\n",
      "2622/3000 training loss: 0.03767502307891846, evaluation loss: 0.05161695554852486\n",
      "2623/3000 training loss: 0.03764835745096207, evaluation loss: 0.05158211663365364\n",
      "2624/3000 training loss: 0.03762173280119896, evaluation loss: 0.051547326147556305\n",
      "2625/3000 training loss: 0.03759513050317764, evaluation loss: 0.051512569189071655\n",
      "2626/3000 training loss: 0.037568576633930206, evaluation loss: 0.05147784948348999\n",
      "2627/3000 training loss: 0.03754204884171486, evaluation loss: 0.05144317448139191\n",
      "2628/3000 training loss: 0.0375155545771122, evaluation loss: 0.051408544182777405\n",
      "2629/3000 training loss: 0.037489090114831924, evaluation loss: 0.05137394368648529\n",
      "2630/3000 training loss: 0.03746265545487404, evaluation loss: 0.05133939906954765\n",
      "2631/3000 training loss: 0.037436261773109436, evaluation loss: 0.051304880529642105\n",
      "2632/3000 training loss: 0.03740990161895752, evaluation loss: 0.051270417869091034\n",
      "2633/3000 training loss: 0.03738357126712799, evaluation loss: 0.051235977560281754\n",
      "2634/3000 training loss: 0.03735727816820145, evaluation loss: 0.05120159313082695\n",
      "2635/3000 training loss: 0.03733101114630699, evaluation loss: 0.05116723105311394\n",
      "2636/3000 training loss: 0.03730478882789612, evaluation loss: 0.051132939755916595\n",
      "2637/3000 training loss: 0.03727858513593674, evaluation loss: 0.05109866335988045\n",
      "2638/3000 training loss: 0.03725242242217064, evaluation loss: 0.051064424216747284\n",
      "2639/3000 training loss: 0.03722628951072693, evaluation loss: 0.0510302297770977\n",
      "2640/3000 training loss: 0.037200190126895905, evaluation loss: 0.0509960800409317\n",
      "2641/3000 training loss: 0.037174124270677567, evaluation loss: 0.05096196383237839\n",
      "2642/3000 training loss: 0.03714809566736221, evaluation loss: 0.05092789977788925\n",
      "2643/3000 training loss: 0.03712208569049835, evaluation loss: 0.05089385807514191\n",
      "2644/3000 training loss: 0.03709612414240837, evaluation loss: 0.05085986480116844\n",
      "2645/3000 training loss: 0.03707018494606018, evaluation loss: 0.05082591250538826\n",
      "2646/3000 training loss: 0.037044283002614975, evaluation loss: 0.050791993737220764\n",
      "2647/3000 training loss: 0.03701840713620186, evaluation loss: 0.05075812339782715\n",
      "2648/3000 training loss: 0.036992572247982025, evaluation loss: 0.05072428658604622\n",
      "2649/3000 training loss: 0.03696676343679428, evaluation loss: 0.05069049075245857\n",
      "2650/3000 training loss: 0.03694098815321922, evaluation loss: 0.050656724721193314\n",
      "2651/3000 training loss: 0.03691524267196655, evaluation loss: 0.050623007118701935\n",
      "2652/3000 training loss: 0.03688953444361687, evaluation loss: 0.05058932676911354\n",
      "2653/3000 training loss: 0.03686385601758957, evaluation loss: 0.05055569112300873\n",
      "2654/3000 training loss: 0.03683820366859436, evaluation loss: 0.050522081553936005\n",
      "2655/3000 training loss: 0.036812588572502136, evaluation loss: 0.05048851668834686\n",
      "2656/3000 training loss: 0.0367870032787323, evaluation loss: 0.050454989075660706\n",
      "2657/3000 training loss: 0.03676145523786545, evaluation loss: 0.05042150616645813\n",
      "2658/3000 training loss: 0.036735933274030685, evaluation loss: 0.05038805305957794\n",
      "2659/3000 training loss: 0.03671044483780861, evaluation loss: 0.05035464093089104\n",
      "2660/3000 training loss: 0.03668498620390892, evaluation loss: 0.05032126605510712\n",
      "2661/3000 training loss: 0.03665955737233162, evaluation loss: 0.050287920981645584\n",
      "2662/3000 training loss: 0.036634162068367004, evaluation loss: 0.050254639238119125\n",
      "2663/3000 training loss: 0.036608800292015076, evaluation loss: 0.050221383571624756\n",
      "2664/3000 training loss: 0.036583464592695236, evaluation loss: 0.05018816143274307\n",
      "2665/3000 training loss: 0.03655816614627838, evaluation loss: 0.05015498027205467\n",
      "2666/3000 training loss: 0.036532893776893616, evaluation loss: 0.05012183636426926\n",
      "2667/3000 training loss: 0.036507654935121536, evaluation loss: 0.050088729709386826\n",
      "2668/3000 training loss: 0.036482445895671844, evaluation loss: 0.05005565285682678\n",
      "2669/3000 training loss: 0.03645726665854454, evaluation loss: 0.05002261698246002\n",
      "2670/3000 training loss: 0.03643212467432022, evaluation loss: 0.04998963326215744\n",
      "2671/3000 training loss: 0.03640700876712799, evaluation loss: 0.04995666816830635\n",
      "2672/3000 training loss: 0.03638192266225815, evaluation loss: 0.04992375895380974\n",
      "2673/3000 training loss: 0.03635686635971069, evaluation loss: 0.049890875816345215\n",
      "2674/3000 training loss: 0.036331843584775925, evaluation loss: 0.049858029931783676\n",
      "2675/3000 training loss: 0.036306846886873245, evaluation loss: 0.04982522875070572\n",
      "2676/3000 training loss: 0.03628188371658325, evaluation loss: 0.04979244992136955\n",
      "2677/3000 training loss: 0.036256950348615646, evaluation loss: 0.049759723246097565\n",
      "2678/3000 training loss: 0.036232054233551025, evaluation loss: 0.04972701892256737\n",
      "2679/3000 training loss: 0.0362071767449379, evaluation loss: 0.04969436302781105\n",
      "2680/3000 training loss: 0.03618234023451805, evaluation loss: 0.04966173693537712\n",
      "2681/3000 training loss: 0.036157529801130295, evaluation loss: 0.04962916299700737\n",
      "2682/3000 training loss: 0.036132749170064926, evaluation loss: 0.049596600234508514\n",
      "2683/3000 training loss: 0.036107998341321945, evaluation loss: 0.049564097076654434\n",
      "2684/3000 training loss: 0.03608327358961105, evaluation loss: 0.049531616270542145\n",
      "2685/3000 training loss: 0.036058586090803146, evaluation loss: 0.049499183893203735\n",
      "2686/3000 training loss: 0.03603392466902733, evaluation loss: 0.049466777592897415\n",
      "2687/3000 training loss: 0.0360092967748642, evaluation loss: 0.04943441227078438\n",
      "2688/3000 training loss: 0.035984694957733154, evaluation loss: 0.04940208047628403\n",
      "2689/3000 training loss: 0.0359601266682148, evaluation loss: 0.04936979338526726\n",
      "2690/3000 training loss: 0.03593558445572853, evaluation loss: 0.04933753237128258\n",
      "2691/3000 training loss: 0.03591107204556465, evaluation loss: 0.04930530861020088\n",
      "2692/3000 training loss: 0.03588658943772316, evaluation loss: 0.04927312582731247\n",
      "2693/3000 training loss: 0.035862140357494354, evaluation loss: 0.04924097657203674\n",
      "2694/3000 training loss: 0.03583771735429764, evaluation loss: 0.049208864569664\n",
      "2695/3000 training loss: 0.03581332787871361, evaluation loss: 0.04917678236961365\n",
      "2696/3000 training loss: 0.03578895330429077, evaluation loss: 0.04914475232362747\n",
      "2697/3000 training loss: 0.03576461970806122, evaluation loss: 0.04911273345351219\n",
      "2698/3000 training loss: 0.03574031963944435, evaluation loss: 0.04908076301217079\n",
      "2699/3000 training loss: 0.035716041922569275, evaluation loss: 0.049048829823732376\n",
      "2700/3000 training loss: 0.035691794008016586, evaluation loss: 0.049016933888196945\n",
      "2701/3000 training loss: 0.03566757217049599, evaluation loss: 0.0489850714802742\n",
      "2702/3000 training loss: 0.035643383860588074, evaluation loss: 0.04895324632525444\n",
      "2703/3000 training loss: 0.035619232803583145, evaluation loss: 0.04892145097255707\n",
      "2704/3000 training loss: 0.03559509664773941, evaluation loss: 0.04888969659805298\n",
      "2705/3000 training loss: 0.03557099401950836, evaluation loss: 0.04885797202587128\n",
      "2706/3000 training loss: 0.03554692491889, evaluation loss: 0.04882628470659256\n",
      "2707/3000 training loss: 0.035522881895303726, evaluation loss: 0.04879463464021683\n",
      "2708/3000 training loss: 0.035498861223459244, evaluation loss: 0.04876301437616348\n",
      "2709/3000 training loss: 0.035474877804517746, evaluation loss: 0.04873143881559372\n",
      "2710/3000 training loss: 0.03545091301202774, evaluation loss: 0.04869988560676575\n",
      "2711/3000 training loss: 0.03542698547244072, evaluation loss: 0.04866837337613106\n",
      "2712/3000 training loss: 0.035403087735176086, evaluation loss: 0.048636890947818756\n",
      "2713/3000 training loss: 0.035379212349653244, evaluation loss: 0.048605456948280334\n",
      "2714/3000 training loss: 0.03535537049174309, evaluation loss: 0.048574041575193405\n",
      "2715/3000 training loss: 0.03533155843615532, evaluation loss: 0.04854265972971916\n",
      "2716/3000 training loss: 0.03530776500701904, evaluation loss: 0.0485113300383091\n",
      "2717/3000 training loss: 0.03528400883078575, evaluation loss: 0.048480018973350525\n",
      "2718/3000 training loss: 0.03526028245687485, evaluation loss: 0.04844875633716583\n",
      "2719/3000 training loss: 0.035236574709415436, evaluation loss: 0.04841751232743263\n",
      "2720/3000 training loss: 0.03521290048956871, evaluation loss: 0.048386309295892715\n",
      "2721/3000 training loss: 0.035189252346754074, evaluation loss: 0.048355139791965485\n",
      "2722/3000 training loss: 0.03516563028097153, evaluation loss: 0.04832400754094124\n",
      "2723/3000 training loss: 0.035142045468091965, evaluation loss: 0.04829290881752968\n",
      "2724/3000 training loss: 0.035118479281663895, evaluation loss: 0.0482618473470211\n",
      "2725/3000 training loss: 0.03509495034813881, evaluation loss: 0.04823080822825432\n",
      "2726/3000 training loss: 0.03507143631577492, evaluation loss: 0.04819981008768082\n",
      "2727/3000 training loss: 0.03504796326160431, evaluation loss: 0.048168838024139404\n",
      "2728/3000 training loss: 0.03502450883388519, evaluation loss: 0.04813790321350098\n",
      "2729/3000 training loss: 0.03500108793377876, evaluation loss: 0.04810701310634613\n",
      "2730/3000 training loss: 0.034977689385414124, evaluation loss: 0.04807615280151367\n",
      "2731/3000 training loss: 0.034954316914081573, evaluation loss: 0.0480453297495842\n",
      "2732/3000 training loss: 0.03493097424507141, evaluation loss: 0.048014525324106216\n",
      "2733/3000 training loss: 0.034907665103673935, evaluation loss: 0.04798375442624092\n",
      "2734/3000 training loss: 0.03488437831401825, evaluation loss: 0.047953035682439804\n",
      "2735/3000 training loss: 0.03486112505197525, evaluation loss: 0.04792233929038048\n",
      "2736/3000 training loss: 0.03483789414167404, evaluation loss: 0.04789167270064354\n",
      "2737/3000 training loss: 0.034814685583114624, evaluation loss: 0.047861047089099884\n",
      "2738/3000 training loss: 0.03479151055216789, evaluation loss: 0.04783044755458832\n",
      "2739/3000 training loss: 0.03476835787296295, evaluation loss: 0.04779987782239914\n",
      "2740/3000 training loss: 0.0347452349960804, evaluation loss: 0.047769345343112946\n",
      "2741/3000 training loss: 0.034722134470939636, evaluation loss: 0.047738850116729736\n",
      "2742/3000 training loss: 0.03469906747341156, evaluation loss: 0.04770838841795921\n",
      "2743/3000 training loss: 0.03467603400349617, evaluation loss: 0.04767795279622078\n",
      "2744/3000 training loss: 0.034653011709451675, evaluation loss: 0.04764754697680473\n",
      "2745/3000 training loss: 0.034630026668310165, evaluation loss: 0.04761718586087227\n",
      "2746/3000 training loss: 0.034607063978910446, evaluation loss: 0.047586843371391296\n",
      "2747/3000 training loss: 0.034584131091833115, evaluation loss: 0.047556545585393906\n",
      "2748/3000 training loss: 0.03456122428178787, evaluation loss: 0.0475262813270092\n",
      "2749/3000 training loss: 0.03453833982348442, evaluation loss: 0.04749603942036629\n",
      "2750/3000 training loss: 0.034515492618083954, evaluation loss: 0.04746583476662636\n",
      "2751/3000 training loss: 0.034492652863264084, evaluation loss: 0.04743565618991852\n",
      "2752/3000 training loss: 0.034469861537218094, evaluation loss: 0.04740552231669426\n",
      "2753/3000 training loss: 0.034447081387043, evaluation loss: 0.04737541079521179\n",
      "2754/3000 training loss: 0.03442433103919029, evaluation loss: 0.047345325350761414\n",
      "2755/3000 training loss: 0.034401606768369675, evaluation loss: 0.047315292060375214\n",
      "2756/3000 training loss: 0.03437891602516174, evaluation loss: 0.047285281121730804\n",
      "2757/3000 training loss: 0.0343562476336956, evaluation loss: 0.047255292534828186\n",
      "2758/3000 training loss: 0.03433360531926155, evaluation loss: 0.04722534120082855\n",
      "2759/3000 training loss: 0.03431098535656929, evaluation loss: 0.0471954271197319\n",
      "2760/3000 training loss: 0.03428838774561882, evaluation loss: 0.04716555029153824\n",
      "2761/3000 training loss: 0.034265827387571335, evaluation loss: 0.04713568836450577\n",
      "2762/3000 training loss: 0.03424328938126564, evaluation loss: 0.04710586741566658\n",
      "2763/3000 training loss: 0.034220777451992035, evaluation loss: 0.04707607626914978\n",
      "2764/3000 training loss: 0.03419828787446022, evaluation loss: 0.047046322375535965\n",
      "2765/3000 training loss: 0.03417582809925079, evaluation loss: 0.04701659455895424\n",
      "2766/3000 training loss: 0.03415338695049286, evaluation loss: 0.0469869002699852\n",
      "2767/3000 training loss: 0.03413098305463791, evaluation loss: 0.04695722833275795\n",
      "2768/3000 training loss: 0.03410859778523445, evaluation loss: 0.04692760109901428\n",
      "2769/3000 training loss: 0.03408623859286308, evaluation loss: 0.04689798876643181\n",
      "2770/3000 training loss: 0.034063905477523804, evaluation loss: 0.04686842858791351\n",
      "2771/3000 training loss: 0.034041594713926315, evaluation loss: 0.04683888703584671\n",
      "2772/3000 training loss: 0.034019313752651215, evaluation loss: 0.04680937901139259\n",
      "2773/3000 training loss: 0.0339970588684082, evaluation loss: 0.046779900789260864\n",
      "2774/3000 training loss: 0.03397483006119728, evaluation loss: 0.04675045236945152\n",
      "2775/3000 training loss: 0.03395262360572815, evaluation loss: 0.04672103375196457\n",
      "2776/3000 training loss: 0.03393044322729111, evaluation loss: 0.0466916523873806\n",
      "2777/3000 training loss: 0.033908288925886154, evaluation loss: 0.04666229709982872\n",
      "2778/3000 training loss: 0.03388616070151329, evaluation loss: 0.046632975339889526\n",
      "2779/3000 training loss: 0.033864058554172516, evaluation loss: 0.04660368710756302\n",
      "2780/3000 training loss: 0.03384197875857353, evaluation loss: 0.046574413776397705\n",
      "2781/3000 training loss: 0.03381992504000664, evaluation loss: 0.04654517397284508\n",
      "2782/3000 training loss: 0.03379789739847183, evaluation loss: 0.04651597514748573\n",
      "2783/3000 training loss: 0.033775895833969116, evaluation loss: 0.046486806124448776\n",
      "2784/3000 training loss: 0.03375391662120819, evaluation loss: 0.04645766317844391\n",
      "2785/3000 training loss: 0.033731963485479355, evaluation loss: 0.04642855376005173\n",
      "2786/3000 training loss: 0.03371003270149231, evaluation loss: 0.04639947786927223\n",
      "2787/3000 training loss: 0.03368813171982765, evaluation loss: 0.046370431780815125\n",
      "2788/3000 training loss: 0.033666256815195084, evaluation loss: 0.046341411769390106\n",
      "2789/3000 training loss: 0.03364439308643341, evaluation loss: 0.04631241783499718\n",
      "2790/3000 training loss: 0.03362257033586502, evaluation loss: 0.04628346115350723\n",
      "2791/3000 training loss: 0.033600762486457825, evaluation loss: 0.04625454172492027\n",
      "2792/3000 training loss: 0.033578984439373016, evaluation loss: 0.04622562974691391\n",
      "2793/3000 training loss: 0.03355722874403, evaluation loss: 0.046196769922971725\n",
      "2794/3000 training loss: 0.03353549912571907, evaluation loss: 0.046167925000190735\n",
      "2795/3000 training loss: 0.03351379558444023, evaluation loss: 0.04613911360502243\n",
      "2796/3000 training loss: 0.033492110669612885, evaluation loss: 0.046110332012176514\n",
      "2797/3000 training loss: 0.033470455557107925, evaluation loss: 0.046081580221652985\n",
      "2798/3000 training loss: 0.03344882279634476, evaluation loss: 0.046052854508161545\n",
      "2799/3000 training loss: 0.03342720866203308, evaluation loss: 0.04602416977286339\n",
      "2800/3000 training loss: 0.03340562805533409, evaluation loss: 0.045995499938726425\n",
      "2801/3000 training loss: 0.03338407352566719, evaluation loss: 0.045966874808073044\n",
      "2802/3000 training loss: 0.033362530171871185, evaluation loss: 0.04593827202916145\n",
      "2803/3000 training loss: 0.03334101662039757, evaluation loss: 0.04590969160199165\n",
      "2804/3000 training loss: 0.03331953287124634, evaluation loss: 0.04588114842772484\n",
      "2805/3000 training loss: 0.0332980677485466, evaluation loss: 0.04585263133049011\n",
      "2806/3000 training loss: 0.033276624977588654, evaluation loss: 0.04582415148615837\n",
      "2807/3000 training loss: 0.033255208283662796, evaluation loss: 0.045795682817697525\n",
      "2808/3000 training loss: 0.033233825117349625, evaluation loss: 0.04576725885272026\n",
      "2809/3000 training loss: 0.03321245312690735, evaluation loss: 0.04573885723948479\n",
      "2810/3000 training loss: 0.03319111093878746, evaluation loss: 0.045710489153862\n",
      "2811/3000 training loss: 0.033169787377119064, evaluation loss: 0.0456821545958519\n",
      "2812/3000 training loss: 0.033148497343063354, evaluation loss: 0.04565384238958359\n",
      "2813/3000 training loss: 0.03312721848487854, evaluation loss: 0.04562555253505707\n",
      "2814/3000 training loss: 0.03310597315430641, evaluation loss: 0.04559730365872383\n",
      "2815/3000 training loss: 0.033084746450185776, evaluation loss: 0.04556906968355179\n",
      "2816/3000 training loss: 0.03306354582309723, evaluation loss: 0.04554087296128273\n",
      "2817/3000 training loss: 0.033042363822460175, evaluation loss: 0.04551270604133606\n",
      "2818/3000 training loss: 0.03302120789885521, evaluation loss: 0.04548456519842148\n",
      "2819/3000 training loss: 0.03300008550286293, evaluation loss: 0.045456450432538986\n",
      "2820/3000 training loss: 0.03297897428274155, evaluation loss: 0.04542836546897888\n",
      "2821/3000 training loss: 0.03295788913965225, evaluation loss: 0.04540030658245087\n",
      "2822/3000 training loss: 0.03293683007359505, evaluation loss: 0.045372284948825836\n",
      "2823/3000 training loss: 0.032915789633989334, evaluation loss: 0.045344285666942596\n",
      "2824/3000 training loss: 0.03289477899670601, evaluation loss: 0.045316312462091446\n",
      "2825/3000 training loss: 0.032873790711164474, evaluation loss: 0.04528837278485298\n",
      "2826/3000 training loss: 0.03285282105207443, evaluation loss: 0.04526045545935631\n",
      "2827/3000 training loss: 0.03283187747001648, evaluation loss: 0.04523256793618202\n",
      "2828/3000 training loss: 0.03281094878911972, evaluation loss: 0.045204710215330124\n",
      "2829/3000 training loss: 0.03279005363583565, evaluation loss: 0.04517688229680061\n",
      "2830/3000 training loss: 0.032769180834293365, evaluation loss: 0.04514908045530319\n",
      "2831/3000 training loss: 0.032748326659202576, evaluation loss: 0.04512130841612816\n",
      "2832/3000 training loss: 0.032727498561143875, evaluation loss: 0.04509355500340462\n",
      "2833/3000 training loss: 0.032706692814826965, evaluation loss: 0.04506583511829376\n",
      "2834/3000 training loss: 0.032685913145542145, evaluation loss: 0.045038145035505295\n",
      "2835/3000 training loss: 0.03266514465212822, evaluation loss: 0.045010488480329514\n",
      "2836/3000 training loss: 0.03264440968632698, evaluation loss: 0.04498285427689552\n",
      "2837/3000 training loss: 0.032623693346977234, evaluation loss: 0.044955238699913025\n",
      "2838/3000 training loss: 0.032603006809949875, evaluation loss: 0.04492765665054321\n",
      "2839/3000 training loss: 0.03258233144879341, evaluation loss: 0.044900111854076385\n",
      "2840/3000 training loss: 0.032561685889959335, evaluation loss: 0.04487258568406105\n",
      "2841/3000 training loss: 0.03254106268286705, evaluation loss: 0.044845081865787506\n",
      "2842/3000 training loss: 0.032520461827516556, evaluation loss: 0.044817615300416946\n",
      "2843/3000 training loss: 0.032499879598617554, evaluation loss: 0.04479016363620758\n",
      "2844/3000 training loss: 0.03247932717204094, evaluation loss: 0.044762756675481796\n",
      "2845/3000 training loss: 0.03245878964662552, evaluation loss: 0.044735364615917206\n",
      "2846/3000 training loss: 0.03243827819824219, evaluation loss: 0.044707994908094406\n",
      "2847/3000 training loss: 0.03241778910160065, evaluation loss: 0.04468066245317459\n",
      "2848/3000 training loss: 0.0323973223567009, evaluation loss: 0.04465335234999657\n",
      "2849/3000 training loss: 0.03237687423825264, evaluation loss: 0.04462607949972153\n",
      "2850/3000 training loss: 0.03235645219683647, evaluation loss: 0.04459882527589798\n",
      "2851/3000 training loss: 0.032336048781871796, evaluation loss: 0.044571589678525925\n",
      "2852/3000 training loss: 0.03231566771864891, evaluation loss: 0.044544387608766556\n",
      "2853/3000 training loss: 0.032295312732458115, evaluation loss: 0.04451721906661987\n",
      "2854/3000 training loss: 0.03227498009800911, evaluation loss: 0.04449007287621498\n",
      "2855/3000 training loss: 0.0322546660900116, evaluation loss: 0.04446294158697128\n",
      "2856/3000 training loss: 0.03223437815904617, evaluation loss: 0.044435855001211166\n",
      "2857/3000 training loss: 0.03221410885453224, evaluation loss: 0.044408783316612244\n",
      "2858/3000 training loss: 0.0321938619017601, evaluation loss: 0.04438174515962601\n",
      "2859/3000 training loss: 0.03217363357543945, evaluation loss: 0.04435473680496216\n",
      "2860/3000 training loss: 0.032153431326150894, evaluation loss: 0.0443277470767498\n",
      "2861/3000 training loss: 0.032133251428604126, evaluation loss: 0.04430078715085983\n",
      "2862/3000 training loss: 0.03211309015750885, evaluation loss: 0.044273849576711655\n",
      "2863/3000 training loss: 0.03209295496344566, evaluation loss: 0.044246938079595566\n",
      "2864/3000 training loss: 0.03207283839583397, evaluation loss: 0.04422006383538246\n",
      "2865/3000 training loss: 0.032052744179964066, evaluation loss: 0.04419320076704025\n",
      "2866/3000 training loss: 0.03203267231583595, evaluation loss: 0.04416637122631073\n",
      "2867/3000 training loss: 0.03201261907815933, evaluation loss: 0.044139564037323\n",
      "2868/3000 training loss: 0.0319925881922245, evaluation loss: 0.044112782925367355\n",
      "2869/3000 training loss: 0.031972579658031464, evaluation loss: 0.0440860390663147\n",
      "2870/3000 training loss: 0.03195260092616081, evaluation loss: 0.04405931010842323\n",
      "2871/3000 training loss: 0.03193262964487076, evaluation loss: 0.04403260722756386\n",
      "2872/3000 training loss: 0.03191268816590309, evaluation loss: 0.04400594159960747\n",
      "2873/3000 training loss: 0.03189276158809662, evaluation loss: 0.04397929087281227\n",
      "2874/3000 training loss: 0.031872861087322235, evaluation loss: 0.043952666223049164\n",
      "2875/3000 training loss: 0.03185298293828964, evaluation loss: 0.04392607510089874\n",
      "2876/3000 training loss: 0.03183312341570854, evaluation loss: 0.04389949515461922\n",
      "2877/3000 training loss: 0.031813282519578934, evaluation loss: 0.04387294873595238\n",
      "2878/3000 training loss: 0.031793467700481415, evaluation loss: 0.04384643957018852\n",
      "2879/3000 training loss: 0.03177367150783539, evaluation loss: 0.043819937855005264\n",
      "2880/3000 training loss: 0.03175390139222145, evaluation loss: 0.043793465942144394\n",
      "2881/3000 training loss: 0.03173414617776871, evaluation loss: 0.04376702383160591\n",
      "2882/3000 training loss: 0.031714413315057755, evaluation loss: 0.04374060779809952\n",
      "2883/3000 training loss: 0.031694699078798294, evaluation loss: 0.043714217841625214\n",
      "2884/3000 training loss: 0.03167501091957092, evaluation loss: 0.0436878465116024\n",
      "2885/3000 training loss: 0.031655341386795044, evaluation loss: 0.04366150498390198\n",
      "2886/3000 training loss: 0.03163569048047066, evaluation loss: 0.043635185807943344\n",
      "2887/3000 training loss: 0.03161606565117836, evaluation loss: 0.0436089001595974\n",
      "2888/3000 training loss: 0.03159646317362785, evaluation loss: 0.04358263313770294\n",
      "2889/3000 training loss: 0.03157687559723854, evaluation loss: 0.043556392192840576\n",
      "2890/3000 training loss: 0.03155730664730072, evaluation loss: 0.0435301810503006\n",
      "2891/3000 training loss: 0.03153776377439499, evaluation loss: 0.04350398853421211\n",
      "2892/3000 training loss: 0.03151824325323105, evaluation loss: 0.04347781464457512\n",
      "2893/3000 training loss: 0.0314987376332283, evaluation loss: 0.04345168173313141\n",
      "2894/3000 training loss: 0.03147925063967705, evaluation loss: 0.04342556744813919\n",
      "2895/3000 training loss: 0.03145978972315788, evaluation loss: 0.04339947551488876\n",
      "2896/3000 training loss: 0.03144034743309021, evaluation loss: 0.043373409658670425\n",
      "2897/3000 training loss: 0.03142092749476433, evaluation loss: 0.04334736987948418\n",
      "2898/3000 training loss: 0.03140152618288994, evaluation loss: 0.04332134500145912\n",
      "2899/3000 training loss: 0.03138214722275734, evaluation loss: 0.04329535737633705\n",
      "2900/3000 training loss: 0.03136278688907623, evaluation loss: 0.04326939582824707\n",
      "2901/3000 training loss: 0.03134344890713692, evaluation loss: 0.04324345290660858\n",
      "2902/3000 training loss: 0.031324129551649094, evaluation loss: 0.04321752488613129\n",
      "2903/3000 training loss: 0.031304825097322464, evaluation loss: 0.043191634118556976\n",
      "2904/3000 training loss: 0.03128555044531822, evaluation loss: 0.04316576570272446\n",
      "2905/3000 training loss: 0.031266290694475174, evaluation loss: 0.043139927089214325\n",
      "2906/3000 training loss: 0.031247049570083618, evaluation loss: 0.04311410337686539\n",
      "2907/3000 training loss: 0.031227830797433853, evaluation loss: 0.04308829829096794\n",
      "2908/3000 training loss: 0.03120863251388073, evaluation loss: 0.04306253418326378\n",
      "2909/3000 training loss: 0.0311894528567791, evaluation loss: 0.04303677752614021\n",
      "2910/3000 training loss: 0.031170297414064407, evaluation loss: 0.04301106184720993\n",
      "2911/3000 training loss: 0.03115115873515606, evaluation loss: 0.04298536106944084\n",
      "2912/3000 training loss: 0.031132038682699203, evaluation loss: 0.04295968636870384\n",
      "2913/3000 training loss: 0.031112942844629288, evaluation loss: 0.04293403401970863\n",
      "2914/3000 training loss: 0.031093861907720566, evaluation loss: 0.04290841519832611\n",
      "2915/3000 training loss: 0.031074805185198784, evaluation loss: 0.042882807552814484\n",
      "2916/3000 training loss: 0.031055765226483345, evaluation loss: 0.042857229709625244\n",
      "2917/3000 training loss: 0.031036749482154846, evaluation loss: 0.04283168166875839\n",
      "2918/3000 training loss: 0.03101774863898754, evaluation loss: 0.042806144803762436\n",
      "2919/3000 training loss: 0.03099876642227173, evaluation loss: 0.04278063774108887\n",
      "2920/3000 training loss: 0.030979804694652557, evaluation loss: 0.04275514930486679\n",
      "2921/3000 training loss: 0.030960863456130028, evaluation loss: 0.0427296943962574\n",
      "2922/3000 training loss: 0.03094194270670414, evaluation loss: 0.042704250663518906\n",
      "2923/3000 training loss: 0.030923040583729744, evaluation loss: 0.042678844183683395\n",
      "2924/3000 training loss: 0.030904164537787437, evaluation loss: 0.042653460055589676\n",
      "2925/3000 training loss: 0.030885299667716026, evaluation loss: 0.04262809082865715\n",
      "2926/3000 training loss: 0.030866459012031555, evaluation loss: 0.04260275512933731\n",
      "2927/3000 training loss: 0.030847633257508278, evaluation loss: 0.042577434331178665\n",
      "2928/3000 training loss: 0.03082883358001709, evaluation loss: 0.04255214333534241\n",
      "2929/3000 training loss: 0.030810045078396797, evaluation loss: 0.04252686724066734\n",
      "2930/3000 training loss: 0.030791278928518295, evaluation loss: 0.042501628398895264\n",
      "2931/3000 training loss: 0.030772535130381584, evaluation loss: 0.04247640073299408\n",
      "2932/3000 training loss: 0.030753808096051216, evaluation loss: 0.042451195418834686\n",
      "2933/3000 training loss: 0.03073509968817234, evaluation loss: 0.04242601618170738\n",
      "2934/3000 training loss: 0.030716411769390106, evaluation loss: 0.04240085557103157\n",
      "2935/3000 training loss: 0.030697742477059364, evaluation loss: 0.04237573966383934\n",
      "2936/3000 training loss: 0.030679091811180115, evaluation loss: 0.04235062748193741\n",
      "2937/3000 training loss: 0.030660465359687805, evaluation loss: 0.042325545102357864\n",
      "2938/3000 training loss: 0.030641848221421242, evaluation loss: 0.042300473898649216\n",
      "2939/3000 training loss: 0.03062325529754162, evaluation loss: 0.04227544739842415\n",
      "2940/3000 training loss: 0.030604684725403786, evaluation loss: 0.04225043207406998\n",
      "2941/3000 training loss: 0.030586130917072296, evaluation loss: 0.0422254279255867\n",
      "2942/3000 training loss: 0.0305675957351923, evaluation loss: 0.04220046475529671\n",
      "2943/3000 training loss: 0.030549075454473495, evaluation loss: 0.04217551648616791\n",
      "2944/3000 training loss: 0.030530577525496483, evaluation loss: 0.042150598019361496\n",
      "2945/3000 training loss: 0.030512096360325813, evaluation loss: 0.04212568700313568\n",
      "2946/3000 training loss: 0.030493639409542084, evaluation loss: 0.04210081323981285\n",
      "2947/3000 training loss: 0.0304751954972744, evaluation loss: 0.04207595810294151\n",
      "2948/3000 training loss: 0.030456772074103355, evaluation loss: 0.042051125317811966\n",
      "2949/3000 training loss: 0.030438369140028954, evaluation loss: 0.04202631488442421\n",
      "2950/3000 training loss: 0.030419984832406044, evaluation loss: 0.042001526802778244\n",
      "2951/3000 training loss: 0.030401617288589478, evaluation loss: 0.04197676479816437\n",
      "2952/3000 training loss: 0.030383272096514702, evaluation loss: 0.041952021420001984\n",
      "2953/3000 training loss: 0.03036493808031082, evaluation loss: 0.04192730039358139\n",
      "2954/3000 training loss: 0.03034662827849388, evaluation loss: 0.04190260171890259\n",
      "2955/3000 training loss: 0.030328333377838135, evaluation loss: 0.04187793284654617\n",
      "2956/3000 training loss: 0.03031006082892418, evaluation loss: 0.04185328260064125\n",
      "2957/3000 training loss: 0.030291805043816566, evaluation loss: 0.04182864725589752\n",
      "2958/3000 training loss: 0.030273567885160446, evaluation loss: 0.04180404171347618\n",
      "2959/3000 training loss: 0.030255353078246117, evaluation loss: 0.041779447346925735\n",
      "2960/3000 training loss: 0.030237149447202682, evaluation loss: 0.04175488278269768\n",
      "2961/3000 training loss: 0.03021896630525589, evaluation loss: 0.04173034802079201\n",
      "2962/3000 training loss: 0.030200805515050888, evaluation loss: 0.04170582816004753\n",
      "2963/3000 training loss: 0.03018265962600708, evaluation loss: 0.04168132692575455\n",
      "2964/3000 training loss: 0.030164534226059914, evaluation loss: 0.04165685921907425\n",
      "2965/3000 training loss: 0.03014642372727394, evaluation loss: 0.041632406413555145\n",
      "2966/3000 training loss: 0.03012833371758461, evaluation loss: 0.04160797595977783\n",
      "2967/3000 training loss: 0.030110260471701622, evaluation loss: 0.04158356785774231\n",
      "2968/3000 training loss: 0.030092211440205574, evaluation loss: 0.04155917838215828\n",
      "2969/3000 training loss: 0.03007417358458042, evaluation loss: 0.04153481125831604\n",
      "2970/3000 training loss: 0.03005615435540676, evaluation loss: 0.04151046648621559\n",
      "2971/3000 training loss: 0.030038151890039444, evaluation loss: 0.04148614779114723\n",
      "2972/3000 training loss: 0.030020171776413918, evaluation loss: 0.04146183654665947\n",
      "2973/3000 training loss: 0.030002206563949585, evaluation loss: 0.04143757373094559\n",
      "2974/3000 training loss: 0.029984261840581894, evaluation loss: 0.0414133183658123\n",
      "2975/3000 training loss: 0.029966337606310844, evaluation loss: 0.04138908535242081\n",
      "2976/3000 training loss: 0.02994842641055584, evaluation loss: 0.041364870965480804\n",
      "2977/3000 training loss: 0.029930533841252327, evaluation loss: 0.04134068638086319\n",
      "2978/3000 training loss: 0.029912659898400307, evaluation loss: 0.04131651297211647\n",
      "2979/3000 training loss: 0.02989480271935463, evaluation loss: 0.04129236936569214\n",
      "2980/3000 training loss: 0.029876964166760445, evaluation loss: 0.041268233209848404\n",
      "2981/3000 training loss: 0.029859144240617752, evaluation loss: 0.041244130581617355\n",
      "2982/3000 training loss: 0.029841339215636253, evaluation loss: 0.0412200428545475\n",
      "2983/3000 training loss: 0.029823554679751396, evaluation loss: 0.04119598865509033\n",
      "2984/3000 training loss: 0.029805786907672882, evaluation loss: 0.04117194935679436\n",
      "2985/3000 training loss: 0.02978803589940071, evaluation loss: 0.041147928684949875\n",
      "2986/3000 training loss: 0.029770301654934883, evaluation loss: 0.04112393409013748\n",
      "2987/3000 training loss: 0.029752586036920547, evaluation loss: 0.04109995812177658\n",
      "2988/3000 training loss: 0.029734889045357704, evaluation loss: 0.041075997054576874\n",
      "2989/3000 training loss: 0.029717206954956055, evaluation loss: 0.041052062064409256\n",
      "2990/3000 training loss: 0.029699545353651047, evaluation loss: 0.041028156876564026\n",
      "2991/3000 training loss: 0.029681896790862083, evaluation loss: 0.041004255414009094\n",
      "2992/3000 training loss: 0.02966427244246006, evaluation loss: 0.04098040238022804\n",
      "2993/3000 training loss: 0.029646659269928932, evaluation loss: 0.040956541895866394\n",
      "2994/3000 training loss: 0.029629062861204147, evaluation loss: 0.04093271121382713\n",
      "2995/3000 training loss: 0.029611490666866302, evaluation loss: 0.04090890660881996\n",
      "2996/3000 training loss: 0.029593931511044502, evaluation loss: 0.040885113179683685\n",
      "2997/3000 training loss: 0.029576389119029045, evaluation loss: 0.0408613458275795\n",
      "2998/3000 training loss: 0.02955886535346508, evaluation loss: 0.0408376045525074\n",
      "2999/3000 training loss: 0.029541360214352608, evaluation loss: 0.040813885629177094\n",
      "3000/3000 training loss: 0.02952386811375618, evaluation loss: 0.04079017788171768\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, eval_loss = 0.0, 0.0\n",
    "    for x, y in train_ds.as_numpy_iterator():\n",
    "        loss, preds, state = step(x, y, state, is_train=True)\n",
    "        train_loss = train_loss + loss\n",
    "\n",
    "    for x, y in test_ds.as_numpy_iterator():\n",
    "        loss, preds, state = step(x, y, state, is_train=False)\n",
    "        eval_loss = eval_loss + loss\n",
    "\n",
    "    print(\"{}/{} training loss: {}, evaluation loss: {}\".format(epoch+1, num_epochs, train_loss, eval_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fvec(x, t, state):\n",
    "    def sum_hamiltonian(params, x):\n",
    "        hamiltonian = state.apply_fn({'params': state.params}, x)\n",
    "        return jnp.sum(hamiltonian)\n",
    "\n",
    "    dhdu_dhdv = jax.jit(jax.grad(sum_hamiltonian, argnums=1))\n",
    "\n",
    "    gradient = dhdu_dhdv(state.params, x)\n",
    "    return jnp.matmul(gradient, St)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = jnp.asarray([1.0, 0.0])\n",
    "teval = jnp.linspace(0.0, 10.0, 100)\n",
    "args = (state,)\n",
    "\n",
    "orbit = odeint(fvec, x0, teval, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from apng import APNG\n",
    "from math import cos, pi, sin\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def make_animation(index, qval, pval):\n",
    "    filename = \"{:0>4}.png\".format(index)\n",
    "    im = Image.new(\"RGB\", (100, 100), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    x = 30*sin(qval) + 50\n",
    "    y = 30*cos(qval) + 50\n",
    "    draw.line((50, 50, x, y), fill=(0, 255, 0), width=2)\n",
    "    draw.ellipse((x-5, y-5, x+5, y+5), fill=(0, 0, 255))\n",
    "    im.save(filename)\n",
    "    return filename\n",
    "\n",
    "os.makedirs(\"./visualization/animation/\", exist_ok=True)\n",
    "\n",
    "os.chdir(\"./visualization/\")\n",
    "\n",
    "files = []\n",
    "for i in range(100):\n",
    "    files.append(make_animation(i, orbit[i,0], orbit[i,1]))\n",
    "APNG.from_files(files, delay=50).save(\"animation/animation_model.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba72c6c2ec42a78497f7dd53cc80d7839de11745de6cd646632e47d2605de955"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
